{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own LLM on Nepali Language (Part-1)\n",
    "\n",
    "* why: customization, nepali language\n",
    "* Datasets : crawl nepali newspapers, commonCrawl, huggingface_datasets, Falcon Refined Web\n",
    "* Model architectures: Encoder, Decoder (combined/stand_alone)\n",
    "* Benchmarks: <to-search>\n",
    "\n",
    "# Resources\n",
    "\n",
    "* [BERT-paper](https://arxiv.org/abs/1810.04805)\n",
    "* [original GPT paper ](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)\n",
    "* [GPT-2 paper](paperLanguage Models are Unsupervised Multitask Learners (2019))\n",
    "* [GPT-3 model paper](paper: Language models are few-shot\n",
    "learners (2020))\n",
    "* Transformer-XL authors (paper:\n",
    "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019))\n",
    "* XLNet (paper: XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019))\n",
    "*  T5 (paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019))\n",
    "* BART model (paper: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019))\n",
    "* exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models, Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann, 2019\n",
    "* [RoBERTa](https://arxiv.org/abs/1907.11692)\n",
    "\n",
    "## Books:\n",
    "* [Mastering Transformers by Savaş Yıldırım](https://www.amazon.com/Mastering-Transformers-state-art-processing/dp/1801077657)\n",
    "\n",
    "## Videos\n",
    "* https://exbert.net/\n",
    "* * [Create a Large Language Model from Scratch with Python – Tutorial](https://m.youtube.com/watch?v=UU1WVnMk4E8)\n",
    "\n",
    "\n",
    "# Nepali language models:\n",
    "* [Nepberta](https://aclanthology.org/2022.aacl-short.34/)\n",
    "* [Nepberta (Huggingface)](https://huggingface.co/Rajan/NepaliBERT)\n",
    "* [Distilled GPT-2 Nepali (Huggingface)](https://huggingface.co/Sakonii/distilgpt2-nepali)\n",
    "\n",
    "\n",
    "# References\n",
    "* [Build Your Own LLM Model Using OpenAI](https://medium.com/@nileshpatel7048/build-your-own-llm-model-using-openai-6ed0954e4db1)\n",
    "* [A Step-by-Step Guide to Training Your Own Large Language Models (LLMs).](https://blog.gopenai.com/a-step-by-step-guide-to-training-your-own-llm-2d81ff810695)\n",
    "* [How to Build an LLM from Scratch](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Nepali LLm (Part-2/N)\n",
    "\n",
    "# Datasets:\n",
    "\n",
    "## 1. nepalitext-language-model-dataset\n",
    "    * 13 million Nepali text sequences\n",
    "    * extracted by combining the datasets: OSCAR , cc100 \n",
    "\n",
    "## 2.OSCAR\n",
    "    * Open Super-large Crawled ALMAnaCH coRpus \n",
    "    * multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture\n",
    "\n",
    "## 3. Common Crawl [4]\n",
    "    * free, open repository of web crawl datahttps://commoncrawl.org\n",
    "\n",
    "## 4. Crawling Online Nepali newspapers\n",
    "    * Nepberta[4] seems to have crawled 36 nepali online newspapers to obtain 1.5 billion tokens in total.\n",
    "    \n",
    "    * It seems good idea to crawl online newspapers as they could provide large amount of up to date data.\n",
    "    \n",
    "    * We have implemented simple scrapy crawler[5] to crawl online nepali newspapers.\n",
    "    * Data crawled so far is stored at [6]\n",
    "\n",
    "\n",
    "# References:\n",
    "1. [nepalitext-language-model-dataset](https://huggingface.co/datasets/Sakonii/nepalitext-language-model-dataset)\n",
    "2. [OSCAR](https://huggingface.co/datasets/oscar)\n",
    "3. [CC100](https://huggingface.co/datasets/cc100)\n",
    "4. [NepBerta](https://aclanthology.org/2022.aacl-short.34/)\n",
    "5. [scrapy-crawler](https://github.com/Aananda-giri/scrapy_engine)\n",
    "6. [crawled-dataset](https://drive.google.com/drive/folders/1v_dv0H56D3J-56VDPIaBkJ601djs8C0w?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Nepali LLm (Part-3/N)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizion\n",
    "\n",
    "* Padding and truncation of sentences.\n",
    "\n",
    "* N-gram: Combination of sentence and word level tokenizion\n",
    "\n",
    "* Special tokens: CLS <classification>, SEP<seperator>, MASK\n",
    "\n",
    "* note: tokena are unit of text data: words, subwords, characters, punctuation marks, etc.\n",
    "\n",
    "\n",
    "\n",
    "# Training (Theory):\n",
    "\n",
    "Language Models like chat-gpt [2] seems to be trained in three stages:\n",
    "\n",
    "\n",
    "\n",
    "## 1. Pre-training\n",
    "\n",
    "* Masked language modelling: predict [MASK]. Mask is token removed from sentence.\n",
    "\n",
    "* NEXT TOKEN PREDICTION\n",
    "\n",
    "* Next Sentence Prediction (NSP):\n",
    "\n",
    "* Continuous Bag of Words (CBOW): predicting a target word based on the context of its surrounding words.\n",
    "\n",
    "* Skip-gram: Given a target word, the model predicts the context words surrounding it.\n",
    "\n",
    "* Denoising Autoencoder: Instead of masking random words, denoising autoencoders add noise to the input data and train the model to reconstruct the original, uncorrupted input\n",
    "\n",
    "* Replaced Token Detection:\n",
    "\n",
    "* Similar to MLM, this approach involves replacing a token in a sentence with another token and training the model to detect the replacement\n",
    "\n",
    "* Permutation Language Modeling (PLM): randomly permuting the order of words in a sentence and training the model to predict the original word order\n",
    "\n",
    "* Document-level Tasks: tasks as predicting the next document in a sequence or understanding document semantics.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Reward Model\n",
    "\n",
    "* Training reward model.\n",
    "\n",
    "* RLHF model is trained to predict the reward given by human feedback.\n",
    "\n",
    "* Model generates multiple outputs and human labeler ranks them from best to worst.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Fine-tuning\n",
    "\n",
    "* train on downstream task like question answering, summarization, etc.\n",
    "\n",
    "* Optimize policy against reward model using PPO RL algorithm.\n",
    "\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "1. [Paper. Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n",
    "\n",
    "2. [Blog. Chat-gpt introduction](https://openai.com/blog/chatgpt)\n",
    "\n",
    "3. [Vid. Let's build GPT-Andrej karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
