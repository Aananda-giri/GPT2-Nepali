{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "12e91914-5f51-43fa-b65b-625e73b4d17b",
      "metadata": {
        "id": "12e91914-5f51-43fa-b65b-625e73b4d17b"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp?1\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
      "metadata": {
        "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
      },
      "source": [
        "# Chapter 7: Finetuning To Follow Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L9DvrsHQR6FF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9DvrsHQR6FF",
        "outputId": "544d086b-40ec-4dba-bf56-56e38751e272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKT_y_kLSHhN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKT_y_kLSHhN",
        "outputId": "d338475e-cc14-4e22-c5a1-98c5abaaa159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Research/GPT2-Fine-Tuning\n"
          ]
        }
      ],
      "source": [
        "# !mkdir /content/drive/MyDrive/Research/\n",
        "%cd /content/drive/MyDrive/Research/GPT2-Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJtHgl-FLBNL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJtHgl-FLBNL",
        "outputId": "6b1dcecb-89a5-4f84-e8d3-5afacb996c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ch07.ipynb  gpt_model_code.py  NepaliBPE\t     __pycache__\n",
            "data.json   gpt_model.py       previous_chapters.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rb34vm442-rD",
      "metadata": {
        "id": "Rb34vm442-rD"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
        "outputId": "8d693ad0-4b31-4020-c3a4-97b7cb10f573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "matplotlib version: 3.10.0\n",
            "tiktoken version: 0.9.0\n",
            "torch version: 2.6.0+cu124\n",
            "tqdm version: 4.67.1\n",
            "tensorflow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "\n",
        "pkgs = [\n",
        "    \"matplotlib\",  # Plotting library\n",
        "    \"tiktoken\",    # Tokenizer\n",
        "    \"torch\",       # Deep learning library\n",
        "    \"tqdm\",        # Progress bar\n",
        "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264fca98-2f9a-4193-b435-2abfa3b4142f",
      "metadata": {
        "id": "264fca98-2f9a-4193-b435-2abfa3b4142f"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/overview.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813",
      "metadata": {
        "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"
      },
      "source": [
        "## 7.1 Introduction to instruction finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab",
      "metadata": {
        "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab"
      },
      "source": [
        "- In chapter 5, we saw that pretraining an LLM involves a training procedure where it learns to generate one word at a time\n",
        "- Hence, a pretrained LLM is good at text completion, but it is not good at following instructions\n",
        "- In this chapter, we teach the LLM to follow instructions better"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18dc0535-0904-44ed-beaf-9b678292ef35",
      "metadata": {
        "id": "18dc0535-0904-44ed-beaf-9b678292ef35"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/instruction-following.webp\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8",
      "metadata": {
        "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8"
      },
      "source": [
        "- The topics covered in this chapter are summarized in the figure below\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-1.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86",
      "metadata": {
        "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86"
      },
      "source": [
        "## 7.2 Preparing a dataset for supervised instruction finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b34ff8-619f-4e89-bd03-ce513269760d",
      "metadata": {
        "id": "f8b34ff8-619f-4e89-bd03-ce513269760d"
      },
      "source": [
        "- We will work with an instruction dataset I prepared for this chapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npMdH0Zjho2G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "npMdH0Zjho2G",
        "outputId": "fb55e53b-57e7-4958-fa75-0dcc1c59bf0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'नुवाकोटको तारकेश्वर गाउँपालिका ५ को बिर्ता व्यथा त्यो जिल्लाका अन्यत्रभन्दा बिल्कुल फरक छ । जिल्लाका राजनीतिकर्मी र भूमि अभियन्ताका अनुसार नुवाकोटमा सबैभन्दा बढी बिर्तापीडित किसान यही वडामा बस्छन् । जिल्लाभरिमै बिर्ता समस्याले सबैभन्दा गाँजेको यस वडाका जमिन जोत्ने किसानलाई शताब्दीऔंदेखि सरकारी अड्डाले अल्झाइरहेको छ ।'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Probably should `clean_data(new_crawled_data)` before merging `new_crawled_data.csv` and previous`cleaned_data.csv`\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class CleanData:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        # return text\n",
        "    # Example of removing HTML tags\n",
        "    def clean_html(self, text):\n",
        "        '''\n",
        "        # HTML Tag Removal:\n",
        "        * removes html tags like: <h1>\n",
        "        * Removes css or js code inside <style> and <script> tags\n",
        "        '''\n",
        "        soup = BeautifulSoup(text, \"lxml\")\n",
        "\n",
        "        # Remove all <script> and <style> tags\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.decompose()\n",
        "\n",
        "        # Get text from the modified HTML\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        # print(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def convert_to_devanagari_digits(self, input_string):\n",
        "        # Function to convert 0-9 to ० - ९\n",
        "        # i.e. Mapping of ASCII digits to Devanagari digits\n",
        "        devanagari_digits = {\n",
        "            '0': '०',\n",
        "            '1': '१',\n",
        "            '2': '२',\n",
        "            '3': '३',\n",
        "            '4': '४',\n",
        "            '5': '५',\n",
        "            '6': '६',\n",
        "            '7': '७',\n",
        "            '8': '८',\n",
        "            '9': '९'\n",
        "        }\n",
        "        # Convert each digit in the input string\n",
        "        result = ''.join(devanagari_digits[char] if char in devanagari_digits else char for char in input_string)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def remove_non_devanagari_characters(self, text, keep_special_characters=True):\n",
        "        '''\n",
        "            # Function to find nepali sequences.\n",
        "            * keep punctuations if they occur between devanagari characters.\n",
        "            * Remove punctuation if previous character is not devanagari.\n",
        "            # Examples\n",
        "            texts = [\n",
        "                \"उनले दुहेको दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे। \\\"hi there\\\". what is your name? उनले दुहेको दूध\",\n",
        "                \"\\\"hi there. \\\"उनले दुहेको\\\" दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे। hi there. what is your name? उनले दुहेको दूध\\\"\",\n",
        "                \"name? उनले दुहेको दूध\\\"\"    #output: (last quatation, name?) should be ignored\n",
        "                ]\n",
        "\n",
        "            for text in texts:\n",
        "                removed = remove_non_devanagari_characters(text)\n",
        "                print(f'text: {text}, \\nclen: {removed}\\n\\n')\n",
        "\n",
        "\n",
        "            # output\n",
        "            text: उनले दुहेको दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे। \"hi there\". what is your name? उनले दुहेको दूध,\n",
        "            clen: उनले दुहेको दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे।             उनले दुहेको दूध\n",
        "\n",
        "\n",
        "            text: \"hi there. \"उनले दुहेको\" दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे। hi there. what is your name? उनले दुहेको दूध\",\n",
        "            clen:    \"उनले दुहेको दूध बेच्नका लागि बजार असाध्यै सानो थियो त्यसैले उनले चीज बनाउने विचार गरे।             उनले दुहेको दूध\"\n",
        "\n",
        "\n",
        "            text: name? उनले दुहेको दूध\",\n",
        "            clen:  उनले दुहेको दूध\"\n",
        "        '''\n",
        "        def is_devanagari(char):\n",
        "            pattern=r'[ऀ-ॿ]'\n",
        "            return bool(re.match(pattern, char))\n",
        "\n",
        "        if not keep_special_characters:\n",
        "            return re.sub(r\"[^ऀ-ॿ ]\", \" \", text)\n",
        "\n",
        "        sequences = []\n",
        "        sequence = ''\n",
        "        punctuation_symbols = string.punctuation    # '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        prefix_punctuations = '\\\"\\'(<[{'\n",
        "        index=0\n",
        "        while index < len(text):\n",
        "            char = text[index]\n",
        "            if is_devanagari(char) or char == ' ':\n",
        "                # Character is devanagari\n",
        "                sequence += char\n",
        "            elif char in punctuation_symbols:\n",
        "                # Character is punctuation\n",
        "                if sequence != '':\n",
        "                    if (len(text) > index+1) and not is_devanagari(text[index+1]):\n",
        "                        # e.g. गरे। \"hi there\" : skip quotation before hi\n",
        "                        pass\n",
        "                    else:\n",
        "                        sequence += char    # Sequence is no empty. i.e. previous char/sequence was devanagari otherwise ignore  punctuation\n",
        "                elif (len(text) > index+1) and is_devanagari(text[index+1]):\n",
        "                    # preserve prefix punctuations in devanagari. e.g. \"\"\"there. \\\"उनले \"\": preserve double-quotation before उनले\n",
        "                    sequence = char + text[index+1]\n",
        "                    index += 1  # another 1 is added at the end\n",
        "            else:\n",
        "                if sequence:\n",
        "                    sequences.append(sequence)\n",
        "                    sequence = ''   # Reset sequence\n",
        "            index += 1\n",
        "\n",
        "            # print(f'{sequences}\\n{sequence}\\n{char}{is_devanagari(char)}\\n\\n')\n",
        "        if sequence:    # last sequence\n",
        "            sequences.append(sequence)\n",
        "        return ' '.join(sequences)\n",
        "        # Example of using regex for special character removal\n",
        "\n",
        "    def normalize_data(self, text):\n",
        "      '''\n",
        "        * Standerize special characters\n",
        "        * e.g. convert different types of quotes to standard quotes\n",
        "      '''\n",
        "      characters_to_replace = {\n",
        "        '₹': 'रु',\n",
        "        'ʻ': \"'\",\n",
        "        'ː': ':',\n",
        "        '？': '?',\n",
        "        '‟': '\"',\n",
        "        '“' : '\"',\n",
        "        '”': '\"',\n",
        "        '`': \"'\",\n",
        "        '৷': '।',\n",
        "        'ˈ': \"'\",\n",
        "        '՛': \"'\",\n",
        "        'ǃ': '!',\n",
        "        '（': '(',\n",
        "        '：': ':',\n",
        "        'ˍ': '_',\n",
        "        '﹣': '-',\n",
        "        '״': '\"',\n",
        "        'ꞌ': \"'\",\n",
        "        '₋': '-',\n",
        "        '％': '%',\n",
        "        '꞉': ':',\n",
        "        '‵': \"'\"\n",
        "      }\n",
        "      # Replace each character in the dictionary with its corresponding standard character\n",
        "      for char, replacement in characters_to_replace.items():\n",
        "          text = text.replace(char, replacement)\n",
        "\n",
        "      return text\n",
        "\n",
        "    def clean_data(self, text):\n",
        "        # Remove HTML tags\n",
        "        text = self.clean_html(text)\n",
        "\n",
        "        # Normalize some characters\n",
        "        text = self.normalize_data(text)\n",
        "\n",
        "        # Convert 0-9 to ० - ९\n",
        "        text = self.convert_to_devanagari_digits(text)\n",
        "\n",
        "        text = self.remove_non_devanagari_characters(text, keep_special_characters=True)\n",
        "        # text = text.lower() # No lower characters in devanagari\n",
        "\n",
        "        # Replace one or more spaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "data_cleaner = CleanData()\n",
        "data_cleaner.clean_data('hi नुवाकोटको there तारकेश्वर गाउँपालिका–५ को बिर्ता–व्यथा त्यो जिल्लाका अन्यत्रभन्दा बिल्कुल फरक छ । जिल्लाका राजनीतिकर्मी र भूमि अभियन्ताका अनुसार, नुवाकोटमा सबैभन्दा बढी बिर्तापीडित किसान यही वडामा बस्छन् । जिल्लाभरिमै बिर्ता समस्याले सबैभन्दा गाँजेको यस वडाका जमिन जोत्ने किसानलाई शताब्दीऔंदेखि सरकारी अड्डाले अल्झाइरहेको छ ।')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVcbmq25hbcG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVcbmq25hbcG",
        "outputId": "30c4c27f-3348-45bb-851c-f1817c5fd4fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52005"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import load_dataset\n",
        "\n",
        "'''\n",
        "# Dataset: Saugatkafley/alpaca-nepali-sft\n",
        "\n",
        "- rows: instruction, input, output\n",
        "'''\n",
        "\n",
        "ds = load_dataset(\"Saugatkafley/alpaca-nepali-sft\")\n",
        "data = [d for d in ds['train']]\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7lqp6bBrf4Gg",
      "metadata": {
        "id": "7lqp6bBrf4Gg"
      },
      "source": [
        "## Detect language\n",
        "- langid is faster but langdetect is more accurate\n",
        "- also there is this issue with langdetect: [choose one or other lang.](https://stackoverflow.com/questions/37235932/python-langdetect-choose-between-one-language-or-the-other-only)\n",
        "- so using langid (as you can make it choose betn `ne`, `en` since dataset by wiseyak is either nepali or english)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ohoq0v0aZyo",
      "metadata": {
        "id": "4ohoq0v0aZyo"
      },
      "source": [
        "### Langdetect example\n",
        "```!pip install langdetect --quiet\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0  # To make results deterministic\n",
        "\n",
        "def is_nepali(text):\n",
        "    try:\n",
        "        return detect(text) == \"ne\"\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Example Usage\n",
        "text = \"यो नेपाली भाषा what do you mean हो।\"\n",
        "print(is_nepali(text))  # Output: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bTUgtBaUdTnc",
      "metadata": {
        "id": "bTUgtBaUdTnc"
      },
      "source": [
        "### langid example\n",
        "```!pip install langid --quiet\n",
        "import langid\n",
        "\n",
        "def is_nepali(text):\n",
        "    langid.set_languages(['ne', 'en'])  # ISO 639-1 codes\n",
        "    lang, score = langid.classify(text)\n",
        "    # print(lang) # ne or en\n",
        "    return lang == \"ne\"\n",
        "\n",
        "# Example Usage\n",
        "print(is_nepali('\"यो नेपाली भाषा what do you mean  हो।\"'))  # Output: True\n",
        "print(is_nepali('यो नेपाली भाषा what do you mean  हो।'))  # Output: True\n",
        "print(is_nepali('Is this text english or nepali?'))  # Output: False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knkOOY3Zu_Jy",
      "metadata": {
        "id": "knkOOY3Zu_Jy"
      },
      "outputs": [],
      "source": [
        "!pip install langid --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6maXmBS1fTDY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "12aa86ab7ccd4d04ae05145cee2ace25",
            "edd8c72bc25b42059f134ecc3b302c44",
            "fc72e11e7fc740e8a8bcd325224eb1f7",
            "c6f4679bc08d48b281a4a49fa557f290",
            "6b76b45fbb3e4abdb271e83429d80071",
            "b40bef8d56fa413aa805fb7f16e53232",
            "a4c6e024e4e84ab6b923b17dd49f2063",
            "0cc80e439e99440a9f837768be4d6e28",
            "f070fd4abb6c417799af732624467359",
            "e430de5ac2b2480a8a33fabec7149caa",
            "ffff2e5683624bf1a29ce248544662b4",
            "f70b9835b1fd42a2837cf76c894661bf",
            "467484aec7e3487aa03c28bea096c903",
            "382e407c5e2640b0bcfa597727508c4d",
            "75448f76e86a417e85b2fa7f40c6589f",
            "8561f0070e304dc5b8956829f664a329",
            "cdf3ceb14eeb4e27856fe79c103332ae",
            "809231b736d4417e941ab684f8f483cc",
            "87be4eec65da4e009c2061f847ad6da1",
            "fc5ee43429894db1ae3afbc426368ffc",
            "c1d252bea60e47beba7a19f5e73b7bcb",
            "f8f4054069e04f21aeb76583fa985807",
            "2762d9882bc445bf92c15dce0b3bbb8f",
            "d9b99e8a7c64462dbe6503f542691718",
            "0c5fe1e2c485414e86005eebbb44b770",
            "9d7e756e12a748ea98de28870300857d",
            "e59c6b8df3464660ac383523e08b9f91",
            "d2ac73bd164940d6aa6f1fbf7c7841b4",
            "2e1751fb411e4b67a66afb7cca604f16",
            "9c076897308c403081c8a793dee2a419",
            "2369a38eb6444b49b0f5bf0188e8249d",
            "21875bc5b7f4479bb95dc7589e5906ff",
            "91351da4c2b54246aed65121da1f253f"
          ]
        },
        "id": "6maXmBS1fTDY",
        "outputId": "1e70284d-0463-4fa8-e0d6-c37b17e9a5e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12aa86ab7ccd4d04ae05145cee2ace25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/744 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f70b9835b1fd42a2837cf76c894661bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/71.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2762d9882bc445bf92c15dce0b3bbb8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/113751 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'यस कोडलाई रिफ्याक्टर गर्नुहोस् र टिप्पणीहरू थप्नुहोस्।', 'input': 'डेफ फ्याक्टोरियलाइज (num):\\n\\nकारक = 1\\n\\nदायरामा आइका लागि (1, नम्बर):\\n\\nकारक * = i\\n\\n........................................................................................................................................................................................................\\n\\nरिटर्न फ्याक्टोरियल', 'output': '\"\"\\n\\n#यो एउटा प्रकार्य हो जसले दिइएको सङ्ख्याको फ्याक्टोरियल गणना गर्दछ।\\n\\nडेफ फ्याक्टोरियलाइज (num):\\n\\n#चर \\'कारक\\' लाई 1 मा सेट गर्नुहोस्\\n\\nकारक = 1\\n\\n#रेन्ज प्रकार्य प्रयोग गरेर 1 बाट \\'num\\' मा पुनरावृत्ति गर्नुहोस्\\n\\nदायरामा आइका लागि (1, नम्बर):\\n\\n#पुनरावृत्तिमा हालको सङ्ख्याद्वारा \\'कारक\\' चरलाई गुणन गर्नुहोस्\\n\\nकारक * = i\\n\\n#\\'फ्याक्टोरियल\\' को अन्तिम मान फिर्ता गर्नुहोस्\\n\\nरिटर्न फ्याक्टोरियल\\n\\n\"\"'}\n",
            "{'instruction': 'प्रदान गरिएको जानकारीको आधारमा, वाक्यको काललाई विगतबाट भविष्यमा परिवर्तन गरेर पुनः लेख्नुहोस्।', 'input': 'उनले घन्टौँसम्म सुन्दर पियानो बजाउँथिन् अनि त्यसपछि मध्यरात भएकाले रोकिइन्।', 'output': 'उनी घन्टौँसम्म सुन्दर पियानो बजाउँछिन् र त्यसपछि मध्यरात हुने हुनाले रोकिन्छिन्।'}\n",
            "{'instruction': 'सक्रिय आवाज प्रयोग गरेर निम्न वाक्यलाई पुनः लेख्नुहोस्।', 'input': 'समाचार प्रतिवेदन कप्तानले पढेका थिए।', 'output': 'कप्तानले समाचार पढेर सुनाए।'}\n",
            "len. entries before filtering: 113751\n",
            "len. entries after filtering: 11476\n",
            "len. data: 63481\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "# Dataset: Wiseyak/Wiseyak-SFT-Mixed-ne-en\n",
        "\n",
        "- rows: instruction, input, output\n",
        "- dataset contains mixed english and nepali text data\n",
        "\n",
        "- using langdetect (by google) to detect nepali language\n",
        "- library:langid is faster but langdetect seems to be more accurate\n",
        "'''\n",
        "\n",
        "from datasets import load_dataset\n",
        "import langid\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"Wiseyak/Wiseyak-SFT-Mixed-ne-en\")\n",
        "\n",
        "langid.set_languages(['ne', 'en'])  # ISO 639-1 codes\n",
        "# Function to check if any field contains Nepali text\n",
        "def is_nepali(text):\n",
        "    lang, score = langid.classify(text)\n",
        "    # print(lang) # ne or en\n",
        "    return lang == \"ne\"\n",
        "\n",
        "# Filter dataset to include only rows where instruction, input, or output is Nepali\n",
        "filtered_data = [d for d in ds['train'] if is_nepali(d['instruction']) and is_nepali(d['input']) and is_nepali(d['output'])]\n",
        "data += filtered_data\n",
        "\n",
        "# Example: Print first 3 filtered rows\n",
        "for row in filtered_data[:3]:\n",
        "    print(row)\n",
        "\n",
        "print(f\"len. entries before filtering: {len(ds['train'])}\")\n",
        "print(f\"len. entries after filtering: {len(filtered_data)}\")\n",
        "print(f'len. data: {len(data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_8UHBVGFNSlC",
      "metadata": {
        "id": "_8UHBVGFNSlC"
      },
      "source": [
        "---\n",
        "data2 wiseyak\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FAFBhRwrLloK",
      "metadata": {
        "id": "FAFBhRwrLloK"
      },
      "source": [
        "```\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Wiseyak/Wiseyak-SFT-Mixed-ne-en\")\n",
        "data = [d for d in ds['train']]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QbgTIpzbLeJh",
      "metadata": {
        "id": "QbgTIpzbLeJh"
      },
      "source": [
        "```\n",
        "len(data) # 113751\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YaN1sKLgNmwW",
      "metadata": {
        "id": "YaN1sKLgNmwW"
      },
      "source": [
        "```\n",
        "a=InstructionDataset(data, tokenizer)\n",
        "len(a)  # 46329\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CkyIoAljNWtl",
      "metadata": {
        "id": "CkyIoAljNWtl"
      },
      "source": [
        "---\n",
        "/data2 wiseyak\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4xbxHTkus5jI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xbxHTkus5jI",
        "outputId": "a2de9de9-f08d-44a3-8422-5be0f680ab77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type: <class 'list'>, len:63481 \n",
            " sample: {'instruction': 'स्वस्थ रहन तीनवटा टिप्स दिनुहोस्।', 'input': '', 'output': '1. सन्तुलित आहार खानुहोस् र प्रशस्त फलफूल र तरकारीहरू समावेश गर्न निश्चित गर्नुहोस्।\\n2. आफ्नो शरीर सक्रिय र बलियो राख्न नियमित रूपमा व्यायाम गर्नुहोस्।\\n3. पर्याप्त निद्रा लिनुहोस् र एक सुत्ने तालिका कायम राख्नुहोस्।', 'id': 0}\n"
          ]
        }
      ],
      "source": [
        "print(f'type: {type(data)}, len:{len(data)} \\n sample: {data[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
      "metadata": {
        "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
      },
      "source": [
        "- Each item in the `data` list we loaded from the JSON file above is a dictionary in the following form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-LiuBMsHkzQV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LiuBMsHkzQV",
        "outputId": "ede23ca3-9dc4-401e-c96d-e53c6cc24cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example entry:\n",
            " {'instruction': 'यसलाई थप संक्षिप्त बनाउन निम्न वाक्य सम्पादन गर्नुहोस्।', 'input': 'पाँच मिनेटमा आइपुग्नु पर्ने बस समात्न उनी दौडिएर बस स्टपमा पुगे ।', 'output': 'पाँच मिनेटमा आइपुग्ने भन्दै उनी दौडिएर बस स्टपमा पुगे ।', 'id': 50}\n"
          ]
        }
      ],
      "source": [
        "print(\"Example entry:\\n\", data[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46",
      "metadata": {
        "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46"
      },
      "source": [
        "- Note that the `'input'` field can be empty:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uFInFxDDk2Je",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFInFxDDk2Je",
        "outputId": "5f99400a-5ef6-4035-bab6-78f219a29d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Another example entry:\n",
            " {'instruction': 'निम्न प्रकारको खानाको स्वाद प्रोफाइल वर्णन गर्नुहोस्', 'input': 'जापानी', 'output': 'जापानी व्यञ्जन यसको सूक्ष्म र नाजुक स्वादहरू द्वारा विशेषता हो, नमकीन, मीठो, खट्टा, र उमामी स्वादहरूको संयोजनको विशेषता। यसले तिनीहरूको प्राकृतिक स्वादको संरक्षणमा ध्यान केन्द्रित गरेर ताजा सामग्रीहरू पनि प्रयोग गर्दछ।', 'id': 999}\n"
          ]
        }
      ],
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f034799a-6575-45fd-98c9-9d1012d0fd58",
      "metadata": {
        "id": "f034799a-6575-45fd-98c9-9d1012d0fd58"
      },
      "source": [
        "- Instruction finetuning is often referred to as \"supervised instruction finetuning\" because it involves training a model on a dataset where the input-output pairs are explicitly provided\n",
        "- There are different ways to format the entries as inputs to the LLM; the figure below illustrates two example formats that were used for training the Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and Phi-3 (https://arxiv.org/abs/2404.14219) LLMs, respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10",
      "metadata": {
        "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/prompt-style.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6",
      "metadata": {
        "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6"
      },
      "source": [
        "- In this chapter, we use Alpaca-style prompt formatting, which was the original prompt template for instruction finetuning\n",
        "- Below, we format the input that we will pass as input to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jhk37nnJnkBh",
      "metadata": {
        "id": "Jhk37nnJnkBh"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\"\n",
        "        f\"\\n\\n### प्रतिक्रिया:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### इनपुट:\\n{entry['input']}\" if entry[\"input\"] and '<noinput>' not in entry['input'] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6",
      "metadata": {
        "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6"
      },
      "source": [
        "- A formatted response with input field looks like as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F9UQRfjzo4Js",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9UQRfjzo4Js",
        "outputId": "f715607b-3aba-4ce9-fd89-38d50b84c0b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "यसलाई थप संक्षिप्त बनाउन निम्न वाक्य सम्पादन गर्नुहोस्।\n",
            "\n",
            "### इनपुट:\n",
            "पाँच मिनेटमा आइपुग्नु पर्ने बस समात्न उनी दौडिएर बस स्टपमा पुगे ।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "पाँच मिनेटमा आइपुग्ने भन्दै उनी दौडिएर बस स्टपमा पुगे ।\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### प्रतिक्रिया:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aFZVopbIlNfx",
      "metadata": {
        "id": "aFZVopbIlNfx"
      },
      "outputs": [],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-zf6oht6bIUQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zf6oht6bIUQ",
        "outputId": "76bb5224-db96-4ad9-9c8f-7d1929b311e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set length: 53958\n",
            "Validation set length: 3175\n",
            "Test set length: 6348\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIQ7BkQO-wJl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIQ7BkQO-wJl",
        "outputId": "79f43215-2bfd-49f6-dfc6-744b05c108e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'स्वस्थ रहन तीनवटा टिप्स दिनुहोस्।',\n",
              "  'input': '',\n",
              "  'output': '1. सन्तुलित आहार खानुहोस् र प्रशस्त फलफूल र तरकारीहरू समावेश गर्न निश्चित गर्नुहोस्।\\n2. आफ्नो शरीर सक्रिय र बलियो राख्न नियमित रूपमा व्यायाम गर्नुहोस्।\\n3. पर्याप्त निद्रा लिनुहोस् र एक सुत्ने तालिका कायम राख्नुहोस्।',\n",
              "  'id': 0},\n",
              " {'instruction': 'तीन प्राथमिक रंगहरू के हुन्?',\n",
              "  'input': '',\n",
              "  'output': 'तीन प्राथमिक रंगहरू रातो, नीलो र पहेंलो हुन्।',\n",
              "  'id': 1},\n",
              " {'instruction': 'परमाणुको संरचना वर्णन गर्नुहोस्।',\n",
              "  'input': '',\n",
              "  'output': 'परमाणु एक न्यूक्लियसबाट बनेको हुन्छ, जसमा प्रोटोन र न्यूट्रोनहरू हुन्छन्, इलेक्ट्रोनहरूले घेरिएको हुन्छ जुन न्यूक्लियसको वरिपरि परिक्रमा गर्दछ। प्रोटोन र न्यूट्रोनहरूमा सकारात्मक चार्ज हुन्छ, जबकि इलेक्ट्रोनहरूमा नकारात्मक चार्ज हुन्छ, परिणामस्वरूप समग्र तटस्थ परमाणु हुन्छ। प्रत्येक कणको संख्याले परमाणु संख्या र परमाणुको प्रकार निर्धारण गर्दछ।',\n",
              "  'id': 2},\n",
              " {'instruction': 'हामी वायु प्रदूषण कसरी कम गर्न सक्छौं?',\n",
              "  'input': '',\n",
              "  'output': 'नवीकरणीय ऊर्जा स्रोतहरूमा सर्ने, सार्वजनिक यातायातको प्रयोगलाई प्रोत्साहन गर्ने, जीवाश्म इन्धन जलाउन निषेध गर्ने, औद्योगिक स्रोतहरूबाट हुने उत्सर्जन कम गर्ने नीतिहरू लागू गर्ने, र सवारी साधनको उत्सर्जन मापदण्डहरू लागू गर्ने जस्ता वायु प्रदूषण कम गर्ने थुप्रै तरिकाहरू छन्। थप रूपमा, व्यक्तिहरूले कारको प्रयोग घटाएर, दाउरा जस्ता जलाउने सामग्रीहरू बेवास्ता गरेर, र ऊर्जा कुशल उपकरणहरूमा परिवर्तन गरेर वायु प्रदूषण कम गर्न आफ्नो भाग गर्न सक्छन्।',\n",
              "  'id': 3},\n",
              " {'instruction': 'तपाईंले कठिन निर्णय लिनु पर्ने समयको वर्णन गर्नुहोस्।',\n",
              "  'input': '',\n",
              "  'output': 'एउटा निर्माण कम्पनीमा परियोजना प्रबन्धकको रूपमा काम गर्दा मैले कठिन निर्णय लिनु परेको थियो। म ग्राहकको अपेक्षाहरू पूरा गर्नको लागि एक निश्चित मितिमा पूरा गर्न आवश्यक पर्ने परियोजनाको प्रभारी थिएँ। तर, अप्रत्याशित ढिलाइका कारण, हामीले समयसीमा पूरा गर्न सकेनौं र त्यसैले मैले कठिन निर्णय लिनु पर्यो। मैले म्याद लम्ब्याउने निर्णय गरें, तर मैले टोलीको स्रोतलाई अझ बढाएर बजेट बढाउनुपर्\\u200dयो। यद्यपि यो एक जोखिमपूर्ण निर्णय थियो, मैले अन्ततः परियोजना समयमै पूरा भएको र ग्राहकको अपेक्षाहरू पूरा भएको सुनिश्चित गर्न यसको साथ अगाडि बढ्ने निर्णय गरें। परियोजना अन्ततः सफलतापूर्वक सम्पन्न भयो र यो मेरो नेतृत्व र निर्णय गर्ने क्षमताको प्रमाणको रूपमा हेरिएको थियो।',\n",
              "  'id': 4},\n",
              " {'instruction': 'अनौठो बाहिर पहिचान गर्नुहोस्।',\n",
              "  'input': 'ट्विटर, इन्स्टाग्राम, टेलिग्राम',\n",
              "  'output': 'टेलिग्राम',\n",
              "  'id': 5},\n",
              " {'instruction': 'तलको अंश किन १/४ को बराबर छ भनेर व्याख्या गर्नुहोस्',\n",
              "  'input': '४/१६',\n",
              "  'output': 'अंश 4/16 1/4 को बराबर छ किनभने दुबै अंक र भाजकहरू 4 द्वारा भाग गर्न सकिन्छ। माथि र तल दुवै संख्याहरूलाई 4 ले भाग गर्दा अंश 1/4 प्राप्त हुन्छ।',\n",
              "  'id': 6},\n",
              " {'instruction': 'महत्त्वपूर्ण क्यारियर निर्णय लिनु पर्ने नायकको बारेमा तेस्रो व्यक्ति कथामा छोटो कथा लेख्नुहोस्।',\n",
              "  'input': '',\n",
              "  'output': 'जोन आफ्नो जीवनको एक चौराहेमा थिए। उसले भर्खर कलेजको स्नातक गरेको थियो र अब कुन क्यारियरको पछि लाग्ने भन्ने ठूलो निर्णयको सामना गरिरहेको थियो। धेरै सोचविचार पछि, उनले निर्णय गरे कि उनी लेखापाल बन्न र आर्थिक रूपमा विपन्नहरूलाई मद्दत गर्न चाहन्छन्। उहाँ सधैं नम्बरहरूमा राम्रो हुनुहुन्थ्यो र आफ्नो कामको मूर्त नतिजाहरू देखेर आनन्द लिनुभयो।\\n\\nजोनले लेखा पाठ्यक्रमहरूमा भर्ना गरे र सुरुमा यो धेरै चुनौतीपूर्ण पाए। उनले धेरै प्रणाली र नियमहरू चाँडै सिक्नुपर्\\u200dयो, तर उनले कडा मेहनत गरे र अन्ततः आफ्नो अध्ययनमा उत्कृष्ट भए। केही वर्षपछि जोनले आफ्नो सहरको एउटा लेखा फर्ममा काम गर्न थाले। उनी कर र लेखाको ज्ञानलाई वास्तविक संसारको सेटिङमा प्रयोग गर्न उत्सुक थिए।\\n\\nजोनले आफ्नो कामलाई माया गर्थे, किनकि यसले उसलाई आफ्ना ग्राहकहरूको पैसा बचत गर्न रणनीतिहरू खोज्नमा आफ्नो रचनात्मकता व्यक्त गर्न दिन्छ। फर्ममा केही वर्ष पछि, उनी एक वरिष्ठ लेखापाल बने र ठूला र थप चुनौतीपूर्ण केसहरू व्यवस्थापन गर्न भनियो। उहाँ अहिले वित्तीय उद्योगमा एक सम्मानित व्यक्तित्व हुनुहुन्थ्यो, तर उहाँ अझै पनि सम्झनुहुन्छ जब उहाँ भर्खरै कलेज स्नातक हुनुहुन्थ्यो, आफ्नो जीवनले उहाँलाई कुन दिशामा लैजान्छ भन्ने बारे अनिश्चित थिए।',\n",
              "  'id': 7},\n",
              " {'instruction': 'घरको 3D मोडेल रेन्डर गर्नुहोस्',\n",
              "  'input': '',\n",
              "  'output': '<nooutput> यस प्रकारको निर्देशन GPT मोडेलले पूरा गर्न सक्दैन।',\n",
              "  'id': 8},\n",
              " {'instruction': 'हिज्जे र व्याकरण त्रुटिहरूको लागि यो वाक्य मूल्याङ्कन गर्नुहोस्',\n",
              "  'input': 'उसले खाना पकाएर रेष्टुरेण्ट छोड्यो',\n",
              "  'output': 'खाना खाएर रेष्टुरेन्टबाट निस्कियो ।',\n",
              "  'id': 9}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcaaf606-f913-4445-8301-632ae10d387d",
      "metadata": {
        "id": "fcaaf606-f913-4445-8301-632ae10d387d"
      },
      "source": [
        "## 7.3 Organizing data into training batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233f63bd-9755-4d07-8884-5e2e5345cf27",
      "metadata": {
        "id": "233f63bd-9755-4d07-8884-5e2e5345cf27"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-2.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c",
      "metadata": {
        "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c"
      },
      "source": [
        "- We tackle this dataset batching in several steps, as summarized in the figure below\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/detailed-batching.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9af423f-aad9-4b3c-bea5-153021c04862",
      "metadata": {
        "id": "b9af423f-aad9-4b3c-bea5-153021c04862"
      },
      "source": [
        "- First, we implement an `InstructionDataset` class that pre-tokenizes all inputs in the dataset, similar to the `SpamDataset` in chapter 6\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/pretokenizing.webp\" width=500px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
      "metadata": {
        "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### प्रतिक्रिया:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            full_text = data_cleaner.clean_data(full_text)\n",
        "            encoded_text = tokenizer.encode(full_text)\n",
        "\n",
        "            # 512 is context length so only append encoded texts whose length is less than 512\n",
        "            if len(encoded_text)<=512:  # update\n",
        "              '''\n",
        "              `and len(encoded_text) >= 65`\n",
        "              setting min. length was to prevent english language entries (from wiseyak) but detecting language beforehand seems to be better approach.\n",
        "              '''\n",
        "              self.encoded_texts.append(\n",
        "                    encoded_text\n",
        "              )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)  # update\n",
        "        # return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MDz3-wPWnrJF",
      "metadata": {
        "id": "MDz3-wPWnrJF"
      },
      "outputs": [],
      "source": [
        "## load tokenizer\n",
        "import os\n",
        "import requests\n",
        "os.makedirs('NepaliBPE', exist_ok=True)\n",
        "\n",
        "res=requests.get(r\"https://raw.githubusercontent.com/Aananda-giri/GPT2-Nepali/main/2.%20tokenizer/NepaliBPE/tokenizer.json\")\n",
        "with open('NepaliBPE/tokenizer.json','w') as f:\n",
        "    f.write(res.text)\n",
        "\n",
        "res=requests.get(r\"https://raw.githubusercontent.com/Aananda-giri/GPT2-Nepali/main/2.%20tokenizer/NepaliBPE/tokenizer_config.json\")\n",
        "with open('NepaliBPE/tokenizer_config.json','w') as f:\n",
        "    f.write(res.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fHddZf6lodro",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "78dd1bf6d63344a294395ece64b58523",
            "c5a4d219898c420e89ba131c913690e4",
            "046c20ee19054545a7da0c2155ccc655",
            "4708e90d116748b2ada25c038626aeea",
            "43435e825a3542a5a1967bd2fa771b5a",
            "a25c7dc72fda44a5b26fd585d516eee4",
            "d7bb83a9396e4c9f8642c6b845764966",
            "115593a1493e4ff2877a71ba027b9153",
            "c6414a951d7945499da92576e2dc08d5",
            "5951e69c91f94ca38074e88b70b6022c",
            "5522f6e20e704a24a63c8b2fe57cb111"
          ]
        },
        "id": "fHddZf6lodro",
        "outputId": "2a5a67b8-410d-4f60-db56-3062c65d08a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78dd1bf6d63344a294395ece64b58523",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['राम</w>', 'ले</w>', 'भात</w>', 'खायो</w>', '।</w>']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load your tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('NepaliBPE')\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\"))\n",
        "tokenizer.tokenize('राम ले भात खायो। ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vzvqEDbOF995",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzvqEDbOF995",
        "outputId": "f6b8a896-2654-411f-e29b-5fd30ace67a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1685, 285, 12434, 24801, 276]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode('राम ले भात खायो।')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LnrOmoG5Ftk6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnrOmoG5Ftk6",
        "outputId": "29bfd5f1-94e7-4410-b8ad-3f58263beffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[31699, 23105]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer2 = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer2.encode(\"fuck hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m9ir2gDPuPUK",
      "metadata": {
        "id": "m9ir2gDPuPUK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('data.json','w') as f:\n",
        "  json.dump(data,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427",
      "metadata": {
        "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"
      },
      "source": [
        "- In chapter 6, we padded all examples in a dataset to the same length\n",
        "  - Here, we take a more sophisticated approach and develop a custom \"collate\" function that we can pass to the data loader\n",
        "  - This custom collate function pads the training examples in each batch to have the same length (but different batches can have different lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3",
      "metadata": {
        "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/padding.webp\" width=500px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yXlaITO9_SRM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXlaITO9_SRM",
        "outputId": "2823f422-8c22-4de3-9c1e-ae72e110d304"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[50256]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2.encode(\"<|endoftext|>\",allowed_special={'<|endoftext|>'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kvWff_gi_vEP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvWff_gi_vEP",
        "outputId": "dcce5eaa-9b66-4a90-e9ae-88e0d3684ac0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"<|endoftext|>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
      "metadata": {
        "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=0, # 0 is index of <|endoftext|> token for our tokenizer  (was 50256 for GPT2 tokenizer)\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    # and increase the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
        "outputId": "56a09a7c-fb1f-476d-94db-cdbc57112303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 0, 0, 0],\n",
            "        [7, 8, 9, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft_1(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b",
      "metadata": {
        "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-4.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17769a19-b961-4213-92ef-34f441b2d1d6",
      "metadata": {
        "id": "17769a19-b961-4213-92ef-34f441b2d1d6"
      },
      "source": [
        "- Above, we only returned the inputs to the LLM; however, for LLM training, we also need the target values\n",
        "- Similar to pretraining an LLM, the targets are the inputs shifted by 1 position to the right, so the LLM learns to predict the next token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef",
      "metadata": {
        "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/inputs-targets.webp?1\" width=400px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
      "metadata": {
        "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=0, # 0 is index of <|endoftext|> token for our tokenizer (was 50256 for GPT2 tokenizer)\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
        "outputId": "b5e137f6-ded9-44e2-9579-1509724d71de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 0, 0, 0],\n",
            "        [7, 8, 9, 0, 0]])\n",
            "tensor([[1, 2, 3, 4, 0],\n",
            "        [6, 0, 0, 0, 0],\n",
            "        [8, 9, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15",
      "metadata": {
        "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15"
      },
      "source": [
        "- Next, we introduce an `ignore_index` value to replace all padding token IDs with a new value; the purpose of this `ignore_index` is that we can ignore padding values in the loss function (more on that later)\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-5.webp?1\" width=500px>\n",
        "\n",
        "- Concretely, this means that we replace the token IDs corresponding to `50256` with `-100` as illustrated below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4bed33-956e-4b3f-a09c-586d8203109a",
      "metadata": {
        "id": "bd4bed33-956e-4b3f-a09c-586d8203109a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ignore-index.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5346513e-c3f4-44fe-af22-4ebd36497728",
      "metadata": {
        "id": "5346513e-c3f4-44fe-af22-4ebd36497728"
      },
      "source": [
        "- (In addition, we also introduce the `allowed_max_length` in case we want to limit the length of the samples; this will be useful if you plan to work with your own datasets that are longer than the 1024 token context size supported by the GPT-2 model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
      "metadata": {
        "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=0, # 0 is index of <|endoftext|> token for our tokenizer (was 50256 for GPT2 tokenizer)\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
        "outputId": "ab24d579-bb57-4a18-b9f1-3f4ceca5e579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 0, 0, 0],\n",
            "        [7, 8, 9, 0, 0]])\n",
            "tensor([[   1,    2,    3,    4,    0],\n",
            "        [   6,    0, -100, -100, -100],\n",
            "        [   8,    9,    0, -100, -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7",
      "metadata": {
        "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7"
      },
      "source": [
        "- Let's see what this replacement by -100 accomplishes\n",
        "- For illustration purposes, let's assume we have a small classification task with 2 class labels, 0 and 1, similar to chapter 6\n",
        "- If we have the following logits values (outputs of the last layer of the model), we calculate the following loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524",
      "metadata": {
        "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524"
      },
      "source": [
        "- In practice, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in the figure below (this is a recommended reader exercise after completing the chapter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39",
      "metadata": {
        "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp?1\" width=600px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
      "metadata": {
        "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
      },
      "source": [
        "## 7.4 Creating data loaders for an instruction dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50",
      "metadata": {
        "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50"
      },
      "source": [
        "- In this section, we use the `InstructionDataset` class and `custom_collate_fn` function to instantiate the training, validation, and test data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fffe390-b226-4d5c-983f-9f4da773cb82",
      "metadata": {
        "id": "9fffe390-b226-4d5c-983f-9f4da773cb82"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-3.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932677e9-9317-42e8-b461-7b0269518f97",
      "metadata": {
        "id": "932677e9-9317-42e8-b461-7b0269518f97"
      },
      "source": [
        "- Another additional detail of the previous `custom_collate_fn` function is that we now directly move the data to the target device (e.g., GPU) instead of doing it in the main training loop, which improves efficiency because it can be carried out as a background process when we use the `custom_collate_fn` as part of the data loader\n",
        "- Using the `partial` function from Python's `functools` standard library, we create a new function with the `device` argument of the original function pre-filled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "etpqqWh8phKc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etpqqWh8phKc",
        "outputId": "801e7fcb-c856-4110-fc5e-6fcf6f44ade2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
      "metadata": {
        "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
      "metadata": {
        "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
      },
      "source": [
        "- Next, we instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BtWkgir6Hlpe",
      "metadata": {
        "id": "BtWkgir6Hlpe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
      "metadata": {
        "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
      },
      "outputs": [],
      "source": [
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pbYYJw7JO90h",
      "metadata": {
        "id": "pbYYJw7JO90h"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xNYfKjpOO_Dl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNYfKjpOO_Dl",
        "outputId": "0017fefc-bd01-4c52-8e5d-d76441eb1ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "for data in train_loader:\n",
        "  print(print(data[0].shape, data[1].shape))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fVdRuzoFBhHJ",
      "metadata": {
        "id": "fVdRuzoFBhHJ"
      },
      "outputs": [],
      "source": [
        "# # Todo this is giving error\n",
        "# assert inputs.shape[0] == 8, f\"input shape 0 {inputs.shape[0]}\"\n",
        "# assert inputs.shape[1] < 300, f\"input shape 1 {inputs.shape[1]}\"\n",
        "# assert targets.shape[0] == 8, f\"input shape 0 {inputs.shape[0]}\"\n",
        "# assert targets.shape[1] < 300, f\"input shape 1 {inputs.shape[1]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
      "metadata": {
        "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
      },
      "source": [
        "- Let's see what the dimensions of the resulting input and target batches look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GGs1AI3vHpnX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGs1AI3vHpnX",
        "outputId": "2990b05d-4363-4269-c84f-f001ca88bcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 319]) torch.Size([8, 319])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 414]) torch.Size([8, 414])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 247]) torch.Size([8, 247])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 290]) torch.Size([8, 290])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 348]) torch.Size([8, 348])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 304]) torch.Size([8, 304])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 323]) torch.Size([8, 323])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 391]) torch.Size([8, 391])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 310]) torch.Size([8, 310])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 270]) torch.Size([8, 270])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 349]) torch.Size([8, 349])\n",
            "torch.Size([8, 283]) torch.Size([8, 283])\n",
            "torch.Size([8, 373]) torch.Size([8, 373])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 365]) torch.Size([8, 365])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 330]) torch.Size([8, 330])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 276]) torch.Size([8, 276])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 369]) torch.Size([8, 369])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 224]) torch.Size([8, 224])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 371]) torch.Size([8, 371])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 402]) torch.Size([8, 402])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 300]) torch.Size([8, 300])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 287]) torch.Size([8, 287])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 296]) torch.Size([8, 296])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 334]) torch.Size([8, 334])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 418]) torch.Size([8, 418])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 265]) torch.Size([8, 265])\n",
            "torch.Size([8, 278]) torch.Size([8, 278])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 55]) torch.Size([8, 55])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 298]) torch.Size([8, 298])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 342]) torch.Size([8, 342])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 501]) torch.Size([8, 501])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 343]) torch.Size([8, 343])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 475]) torch.Size([8, 475])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 323]) torch.Size([8, 323])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 333]) torch.Size([8, 333])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 282]) torch.Size([8, 282])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 378]) torch.Size([8, 378])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 437]) torch.Size([8, 437])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 347]) torch.Size([8, 347])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 475]) torch.Size([8, 475])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 224]) torch.Size([8, 224])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 308]) torch.Size([8, 308])\n",
            "torch.Size([8, 372]) torch.Size([8, 372])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 289]) torch.Size([8, 289])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 360]) torch.Size([8, 360])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 330]) torch.Size([8, 330])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 302]) torch.Size([8, 302])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 348]) torch.Size([8, 348])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 388]) torch.Size([8, 388])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 227]) torch.Size([8, 227])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 318]) torch.Size([8, 318])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 330]) torch.Size([8, 330])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 234]) torch.Size([8, 234])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 263]) torch.Size([8, 263])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 317]) torch.Size([8, 317])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 335]) torch.Size([8, 335])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 452]) torch.Size([8, 452])\n",
            "torch.Size([8, 401]) torch.Size([8, 401])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 311]) torch.Size([8, 311])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 297]) torch.Size([8, 297])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 227]) torch.Size([8, 227])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 248]) torch.Size([8, 248])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 335]) torch.Size([8, 335])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 356]) torch.Size([8, 356])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 329]) torch.Size([8, 329])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 399]) torch.Size([8, 399])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 263]) torch.Size([8, 263])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 304]) torch.Size([8, 304])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 310]) torch.Size([8, 310])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 439]) torch.Size([8, 439])\n",
            "torch.Size([8, 334]) torch.Size([8, 334])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 300]) torch.Size([8, 300])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 334]) torch.Size([8, 334])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 378]) torch.Size([8, 378])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 259]) torch.Size([8, 259])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 465]) torch.Size([8, 465])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 259]) torch.Size([8, 259])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 275]) torch.Size([8, 275])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 309]) torch.Size([8, 309])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 366]) torch.Size([8, 366])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 276]) torch.Size([8, 276])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 384]) torch.Size([8, 384])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 310]) torch.Size([8, 310])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 318]) torch.Size([8, 318])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 306]) torch.Size([8, 306])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 321]) torch.Size([8, 321])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 248]) torch.Size([8, 248])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 274]) torch.Size([8, 274])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 227]) torch.Size([8, 227])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 334]) torch.Size([8, 334])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 377]) torch.Size([8, 377])\n",
            "torch.Size([8, 259]) torch.Size([8, 259])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 333]) torch.Size([8, 333])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 288]) torch.Size([8, 288])\n",
            "torch.Size([8, 276]) torch.Size([8, 276])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 340]) torch.Size([8, 340])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 327]) torch.Size([8, 327])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 270]) torch.Size([8, 270])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 318]) torch.Size([8, 318])\n",
            "torch.Size([8, 247]) torch.Size([8, 247])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 227]) torch.Size([8, 227])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 355]) torch.Size([8, 355])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 327]) torch.Size([8, 327])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 348]) torch.Size([8, 348])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 290]) torch.Size([8, 290])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 278]) torch.Size([8, 278])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 380]) torch.Size([8, 380])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 313]) torch.Size([8, 313])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 311]) torch.Size([8, 311])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 268]) torch.Size([8, 268])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 275]) torch.Size([8, 275])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 429]) torch.Size([8, 429])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 275]) torch.Size([8, 275])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 343]) torch.Size([8, 343])\n",
            "torch.Size([8, 410]) torch.Size([8, 410])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 409]) torch.Size([8, 409])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 350]) torch.Size([8, 350])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 386]) torch.Size([8, 386])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 56]) torch.Size([8, 56])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 329]) torch.Size([8, 329])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 304]) torch.Size([8, 304])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 401]) torch.Size([8, 401])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 288]) torch.Size([8, 288])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 391]) torch.Size([8, 391])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 283]) torch.Size([8, 283])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 416]) torch.Size([8, 416])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 334]) torch.Size([8, 334])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 353]) torch.Size([8, 353])\n",
            "torch.Size([8, 278]) torch.Size([8, 278])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 368]) torch.Size([8, 368])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 311]) torch.Size([8, 311])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 258]) torch.Size([8, 258])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 224]) torch.Size([8, 224])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 274]) torch.Size([8, 274])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 258]) torch.Size([8, 258])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 282]) torch.Size([8, 282])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 326]) torch.Size([8, 326])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 318]) torch.Size([8, 318])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 294]) torch.Size([8, 294])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 393]) torch.Size([8, 393])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 300]) torch.Size([8, 300])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 273]) torch.Size([8, 273])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 224]) torch.Size([8, 224])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 308]) torch.Size([8, 308])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 297]) torch.Size([8, 297])\n",
            "torch.Size([8, 287]) torch.Size([8, 287])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 349]) torch.Size([8, 349])\n",
            "torch.Size([8, 389]) torch.Size([8, 389])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 301]) torch.Size([8, 301])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 359]) torch.Size([8, 359])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 296]) torch.Size([8, 296])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 270]) torch.Size([8, 270])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 285]) torch.Size([8, 285])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 323]) torch.Size([8, 323])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 392]) torch.Size([8, 392])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 332]) torch.Size([8, 332])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 305]) torch.Size([8, 305])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 302]) torch.Size([8, 302])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 296]) torch.Size([8, 296])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 311]) torch.Size([8, 311])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 279]) torch.Size([8, 279])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 331]) torch.Size([8, 331])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 483]) torch.Size([8, 483])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 406]) torch.Size([8, 406])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 296]) torch.Size([8, 296])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 368]) torch.Size([8, 368])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 253]) torch.Size([8, 253])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 302]) torch.Size([8, 302])\n",
            "torch.Size([8, 447]) torch.Size([8, 447])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 387]) torch.Size([8, 387])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 287]) torch.Size([8, 287])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 288]) torch.Size([8, 288])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 471]) torch.Size([8, 471])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 371]) torch.Size([8, 371])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 434]) torch.Size([8, 434])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 367]) torch.Size([8, 367])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 234]) torch.Size([8, 234])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 265]) torch.Size([8, 265])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 339]) torch.Size([8, 339])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 375]) torch.Size([8, 375])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 393]) torch.Size([8, 393])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 449]) torch.Size([8, 449])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 459]) torch.Size([8, 459])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 247]) torch.Size([8, 247])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 294]) torch.Size([8, 294])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 325]) torch.Size([8, 325])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 357]) torch.Size([8, 357])\n",
            "torch.Size([8, 341]) torch.Size([8, 341])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 319]) torch.Size([8, 319])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 344]) torch.Size([8, 344])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 330]) torch.Size([8, 330])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 300]) torch.Size([8, 300])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 394]) torch.Size([8, 394])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 454]) torch.Size([8, 454])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 366]) torch.Size([8, 366])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 408]) torch.Size([8, 408])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 300]) torch.Size([8, 300])\n",
            "torch.Size([8, 317]) torch.Size([8, 317])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 325]) torch.Size([8, 325])\n",
            "torch.Size([8, 357]) torch.Size([8, 357])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 417]) torch.Size([8, 417])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 43]) torch.Size([8, 43])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 322]) torch.Size([8, 322])\n",
            "torch.Size([8, 382]) torch.Size([8, 382])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 274]) torch.Size([8, 274])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 468]) torch.Size([8, 468])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 234]) torch.Size([8, 234])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 279]) torch.Size([8, 279])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 357]) torch.Size([8, 357])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 320]) torch.Size([8, 320])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 350]) torch.Size([8, 350])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 275]) torch.Size([8, 275])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 317]) torch.Size([8, 317])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 339]) torch.Size([8, 339])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 237]) torch.Size([8, 237])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 258]) torch.Size([8, 258])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 381]) torch.Size([8, 381])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 288]) torch.Size([8, 288])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 298]) torch.Size([8, 298])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 248]) torch.Size([8, 248])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 338]) torch.Size([8, 338])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 290]) torch.Size([8, 290])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 285]) torch.Size([8, 285])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 494]) torch.Size([8, 494])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 301]) torch.Size([8, 301])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 339]) torch.Size([8, 339])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 256]) torch.Size([8, 256])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 376]) torch.Size([8, 376])\n",
            "torch.Size([8, 337]) torch.Size([8, 337])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 263]) torch.Size([8, 263])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 257]) torch.Size([8, 257])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 434]) torch.Size([8, 434])\n",
            "torch.Size([8, 352]) torch.Size([8, 352])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 307]) torch.Size([8, 307])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 247]) torch.Size([8, 247])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 321]) torch.Size([8, 321])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 315]) torch.Size([8, 315])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 320]) torch.Size([8, 320])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 395]) torch.Size([8, 395])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 319]) torch.Size([8, 319])\n",
            "torch.Size([8, 401]) torch.Size([8, 401])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 253]) torch.Size([8, 253])\n",
            "torch.Size([8, 286]) torch.Size([8, 286])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 371]) torch.Size([8, 371])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 364]) torch.Size([8, 364])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 294]) torch.Size([8, 294])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 288]) torch.Size([8, 288])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 277]) torch.Size([8, 277])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 336]) torch.Size([8, 336])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 333]) torch.Size([8, 333])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 353]) torch.Size([8, 353])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 275]) torch.Size([8, 275])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 286]) torch.Size([8, 286])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 401]) torch.Size([8, 401])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 337]) torch.Size([8, 337])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 346]) torch.Size([8, 346])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 263]) torch.Size([8, 263])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 317]) torch.Size([8, 317])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 234]) torch.Size([8, 234])\n",
            "torch.Size([8, 403]) torch.Size([8, 403])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 253]) torch.Size([8, 253])\n",
            "torch.Size([8, 185]) torch.Size([8, 185])\n",
            "torch.Size([8, 305]) torch.Size([8, 305])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 248]) torch.Size([8, 248])\n",
            "torch.Size([8, 372]) torch.Size([8, 372])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 378]) torch.Size([8, 378])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 394]) torch.Size([8, 394])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 293]) torch.Size([8, 293])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 267]) torch.Size([8, 267])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 360]) torch.Size([8, 360])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 200]) torch.Size([8, 200])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 276]) torch.Size([8, 276])\n",
            "torch.Size([8, 428]) torch.Size([8, 428])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 385]) torch.Size([8, 385])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 262]) torch.Size([8, 262])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 243]) torch.Size([8, 243])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 374]) torch.Size([8, 374])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 55]) torch.Size([8, 55])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 283]) torch.Size([8, 283])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 337]) torch.Size([8, 337])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 442]) torch.Size([8, 442])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 273]) torch.Size([8, 273])\n",
            "torch.Size([8, 390]) torch.Size([8, 390])\n",
            "torch.Size([8, 393]) torch.Size([8, 393])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 328]) torch.Size([8, 328])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 316]) torch.Size([8, 316])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 206]) torch.Size([8, 206])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 191]) torch.Size([8, 191])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 226]) torch.Size([8, 226])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 279]) torch.Size([8, 279])\n",
            "torch.Size([8, 247]) torch.Size([8, 247])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 198]) torch.Size([8, 198])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 308]) torch.Size([8, 308])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 436]) torch.Size([8, 436])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 333]) torch.Size([8, 333])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 240]) torch.Size([8, 240])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 253]) torch.Size([8, 253])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 184]) torch.Size([8, 184])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 272]) torch.Size([8, 272])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 231]) torch.Size([8, 231])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 270]) torch.Size([8, 270])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 491]) torch.Size([8, 491])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 224]) torch.Size([8, 224])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 238]) torch.Size([8, 238])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 199]) torch.Size([8, 199])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 217]) torch.Size([8, 217])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 358]) torch.Size([8, 358])\n",
            "torch.Size([8, 246]) torch.Size([8, 246])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 339]) torch.Size([8, 339])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 232]) torch.Size([8, 232])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 292]) torch.Size([8, 292])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 258]) torch.Size([8, 258])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 221]) torch.Size([8, 221])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 299]) torch.Size([8, 299])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 313]) torch.Size([8, 313])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 228]) torch.Size([8, 228])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 303]) torch.Size([8, 303])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 249]) torch.Size([8, 249])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 236]) torch.Size([8, 236])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 279]) torch.Size([8, 279])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 342]) torch.Size([8, 342])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 306]) torch.Size([8, 306])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 111]) torch.Size([8, 111])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 439]) torch.Size([8, 439])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 127]) torch.Size([8, 127])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 318]) torch.Size([8, 318])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 380]) torch.Size([8, 380])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 310]) torch.Size([8, 310])\n",
            "torch.Size([8, 223]) torch.Size([8, 223])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 233]) torch.Size([8, 233])\n",
            "torch.Size([8, 251]) torch.Size([8, 251])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 269]) torch.Size([8, 269])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 369]) torch.Size([8, 369])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 295]) torch.Size([8, 295])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 219]) torch.Size([8, 219])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 227]) torch.Size([8, 227])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 142]) torch.Size([8, 142])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 290]) torch.Size([8, 290])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 105]) torch.Size([8, 105])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 193]) torch.Size([8, 193])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 395]) torch.Size([8, 395])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 466]) torch.Size([8, 466])\n",
            "torch.Size([8, 338]) torch.Size([8, 338])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 201]) torch.Size([8, 201])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 214]) torch.Size([8, 214])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 212]) torch.Size([8, 212])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 282]) torch.Size([8, 282])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 186]) torch.Size([8, 186])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 177]) torch.Size([8, 177])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 341]) torch.Size([8, 341])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 234]) torch.Size([8, 234])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 118]) torch.Size([8, 118])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 278]) torch.Size([8, 278])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 172]) torch.Size([8, 172])\n",
            "torch.Size([8, 355]) torch.Size([8, 355])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 242]) torch.Size([8, 242])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 282]) torch.Size([8, 282])\n",
            "torch.Size([8, 241]) torch.Size([8, 241])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 252]) torch.Size([8, 252])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 281]) torch.Size([8, 281])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 174]) torch.Size([8, 174])\n",
            "torch.Size([8, 176]) torch.Size([8, 176])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 181]) torch.Size([8, 181])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 225]) torch.Size([8, 225])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 146]) torch.Size([8, 146])\n",
            "torch.Size([8, 278]) torch.Size([8, 278])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 163]) torch.Size([8, 163])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 325]) torch.Size([8, 325])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 220]) torch.Size([8, 220])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 433]) torch.Size([8, 433])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 285]) torch.Size([8, 285])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 230]) torch.Size([8, 230])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 266]) torch.Size([8, 266])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 302]) torch.Size([8, 302])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 99]) torch.Size([8, 99])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 145]) torch.Size([8, 145])\n",
            "torch.Size([8, 244]) torch.Size([8, 244])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 213]) torch.Size([8, 213])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 447]) torch.Size([8, 447])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 158]) torch.Size([8, 158])\n",
            "torch.Size([8, 110]) torch.Size([8, 110])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 216]) torch.Size([8, 216])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 113]) torch.Size([8, 113])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 260]) torch.Size([8, 260])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 141]) torch.Size([8, 141])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 166]) torch.Size([8, 166])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 160]) torch.Size([8, 160])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 197]) torch.Size([8, 197])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 109]) torch.Size([8, 109])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 291]) torch.Size([8, 291])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 102]) torch.Size([8, 102])\n",
            "torch.Size([8, 124]) torch.Size([8, 124])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 95]) torch.Size([8, 95])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 245]) torch.Size([8, 245])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 280]) torch.Size([8, 280])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 194]) torch.Size([8, 194])\n",
            "torch.Size([8, 204]) torch.Size([8, 204])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 150]) torch.Size([8, 150])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 207]) torch.Size([8, 207])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 264]) torch.Size([8, 264])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 195]) torch.Size([8, 195])\n",
            "torch.Size([8, 279]) torch.Size([8, 279])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 137]) torch.Size([8, 137])\n",
            "torch.Size([8, 274]) torch.Size([8, 274])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n",
            "torch.Size([8, 123]) torch.Size([8, 123])\n",
            "torch.Size([8, 271]) torch.Size([8, 271])\n",
            "torch.Size([8, 346]) torch.Size([8, 346])\n",
            "torch.Size([8, 132]) torch.Size([8, 132])\n",
            "torch.Size([8, 192]) torch.Size([8, 192])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 94]) torch.Size([8, 94])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 203]) torch.Size([8, 203])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 125]) torch.Size([8, 125])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 218]) torch.Size([8, 218])\n",
            "torch.Size([8, 183]) torch.Size([8, 183])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 305]) torch.Size([8, 305])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 130]) torch.Size([8, 130])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 129]) torch.Size([8, 129])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 115]) torch.Size([8, 115])\n",
            "torch.Size([8, 170]) torch.Size([8, 170])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 128]) torch.Size([8, 128])\n",
            "torch.Size([8, 187]) torch.Size([8, 187])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 161]) torch.Size([8, 161])\n",
            "torch.Size([8, 168]) torch.Size([8, 168])\n",
            "torch.Size([8, 284]) torch.Size([8, 284])\n",
            "torch.Size([8, 215]) torch.Size([8, 215])\n",
            "torch.Size([8, 149]) torch.Size([8, 149])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 295]) torch.Size([8, 295])\n",
            "torch.Size([8, 222]) torch.Size([8, 222])\n",
            "torch.Size([8, 155]) torch.Size([8, 155])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 205]) torch.Size([8, 205])\n",
            "torch.Size([8, 152]) torch.Size([8, 152])\n",
            "torch.Size([8, 189]) torch.Size([8, 189])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 211]) torch.Size([8, 211])\n",
            "torch.Size([8, 190]) torch.Size([8, 190])\n",
            "torch.Size([8, 175]) torch.Size([8, 175])\n",
            "torch.Size([8, 135]) torch.Size([8, 135])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 320]) torch.Size([8, 320])\n",
            "torch.Size([8, 134]) torch.Size([8, 134])\n",
            "torch.Size([8, 106]) torch.Size([8, 106])\n",
            "torch.Size([8, 114]) torch.Size([8, 114])\n",
            "torch.Size([8, 162]) torch.Size([8, 162])\n",
            "torch.Size([8, 147]) torch.Size([8, 147])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 179]) torch.Size([8, 179])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 169]) torch.Size([8, 169])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 107]) torch.Size([8, 107])\n",
            "torch.Size([8, 254]) torch.Size([8, 254])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 250]) torch.Size([8, 250])\n",
            "torch.Size([8, 154]) torch.Size([8, 154])\n",
            "torch.Size([8, 108]) torch.Size([8, 108])\n",
            "torch.Size([8, 301]) torch.Size([8, 301])\n",
            "torch.Size([8, 117]) torch.Size([8, 117])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 346]) torch.Size([8, 346])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 144]) torch.Size([8, 144])\n",
            "torch.Size([8, 208]) torch.Size([8, 208])\n",
            "torch.Size([8, 367]) torch.Size([8, 367])\n",
            "torch.Size([8, 180]) torch.Size([8, 180])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 287]) torch.Size([8, 287])\n",
            "torch.Size([8, 121]) torch.Size([8, 121])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 188]) torch.Size([8, 188])\n",
            "torch.Size([8, 235]) torch.Size([8, 235])\n",
            "torch.Size([8, 139]) torch.Size([8, 139])\n",
            "torch.Size([8, 143]) torch.Size([8, 143])\n",
            "torch.Size([8, 182]) torch.Size([8, 182])\n",
            "torch.Size([8, 357]) torch.Size([8, 357])\n",
            "torch.Size([8, 165]) torch.Size([8, 165])\n",
            "torch.Size([8, 116]) torch.Size([8, 116])\n",
            "torch.Size([8, 126]) torch.Size([8, 126])\n",
            "torch.Size([8, 133]) torch.Size([8, 133])\n",
            "torch.Size([8, 112]) torch.Size([8, 112])\n",
            "torch.Size([8, 297]) torch.Size([8, 297])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 103]) torch.Size([8, 103])\n",
            "torch.Size([8, 229]) torch.Size([8, 229])\n",
            "torch.Size([8, 309]) torch.Size([8, 309])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 138]) torch.Size([8, 138])\n",
            "torch.Size([8, 164]) torch.Size([8, 164])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 101]) torch.Size([8, 101])\n",
            "torch.Size([8, 263]) torch.Size([8, 263])\n",
            "torch.Size([8, 178]) torch.Size([8, 178])\n",
            "torch.Size([8, 156]) torch.Size([8, 156])\n",
            "torch.Size([8, 122]) torch.Size([8, 122])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 104]) torch.Size([8, 104])\n",
            "torch.Size([8, 239]) torch.Size([8, 239])\n",
            "torch.Size([8, 202]) torch.Size([8, 202])\n",
            "torch.Size([8, 157]) torch.Size([8, 157])\n",
            "torch.Size([8, 173]) torch.Size([8, 173])\n",
            "torch.Size([8, 151]) torch.Size([8, 151])\n",
            "torch.Size([8, 159]) torch.Size([8, 159])\n",
            "torch.Size([8, 209]) torch.Size([8, 209])\n",
            "torch.Size([8, 140]) torch.Size([8, 140])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 167]) torch.Size([8, 167])\n",
            "torch.Size([8, 196]) torch.Size([8, 196])\n",
            "torch.Size([8, 119]) torch.Size([8, 119])\n",
            "torch.Size([8, 290]) torch.Size([8, 290])\n",
            "torch.Size([8, 273]) torch.Size([8, 273])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 136]) torch.Size([8, 136])\n",
            "torch.Size([8, 261]) torch.Size([8, 261])\n",
            "torch.Size([8, 255]) torch.Size([8, 255])\n",
            "torch.Size([8, 210]) torch.Size([8, 210])\n",
            "torch.Size([8, 153]) torch.Size([8, 153])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 120]) torch.Size([8, 120])\n",
            "torch.Size([8, 131]) torch.Size([8, 131])\n",
            "torch.Size([8, 171]) torch.Size([8, 171])\n",
            "torch.Size([8, 148]) torch.Size([8, 148])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "count=0\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)\n",
        "    assert inputs.shape[0] == 8, f\"input shape 0 {inputs.shape[0]}\"\n",
        "    # assert inputs.shape[1] < 1000, f\"input shape 1 {inputs.shape[1]}\"\n",
        "    assert targets.shape[0] == 8, f\"input shape 0 {inputs.shape[0]}\"\n",
        "    # assert targets.shape[1] < 1000, f\"input shape 1 {inputs.shape[1]}\"\n",
        "    # count +=1\n",
        "    # if count>10:break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657",
      "metadata": {
        "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657"
      },
      "source": [
        "- As we can see based on the output above, all batches have a batch size of 8 but a different length, as expected\n",
        "- Let's also double-check that the inputs contain the `<|endoftext|>` padding tokens corresponding to token ID 50256 by printing the contents of the first training example in the `inputs` batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
        "outputId": "a8bd5910-1d44-471e-cdf3-ea104b69bb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 4051,   325,  8808,   298,  1771,   280,  3980,  4159,  4292,  8695,\n",
            "          276,  4159,   468,  2386, 20031,  5929, 16108,  9209,   276,  4159,\n",
            "         3307, 10210, 14013, 45678,  9722,   161,   573,   378,   247, 22188,\n",
            "        35512,  2627,   353,    59,  4511, 11969,   403,   276,  2384,  5450,\n",
            "         3072,  3131,   293,   691,  3949,  9021, 13332,  1889,   334,  2897,\n",
            "        17199, 40446,   691,  3055,  4330,   276,   330,  1339, 28112,  8831,\n",
            "        10344,  4074,   161,  6185,   972, 11049,   186,   276,  2384,  9989,\n",
            "        32023,  1088,  5929, 10521,   186,  1785, 10878,   536,  5845,  5845,\n",
            "          520,   276,  2384,   363,  6955,  6492,   290,  1662,   536,  6060,\n",
            "        35538,  5682, 10140,   501,   520,   911,  5425, 10739,   161,  2386,\n",
            "         1834,   691,   334,   276,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(inputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360",
      "metadata": {
        "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360"
      },
      "source": [
        "- Similarly, we visually double-check that the targets contain the -100 placeholder tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
        "outputId": "c9fd01c7-3cca-4e8b-f692-4ea941ee721d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  325,  8808,   298,  1771,   280,  3980,  4159,  4292,  8695,   276,\n",
            "         4159,   468,  2386, 20031,  5929, 16108,  9209,   276,  4159,  3307,\n",
            "        10210, 14013, 45678,  9722,   161,   573,   378,   247, 22188, 35512,\n",
            "         2627,   353,    59,  4511, 11969,   403,   276,  2384,  5450,  3072,\n",
            "         3131,   293,   691,  3949,  9021, 13332,  1889,   334,  2897, 17199,\n",
            "        40446,   691,  3055,  4330,   276,   330,  1339, 28112,  8831, 10344,\n",
            "         4074,   161,  6185,   972, 11049,   186,   276,  2384,  9989, 32023,\n",
            "         1088,  5929, 10521,   186,  1785, 10878,   536,  5845,  5845,   520,\n",
            "          276,  2384,   363,  6955,  6492,   290,  1662,   536,  6060, 35538,\n",
            "         5682, 10140,   501,   520,   911,  5425, 10739,   161,  2386,  1834,\n",
            "          691,   334,   276,     0,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(targets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
      "metadata": {
        "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
      },
      "source": [
        "## 7.5 Loading a pretrained LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b",
      "metadata": {
        "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b"
      },
      "source": [
        "- In this section, we load a pretrained GPT model using the same code that we used in section 5.5 of chapter 5 and section 6.4 in chapter 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4",
      "metadata": {
        "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-4.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2",
      "metadata": {
        "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2"
      },
      "source": [
        "- However, instead of loading the smallest 124 million parameter model, we load the medium version with 355 million parameters since the 124 million model is too small for achieving qualitatively reasonable results via instruction finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821H2j8-wA-W",
      "metadata": {
        "id": "821H2j8-wA-W"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "# modified.\n",
        "res = requests.get('https://raw.githubusercontent.com/Aananda-giri/GPT2-Nepali/main/3.%20GPT2-Nepali/2_inference/gpt_model_code.py')\n",
        "with open('gpt_model_code.py','w') as f:\n",
        "    f.write(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uP8QGusPw_hF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "83e7f4df09c947bba4651a57e245caec",
            "59b7603d166747a6b4ec5c8b2663615f",
            "7840bb35972d49d7aabecc3679310a39",
            "dbefb28230734a9396970295b8aa40c1",
            "16a535ce3a0540d38427b848d7d9f724",
            "c5a7d47d2c484afda438f4d30e384353",
            "e6d5b704defd43109d8e504d405ec770",
            "3c03289a162f4665ad9da7eed76a5629",
            "e000427fcdbc46fa9c2f8ec9765f6a24",
            "0f68fdc697a24efd8247482209190015",
            "381ee4c0cc554816a066d3ee3c8d21ea",
            "902d74e63c904d409d9d38177cd98611",
            "f0dd48dd4af84d108816da0f859a9437",
            "5dd7c59ad9d0400ca307f0a94bdd5054",
            "28b6460138ab483f8739217089037bf3",
            "71e3df1690e94ef8a40e10c5ddbf55a7",
            "f4ed7c352b2c46c4ae7fcc9c4912a9c5",
            "ca3d854bc60b44fb98916cfba03ff82e",
            "f52e57167237411683e53f80b7111d37",
            "4f6ec3d692f943e4843b01c93d3d85b4",
            "eb6e39930e854ebe857f6854c3ac91cf",
            "a0de7080f8d84025a8c1c6c8f80f4f13"
          ]
        },
        "id": "uP8QGusPw_hF",
        "outputId": "adedfa68-82b1-4f75-93dd-283220e29641"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e7f4df09c947bba4651a57e245caec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "902d74e63c904d409d9d38177cd98611",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/661M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "रामले भात दाल र तरकारी गरी दुई लाख २० हजार रुपैयाँ पाएका छन् ।\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from gpt_model_code import GPTModel, GPT_CONFIG_124M, generate\n",
        "\n",
        "\n",
        "# load the model\n",
        "# ----------------------------\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# checkpoint = torch.load('/kaggle/input/sebastian-v4/model_checkpoints/model_pg_190000_steps.pth', weights_only=False)\n",
        "# # modified (added model loading code)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "model = GPTModel.from_pretrained(\"Aananda-giri/GPT2-Nepali\")\n",
        "model.to(device)\n",
        "\n",
        "# load the tokenizer\n",
        "# ----------------------------\n",
        "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "\n",
        "\n",
        "# generate a sample\n",
        "# ----------------------------\n",
        "\n",
        "prompt = \"रामले भात\"\n",
        "\n",
        "generated_text = generate(\n",
        "    model,\n",
        "    prompt,\n",
        "    tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=None,  # New parameter for nucleus sampling\n",
        "    eos_id=None,\n",
        "    repetition_penalty=1.2,\n",
        "    penalize_len_below=50\n",
        ")\n",
        "\n",
        "model.eval();\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
      "metadata": {
        "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
      },
      "source": [
        "- Before we start finetuning the model in the next section, let's see how it performs on one of the validation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kPzOnCA-1now",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPzOnCA-1now",
        "outputId": "472e07de-6132-4bf2-f64f-dcdce191dd52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'थप रूपमा, इनपुटमा जनावरको सबैभन्दा प्रभावशाली विशेषताहरू मध्ये 5 सूचीबद्ध गर्नुहोस्',\n",
              " 'input': 'जनावरः पेन्गुइन',\n",
              " 'output': '1) पेन्गुइनहरू निपुण पौडी खेल्नेहरू हुन् र 22 माइल प्रति घण्टासम्मको गतिमा पुग्न सक्छन्।\\n\\n2) तिनीहरूसँग विशेष पंखहरू छन् जसले न्यानो, जलरोधक, र पौडी खेल्नका लागि सुव्यवस्थित रूप प्रदान गर्दछ।\\n\\n3) पेन्गुइनहरू सामाजिक जनावरहरू हुन् र हजारौँ चराहरू समावेश गर्न सक्ने रोकरीज भनिने उपनिवेशहरूमा बस्छन्।\\n\\n4) तिनीहरूसँग एक अद्वितीय संभोग अनुष्ठान छ जहाँ तिनीहरूले साथीलाई आकर्षित गर्न प्रेमालाप प्रदर्शन गर्छन्।\\n\\n5) तिनीहरू चिसो अन्टार्कटिकादेखि दक्षिण अमेरिका, अस्ट्रेलिया र अफ्रिकाको समशीतोष्ण तटीय क्षेत्रहरूमा बस्ने प्रजातिहरूसहित चरम वातावरणमा अनुकूल हुनमा असाधारण छन्।'}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
        "outputId": "c73b2829-9170-46bf-b188-8f76d690e824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "थप रूपमा, इनपुटमा जनावरको सबैभन्दा प्रभावशाली विशेषताहरू मध्ये 5 सूचीबद्ध गर्नुहोस्\n",
            "\n",
            "### इनपुट:\n",
            "जनावरः पेन्गुइन\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7jSWaK0tWEXX",
      "metadata": {
        "id": "7jSWaK0tWEXX"
      },
      "outputs": [],
      "source": [
        "pretrain_simple_code = '''\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R9shGC3O0drk",
      "metadata": {
        "id": "R9shGC3O0drk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "# modified.\n",
        "# res = requests.get(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/previous_chapters.py\")\n",
        "res = requests.get(\"https://raw.githubusercontent.com/Aananda-giri/GPT2-Nepali/main/3.%20GPT2-Nepali/1.%20pre_training/previous_chapters.py\")\n",
        "with open('previous_chapters.py','w') as f:\n",
        "  f.write(res.text+pretrain_simple_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
      "metadata": {
        "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
      },
      "outputs": [],
      "source": [
        "from gpt_model_code import (\n",
        "    generate,\n",
        "    # text_to_token_ids,\n",
        "    # token_ids_to_text\n",
        ")\n",
        "\n",
        "\n",
        "# token_ids = generate(\n",
        "#     model=model,\n",
        "#     idx=text_to_token_ids(input_text, tokenizer),\n",
        "#     max_new_tokens=35,\n",
        "#     context_size=BASE_CONFIG[\"context_length\"],\n",
        "#     eos_id=50256,\n",
        "# )\n",
        "#  = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "generated_text = generate(\n",
        "    model,\n",
        "    input_text,\n",
        "    tokenizer,\n",
        "    max_new_tokens=35,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=None,  # New parameter for nucleus sampling\n",
        "    eos_id=None,\n",
        "    repetition_penalty=1.2,\n",
        "    penalize_len_below=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NurJx0_u1b4z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NurJx0_u1b4z",
        "outputId": "eacf8b05-6d73-49c1-a5bb-af808c3b96cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\\n\\n### प्रतिक्रिया:\\nथप रूपमा, इनपुटमा जनावरको सबैभन्दा प्रभावशाली विशेषताहरू मध्ये 5 सूचीबद्ध गर्नुहोस्\\n\\n### इनपुट:\\nजनावरः पेन्गुइन'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e2fda5-f796-4954-8f72-1dd1123e3344",
      "metadata": {
        "id": "36e2fda5-f796-4954-8f72-1dd1123e3344"
      },
      "source": [
        "- Note that the `generate` function we used in previous chapters returns the combined input and output text, which was convenient in the previous section for creating legible text\n",
        "- To isolate the response, we can subtract the length of the instruction from the start of the `generated_text`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
        "outputId": "842e29fb-132f-4963-a37f-182024588983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ः पेन्गुइन पेन्गुइनले निर्माण गरेको स्वचालित कारहरूको बारेमा । यो कार एक विशेष प्रकारको सुरक्षा कवच हो जसले कुनै पनि जनावर मानिसबीच सजिलै सम्पर्क गर्न सक्छ\n"
          ]
        }
      ],
      "source": [
        "response_text = (\n",
        "    generated_text[len(input_text):]\n",
        "    .replace(\"### प्रतिक्रिया:\", \"\")\n",
        "    .strip()\n",
        ")\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
      "metadata": {
        "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
      },
      "source": [
        "- As we can see, the model is not capable of following the instructions, yet; it creates a \"Response\" section but it simply repeats the original input sentence as well as the instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
      "metadata": {
        "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
      },
      "source": [
        "## 7.6 Finetuning the LLM on instruction data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a",
      "metadata": {
        "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a"
      },
      "source": [
        "- In this section, we finetune the model\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-5.webp?1\" width=500px>\n",
        "\n",
        "- Note that we can reuse all the loss calculation and training functions that we used in previous chapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65444865-df87-4d98-9faf-875e1c4be860",
      "metadata": {
        "id": "65444865-df87-4d98-9faf-875e1c4be860"
      },
      "outputs": [],
      "source": [
        "from previous_chapters import (\n",
        "    calc_loss_loader,\n",
        "    train_model_simple\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
      "metadata": {
        "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
      },
      "source": [
        "- Let's calculate the initial training and validation set loss before we start training (as in previous chapters, the goal is to minimize the loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
        "outputId": "288eab97-7256-480a-e701-c25479309021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 5.673533344268799\n",
            "Validation loss: 5.408612632751465\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9",
      "metadata": {
        "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9"
      },
      "source": [
        "- Note that the training is a bit more expensive than in previous chapters since we are using a larger model (355 million instead of 124 million parameters)\n",
        "- The runtimes for various devices are shown for reference below (running this notebook on a compatible GPU device requires no changes to the code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4b57fb-e689-4550-931c-6d34a932487c",
      "metadata": {
        "id": "db4b57fb-e689-4550-931c-6d34a932487c"
      },
      "source": [
        "<div style=\"text-align: left;\">\n",
        "    \n",
        "| Model              | Device                | Runtime for 2 Epochs |\n",
        "|--------------------|-----------------------|----------------------|\n",
        "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
        "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
        "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
        "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
        "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
        "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
        "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
        "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
        "\n",
        "</div>\n",
        "\n",
        "- I ran this notebook using the `\"gpt2-medium (355M)\"` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
        "outputId": "b7e156db-d95b-45fc-b89d-327d82367374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 5.221, Val loss 5.014\n",
            "Ep 1 (Step 000005): Train loss 4.311, Val loss 4.435\n",
            "Ep 1 (Step 000010): Train loss 4.218, Val loss 4.275\n",
            "Ep 1 (Step 000015): Train loss 4.112, Val loss 4.245\n",
            "Ep 1 (Step 000020): Train loss 4.072, Val loss 4.171\n",
            "Ep 1 (Step 000025): Train loss 3.939, Val loss 4.131\n",
            "Ep 1 (Step 000030): Train loss 3.767, Val loss 4.102\n",
            "Ep 1 (Step 000035): Train loss 3.771, Val loss 4.060\n",
            "Ep 1 (Step 000040): Train loss 4.050, Val loss 4.030\n",
            "Ep 1 (Step 000045): Train loss 3.945, Val loss 4.004\n",
            "Ep 1 (Step 000050): Train loss 3.739, Val loss 3.998\n",
            "Ep 1 (Step 000055): Train loss 3.849, Val loss 4.003\n",
            "Ep 1 (Step 000060): Train loss 3.736, Val loss 3.986\n",
            "Ep 1 (Step 000065): Train loss 3.695, Val loss 3.978\n",
            "Ep 1 (Step 000070): Train loss 3.949, Val loss 3.957\n",
            "Ep 1 (Step 000075): Train loss 3.802, Val loss 3.942\n",
            "Ep 1 (Step 000080): Train loss 3.975, Val loss 3.934\n",
            "Ep 1 (Step 000085): Train loss 3.804, Val loss 3.921\n",
            "Ep 1 (Step 000090): Train loss 3.860, Val loss 3.912\n",
            "Ep 1 (Step 000095): Train loss 3.847, Val loss 3.903\n",
            "Ep 1 (Step 000100): Train loss 3.691, Val loss 3.920\n",
            "Ep 1 (Step 000105): Train loss 3.734, Val loss 3.898\n",
            "Ep 1 (Step 000110): Train loss 3.793, Val loss 3.888\n",
            "Ep 1 (Step 000115): Train loss 3.677, Val loss 3.891\n",
            "Ep 1 (Step 000120): Train loss 3.758, Val loss 3.882\n",
            "Ep 1 (Step 000125): Train loss 3.579, Val loss 3.884\n",
            "Ep 1 (Step 000130): Train loss 3.768, Val loss 3.870\n",
            "Ep 1 (Step 000135): Train loss 3.842, Val loss 3.888\n",
            "Ep 1 (Step 000140): Train loss 3.807, Val loss 3.882\n",
            "Ep 1 (Step 000145): Train loss 3.565, Val loss 3.862\n",
            "Ep 1 (Step 000150): Train loss 3.583, Val loss 3.861\n",
            "Ep 1 (Step 000155): Train loss 3.491, Val loss 3.856\n",
            "Ep 1 (Step 000160): Train loss 3.596, Val loss 3.859\n",
            "Ep 1 (Step 000165): Train loss 3.893, Val loss 3.856\n",
            "Ep 1 (Step 000170): Train loss 3.860, Val loss 3.845\n",
            "Ep 1 (Step 000175): Train loss 3.478, Val loss 3.834\n",
            "Ep 1 (Step 000180): Train loss 3.462, Val loss 3.843\n",
            "Ep 1 (Step 000185): Train loss 3.941, Val loss 3.830\n",
            "Ep 1 (Step 000190): Train loss 3.527, Val loss 3.823\n",
            "Ep 1 (Step 000195): Train loss 3.844, Val loss 3.848\n",
            "Ep 1 (Step 000200): Train loss 3.694, Val loss 3.844\n",
            "Ep 1 (Step 000205): Train loss 3.639, Val loss 3.831\n",
            "Ep 1 (Step 000210): Train loss 3.680, Val loss 3.818\n",
            "Ep 1 (Step 000215): Train loss 3.699, Val loss 3.829\n",
            "Ep 1 (Step 000220): Train loss 3.724, Val loss 3.805\n",
            "Ep 1 (Step 000225): Train loss 3.800, Val loss 3.800\n",
            "Ep 1 (Step 000230): Train loss 3.722, Val loss 3.800\n",
            "Ep 1 (Step 000235): Train loss 3.446, Val loss 3.797\n",
            "Ep 1 (Step 000240): Train loss 3.482, Val loss 3.799\n",
            "Ep 1 (Step 000245): Train loss 3.354, Val loss 3.800\n",
            "Ep 1 (Step 000250): Train loss 3.671, Val loss 3.789\n",
            "Ep 1 (Step 000255): Train loss 3.604, Val loss 3.788\n",
            "Ep 1 (Step 000260): Train loss 3.622, Val loss 3.792\n",
            "Ep 1 (Step 000265): Train loss 3.355, Val loss 3.800\n",
            "Ep 1 (Step 000270): Train loss 3.692, Val loss 3.791\n",
            "Ep 1 (Step 000275): Train loss 3.694, Val loss 3.783\n",
            "Ep 1 (Step 000280): Train loss 3.526, Val loss 3.788\n",
            "Ep 1 (Step 000285): Train loss 3.510, Val loss 3.775\n",
            "Ep 1 (Step 000290): Train loss 3.431, Val loss 3.768\n",
            "Ep 1 (Step 000295): Train loss 3.547, Val loss 3.775\n",
            "Ep 1 (Step 000300): Train loss 3.508, Val loss 3.762\n",
            "Ep 1 (Step 000305): Train loss 3.668, Val loss 3.769\n",
            "Ep 1 (Step 000310): Train loss 3.749, Val loss 3.763\n",
            "Ep 1 (Step 000315): Train loss 3.431, Val loss 3.765\n",
            "Ep 1 (Step 000320): Train loss 3.652, Val loss 3.768\n",
            "Ep 1 (Step 000325): Train loss 3.786, Val loss 3.756\n",
            "Ep 1 (Step 000330): Train loss 3.803, Val loss 3.751\n",
            "Ep 1 (Step 000335): Train loss 3.686, Val loss 3.757\n",
            "Ep 1 (Step 000340): Train loss 3.374, Val loss 3.759\n",
            "Ep 1 (Step 000345): Train loss 3.437, Val loss 3.778\n",
            "Ep 1 (Step 000350): Train loss 3.478, Val loss 3.752\n",
            "Ep 1 (Step 000355): Train loss 3.585, Val loss 3.751\n",
            "Ep 1 (Step 000360): Train loss 3.556, Val loss 3.747\n",
            "Ep 1 (Step 000365): Train loss 3.567, Val loss 3.738\n",
            "Ep 1 (Step 000370): Train loss 3.644, Val loss 3.742\n",
            "Ep 1 (Step 000375): Train loss 3.361, Val loss 3.752\n",
            "Ep 1 (Step 000380): Train loss 3.390, Val loss 3.736\n",
            "Ep 1 (Step 000385): Train loss 3.390, Val loss 3.726\n",
            "Ep 1 (Step 000390): Train loss 3.474, Val loss 3.724\n",
            "Ep 1 (Step 000395): Train loss 3.381, Val loss 3.725\n",
            "Ep 1 (Step 000400): Train loss 3.546, Val loss 3.738\n",
            "Ep 1 (Step 000405): Train loss 3.571, Val loss 3.737\n",
            "Ep 1 (Step 000410): Train loss 3.479, Val loss 3.731\n",
            "Ep 1 (Step 000415): Train loss 3.553, Val loss 3.723\n",
            "Ep 1 (Step 000420): Train loss 3.475, Val loss 3.725\n",
            "Ep 1 (Step 000425): Train loss 3.589, Val loss 3.723\n",
            "Ep 1 (Step 000430): Train loss 3.487, Val loss 3.730\n",
            "Ep 1 (Step 000435): Train loss 3.597, Val loss 3.711\n",
            "Ep 1 (Step 000440): Train loss 3.510, Val loss 3.714\n",
            "Ep 1 (Step 000445): Train loss 3.448, Val loss 3.728\n",
            "Ep 1 (Step 000450): Train loss 3.527, Val loss 3.735\n",
            "Ep 1 (Step 000455): Train loss 3.809, Val loss 3.737\n",
            "Ep 1 (Step 000460): Train loss 3.651, Val loss 3.738\n",
            "Ep 1 (Step 000465): Train loss 3.449, Val loss 3.725\n",
            "Ep 1 (Step 000470): Train loss 3.476, Val loss 3.713\n",
            "Ep 1 (Step 000475): Train loss 3.402, Val loss 3.706\n",
            "Ep 1 (Step 000480): Train loss 3.574, Val loss 3.705\n",
            "Ep 1 (Step 000485): Train loss 3.421, Val loss 3.700\n",
            "Ep 1 (Step 000490): Train loss 3.530, Val loss 3.692\n",
            "Ep 1 (Step 000495): Train loss 3.416, Val loss 3.693\n",
            "Ep 1 (Step 000500): Train loss 3.545, Val loss 3.706\n",
            "Ep 1 (Step 000505): Train loss 3.472, Val loss 3.705\n",
            "Ep 1 (Step 000510): Train loss 3.490, Val loss 3.700\n",
            "Ep 1 (Step 000515): Train loss 3.390, Val loss 3.708\n",
            "Ep 1 (Step 000520): Train loss 3.213, Val loss 3.699\n",
            "Ep 1 (Step 000525): Train loss 3.400, Val loss 3.701\n",
            "Ep 1 (Step 000530): Train loss 3.334, Val loss 3.691\n",
            "Ep 1 (Step 000535): Train loss 3.331, Val loss 3.689\n",
            "Ep 1 (Step 000540): Train loss 3.532, Val loss 3.678\n",
            "Ep 1 (Step 000545): Train loss 3.314, Val loss 3.673\n",
            "Ep 1 (Step 000550): Train loss 3.355, Val loss 3.681\n",
            "Ep 1 (Step 000555): Train loss 3.395, Val loss 3.678\n",
            "Ep 1 (Step 000560): Train loss 3.557, Val loss 3.686\n",
            "Ep 1 (Step 000565): Train loss 3.315, Val loss 3.685\n",
            "Ep 1 (Step 000570): Train loss 3.631, Val loss 3.691\n",
            "Ep 1 (Step 000575): Train loss 3.336, Val loss 3.689\n",
            "Ep 1 (Step 000580): Train loss 3.550, Val loss 3.691\n",
            "Ep 1 (Step 000585): Train loss 3.643, Val loss 3.679\n",
            "Ep 1 (Step 000590): Train loss 3.492, Val loss 3.678\n",
            "Ep 1 (Step 000595): Train loss 3.383, Val loss 3.690\n",
            "Ep 1 (Step 000600): Train loss 3.480, Val loss 3.675\n",
            "Ep 1 (Step 000605): Train loss 3.347, Val loss 3.670\n",
            "Ep 1 (Step 000610): Train loss 3.441, Val loss 3.678\n",
            "Ep 1 (Step 000615): Train loss 3.354, Val loss 3.686\n",
            "Ep 1 (Step 000620): Train loss 3.280, Val loss 3.689\n",
            "Ep 1 (Step 000625): Train loss 3.513, Val loss 3.677\n",
            "Ep 1 (Step 000630): Train loss 3.401, Val loss 3.675\n",
            "Ep 1 (Step 000635): Train loss 3.597, Val loss 3.686\n",
            "Ep 1 (Step 000640): Train loss 3.525, Val loss 3.679\n",
            "Ep 1 (Step 000645): Train loss 3.334, Val loss 3.678\n",
            "Ep 1 (Step 000650): Train loss 3.566, Val loss 3.669\n",
            "Ep 1 (Step 000655): Train loss 3.557, Val loss 3.660\n",
            "Ep 1 (Step 000660): Train loss 3.409, Val loss 3.661\n",
            "Ep 1 (Step 000665): Train loss 3.482, Val loss 3.657\n",
            "Ep 1 (Step 000670): Train loss 3.617, Val loss 3.665\n",
            "Ep 1 (Step 000675): Train loss 3.207, Val loss 3.664\n",
            "Ep 1 (Step 000680): Train loss 3.408, Val loss 3.665\n",
            "Ep 1 (Step 000685): Train loss 3.219, Val loss 3.658\n",
            "Ep 1 (Step 000690): Train loss 3.322, Val loss 3.643\n",
            "Ep 1 (Step 000695): Train loss 3.349, Val loss 3.643\n",
            "Ep 1 (Step 000700): Train loss 3.656, Val loss 3.641\n",
            "Ep 1 (Step 000705): Train loss 3.387, Val loss 3.632\n",
            "Ep 1 (Step 000710): Train loss 3.268, Val loss 3.640\n",
            "Ep 1 (Step 000715): Train loss 3.204, Val loss 3.650\n",
            "Ep 1 (Step 000720): Train loss 3.453, Val loss 3.647\n",
            "Ep 1 (Step 000725): Train loss 3.464, Val loss 3.649\n",
            "Ep 1 (Step 000730): Train loss 3.337, Val loss 3.626\n",
            "Ep 1 (Step 000735): Train loss 3.501, Val loss 3.633\n",
            "Ep 1 (Step 000740): Train loss 3.477, Val loss 3.628\n",
            "Ep 1 (Step 000745): Train loss 3.395, Val loss 3.637\n",
            "Ep 1 (Step 000750): Train loss 3.265, Val loss 3.639\n",
            "Ep 1 (Step 000755): Train loss 3.172, Val loss 3.639\n",
            "Ep 1 (Step 000760): Train loss 3.402, Val loss 3.627\n",
            "Ep 1 (Step 000765): Train loss 3.436, Val loss 3.625\n",
            "Ep 1 (Step 000770): Train loss 3.569, Val loss 3.624\n",
            "Ep 1 (Step 000775): Train loss 3.345, Val loss 3.620\n",
            "Ep 1 (Step 000780): Train loss 3.313, Val loss 3.625\n",
            "Ep 1 (Step 000785): Train loss 3.229, Val loss 3.628\n",
            "Ep 1 (Step 000790): Train loss 3.417, Val loss 3.623\n",
            "Ep 1 (Step 000795): Train loss 3.360, Val loss 3.635\n",
            "Ep 1 (Step 000800): Train loss 3.346, Val loss 3.628\n",
            "Ep 1 (Step 000805): Train loss 3.210, Val loss 3.623\n",
            "Ep 1 (Step 000810): Train loss 3.498, Val loss 3.619\n",
            "Ep 1 (Step 000815): Train loss 3.424, Val loss 3.628\n",
            "Ep 1 (Step 000820): Train loss 3.264, Val loss 3.633\n",
            "Ep 1 (Step 000825): Train loss 3.396, Val loss 3.628\n",
            "Ep 1 (Step 000830): Train loss 3.507, Val loss 3.635\n",
            "Ep 1 (Step 000835): Train loss 3.459, Val loss 3.629\n",
            "Ep 1 (Step 000840): Train loss 3.376, Val loss 3.624\n",
            "Ep 1 (Step 000845): Train loss 3.171, Val loss 3.607\n",
            "Ep 1 (Step 000850): Train loss 3.419, Val loss 3.619\n",
            "Ep 1 (Step 000855): Train loss 3.335, Val loss 3.621\n",
            "Ep 1 (Step 000860): Train loss 3.377, Val loss 3.610\n",
            "Ep 1 (Step 000865): Train loss 3.367, Val loss 3.600\n",
            "Ep 1 (Step 000870): Train loss 3.386, Val loss 3.608\n",
            "Ep 1 (Step 000875): Train loss 3.441, Val loss 3.620\n",
            "Ep 1 (Step 000880): Train loss 3.608, Val loss 3.631\n",
            "Ep 1 (Step 000885): Train loss 3.351, Val loss 3.617\n",
            "Ep 1 (Step 000890): Train loss 3.167, Val loss 3.616\n",
            "Ep 1 (Step 000895): Train loss 3.407, Val loss 3.620\n",
            "Ep 1 (Step 000900): Train loss 3.400, Val loss 3.615\n",
            "Ep 1 (Step 000905): Train loss 3.634, Val loss 3.598\n",
            "Ep 1 (Step 000910): Train loss 3.225, Val loss 3.602\n",
            "Ep 1 (Step 000915): Train loss 3.255, Val loss 3.598\n",
            "Ep 1 (Step 000920): Train loss 3.495, Val loss 3.611\n",
            "Ep 1 (Step 000925): Train loss 3.331, Val loss 3.604\n",
            "Ep 1 (Step 000930): Train loss 3.292, Val loss 3.602\n",
            "Ep 1 (Step 000935): Train loss 3.218, Val loss 3.604\n",
            "Ep 1 (Step 000940): Train loss 3.373, Val loss 3.591\n",
            "Ep 1 (Step 000945): Train loss 3.314, Val loss 3.597\n",
            "Ep 1 (Step 000950): Train loss 3.138, Val loss 3.588\n",
            "Ep 1 (Step 000955): Train loss 3.233, Val loss 3.591\n",
            "Ep 1 (Step 000960): Train loss 3.467, Val loss 3.585\n",
            "Ep 1 (Step 000965): Train loss 3.259, Val loss 3.597\n",
            "Ep 1 (Step 000970): Train loss 3.382, Val loss 3.596\n",
            "Ep 1 (Step 000975): Train loss 3.331, Val loss 3.585\n",
            "Ep 1 (Step 000980): Train loss 3.399, Val loss 3.584\n",
            "Ep 1 (Step 000985): Train loss 3.515, Val loss 3.583\n",
            "Ep 1 (Step 000990): Train loss 3.166, Val loss 3.586\n",
            "Ep 1 (Step 000995): Train loss 3.294, Val loss 3.605\n",
            "Ep 1 (Step 001000): Train loss 3.125, Val loss 3.595\n",
            "Ep 1 (Step 001005): Train loss 3.173, Val loss 3.595\n",
            "Ep 1 (Step 001010): Train loss 3.418, Val loss 3.591\n",
            "Ep 1 (Step 001015): Train loss 3.218, Val loss 3.607\n",
            "Ep 1 (Step 001020): Train loss 3.204, Val loss 3.579\n",
            "Ep 1 (Step 001025): Train loss 3.274, Val loss 3.579\n",
            "Ep 1 (Step 001030): Train loss 3.162, Val loss 3.571\n",
            "Ep 1 (Step 001035): Train loss 3.320, Val loss 3.567\n",
            "Ep 1 (Step 001040): Train loss 3.269, Val loss 3.564\n",
            "Ep 1 (Step 001045): Train loss 3.334, Val loss 3.574\n",
            "Ep 1 (Step 001050): Train loss 3.169, Val loss 3.577\n",
            "Ep 1 (Step 001055): Train loss 3.301, Val loss 3.599\n",
            "Ep 1 (Step 001060): Train loss 3.405, Val loss 3.596\n",
            "Ep 1 (Step 001065): Train loss 3.375, Val loss 3.587\n",
            "Ep 1 (Step 001070): Train loss 3.376, Val loss 3.581\n",
            "Ep 1 (Step 001075): Train loss 3.254, Val loss 3.584\n",
            "Ep 1 (Step 001080): Train loss 3.400, Val loss 3.593\n",
            "Ep 1 (Step 001085): Train loss 3.134, Val loss 3.591\n",
            "Ep 1 (Step 001090): Train loss 3.619, Val loss 3.591\n",
            "Ep 1 (Step 001095): Train loss 3.524, Val loss 3.584\n",
            "Ep 1 (Step 001100): Train loss 3.347, Val loss 3.573\n",
            "Ep 1 (Step 001105): Train loss 3.030, Val loss 3.571\n",
            "Ep 1 (Step 001110): Train loss 3.043, Val loss 3.571\n",
            "Ep 1 (Step 001115): Train loss 3.230, Val loss 3.583\n",
            "Ep 1 (Step 001120): Train loss 3.244, Val loss 3.574\n",
            "Ep 1 (Step 001125): Train loss 3.262, Val loss 3.564\n",
            "Ep 1 (Step 001130): Train loss 3.276, Val loss 3.555\n",
            "Ep 1 (Step 001135): Train loss 3.229, Val loss 3.572\n",
            "Ep 1 (Step 001140): Train loss 3.301, Val loss 3.577\n",
            "Ep 1 (Step 001145): Train loss 3.343, Val loss 3.562\n",
            "Ep 1 (Step 001150): Train loss 3.326, Val loss 3.556\n",
            "Ep 1 (Step 001155): Train loss 3.222, Val loss 3.566\n",
            "Ep 1 (Step 001160): Train loss 3.194, Val loss 3.568\n",
            "Ep 1 (Step 001165): Train loss 3.035, Val loss 3.568\n",
            "Ep 1 (Step 001170): Train loss 3.130, Val loss 3.565\n",
            "Ep 1 (Step 001175): Train loss 3.467, Val loss 3.561\n",
            "Ep 1 (Step 001180): Train loss 3.550, Val loss 3.546\n",
            "Ep 1 (Step 001185): Train loss 3.341, Val loss 3.550\n",
            "Ep 1 (Step 001190): Train loss 3.081, Val loss 3.564\n",
            "Ep 1 (Step 001195): Train loss 3.176, Val loss 3.560\n",
            "Ep 1 (Step 001200): Train loss 3.214, Val loss 3.546\n",
            "Ep 1 (Step 001205): Train loss 3.171, Val loss 3.548\n",
            "Ep 1 (Step 001210): Train loss 3.254, Val loss 3.556\n",
            "Ep 1 (Step 001215): Train loss 3.297, Val loss 3.552\n",
            "Ep 1 (Step 001220): Train loss 3.197, Val loss 3.548\n",
            "Ep 1 (Step 001225): Train loss 3.180, Val loss 3.538\n",
            "Ep 1 (Step 001230): Train loss 3.241, Val loss 3.537\n",
            "Ep 1 (Step 001235): Train loss 3.284, Val loss 3.549\n",
            "Ep 1 (Step 001240): Train loss 3.099, Val loss 3.552\n",
            "Ep 1 (Step 001245): Train loss 3.089, Val loss 3.555\n",
            "Ep 1 (Step 001250): Train loss 3.060, Val loss 3.549\n",
            "Ep 1 (Step 001255): Train loss 3.412, Val loss 3.556\n",
            "Ep 1 (Step 001260): Train loss 3.243, Val loss 3.550\n",
            "Ep 1 (Step 001265): Train loss 3.576, Val loss 3.556\n",
            "Ep 1 (Step 001270): Train loss 3.416, Val loss 3.561\n",
            "Ep 1 (Step 001275): Train loss 3.130, Val loss 3.556\n",
            "Ep 1 (Step 001280): Train loss 3.195, Val loss 3.540\n",
            "Ep 1 (Step 001285): Train loss 3.134, Val loss 3.547\n",
            "Ep 1 (Step 001290): Train loss 3.334, Val loss 3.550\n",
            "Ep 1 (Step 001295): Train loss 3.338, Val loss 3.548\n",
            "Ep 1 (Step 001300): Train loss 3.176, Val loss 3.546\n",
            "Ep 1 (Step 001305): Train loss 3.370, Val loss 3.546\n",
            "Ep 1 (Step 001310): Train loss 3.210, Val loss 3.548\n",
            "Ep 1 (Step 001315): Train loss 3.437, Val loss 3.545\n",
            "Ep 1 (Step 001320): Train loss 3.026, Val loss 3.537\n",
            "Ep 1 (Step 001325): Train loss 3.169, Val loss 3.535\n",
            "Ep 1 (Step 001330): Train loss 3.226, Val loss 3.535\n",
            "Ep 1 (Step 001335): Train loss 3.250, Val loss 3.547\n",
            "Ep 1 (Step 001340): Train loss 3.355, Val loss 3.554\n",
            "Ep 1 (Step 001345): Train loss 3.069, Val loss 3.547\n",
            "Ep 1 (Step 001350): Train loss 3.191, Val loss 3.541\n",
            "Ep 1 (Step 001355): Train loss 3.081, Val loss 3.541\n",
            "Ep 1 (Step 001360): Train loss 3.321, Val loss 3.545\n",
            "Ep 1 (Step 001365): Train loss 3.068, Val loss 3.541\n",
            "Ep 1 (Step 001370): Train loss 3.416, Val loss 3.540\n",
            "Ep 1 (Step 001375): Train loss 3.383, Val loss 3.530\n",
            "Ep 1 (Step 001380): Train loss 3.143, Val loss 3.530\n",
            "Ep 1 (Step 001385): Train loss 3.458, Val loss 3.527\n",
            "Ep 1 (Step 001390): Train loss 3.057, Val loss 3.522\n",
            "Ep 1 (Step 001395): Train loss 3.098, Val loss 3.522\n",
            "Ep 1 (Step 001400): Train loss 3.132, Val loss 3.523\n",
            "Ep 1 (Step 001405): Train loss 3.163, Val loss 3.526\n",
            "Ep 1 (Step 001410): Train loss 3.097, Val loss 3.520\n",
            "Ep 1 (Step 001415): Train loss 2.989, Val loss 3.514\n",
            "Ep 1 (Step 001420): Train loss 3.243, Val loss 3.513\n",
            "Ep 1 (Step 001425): Train loss 3.335, Val loss 3.517\n",
            "Ep 1 (Step 001430): Train loss 3.122, Val loss 3.530\n",
            "Ep 1 (Step 001435): Train loss 3.241, Val loss 3.519\n",
            "Ep 1 (Step 001440): Train loss 3.096, Val loss 3.515\n",
            "Ep 1 (Step 001445): Train loss 3.167, Val loss 3.529\n",
            "Ep 1 (Step 001450): Train loss 3.051, Val loss 3.537\n",
            "Ep 1 (Step 001455): Train loss 3.279, Val loss 3.524\n",
            "Ep 1 (Step 001460): Train loss 3.166, Val loss 3.511\n",
            "Ep 1 (Step 001465): Train loss 2.854, Val loss 3.530\n",
            "Ep 1 (Step 001470): Train loss 3.191, Val loss 3.527\n",
            "Ep 1 (Step 001475): Train loss 3.168, Val loss 3.506\n",
            "Ep 1 (Step 001480): Train loss 3.252, Val loss 3.507\n",
            "Ep 1 (Step 001485): Train loss 3.140, Val loss 3.517\n",
            "Ep 1 (Step 001490): Train loss 3.141, Val loss 3.506\n",
            "Ep 1 (Step 001495): Train loss 2.977, Val loss 3.520\n",
            "Ep 1 (Step 001500): Train loss 3.146, Val loss 3.525\n",
            "Ep 1 (Step 001505): Train loss 3.002, Val loss 3.522\n",
            "Ep 1 (Step 001510): Train loss 3.153, Val loss 3.515\n",
            "Ep 1 (Step 001515): Train loss 3.038, Val loss 3.517\n",
            "Ep 1 (Step 001520): Train loss 3.203, Val loss 3.530\n",
            "Ep 1 (Step 001525): Train loss 3.295, Val loss 3.520\n",
            "Ep 1 (Step 001530): Train loss 3.015, Val loss 3.523\n",
            "Ep 1 (Step 001535): Train loss 3.255, Val loss 3.528\n",
            "Ep 1 (Step 001540): Train loss 3.009, Val loss 3.526\n",
            "Ep 1 (Step 001545): Train loss 3.304, Val loss 3.525\n",
            "Ep 1 (Step 001550): Train loss 3.256, Val loss 3.525\n",
            "Ep 1 (Step 001555): Train loss 3.426, Val loss 3.527\n",
            "Ep 1 (Step 001560): Train loss 3.064, Val loss 3.512\n",
            "Ep 1 (Step 001565): Train loss 3.247, Val loss 3.508\n",
            "Ep 1 (Step 001570): Train loss 3.272, Val loss 3.519\n",
            "Ep 1 (Step 001575): Train loss 3.197, Val loss 3.526\n",
            "Ep 1 (Step 001580): Train loss 3.227, Val loss 3.510\n",
            "Ep 1 (Step 001585): Train loss 3.212, Val loss 3.511\n",
            "Ep 1 (Step 001590): Train loss 3.121, Val loss 3.517\n",
            "Ep 1 (Step 001595): Train loss 3.233, Val loss 3.516\n",
            "Ep 1 (Step 001600): Train loss 2.922, Val loss 3.520\n",
            "Ep 1 (Step 001605): Train loss 3.176, Val loss 3.519\n",
            "Ep 1 (Step 001610): Train loss 3.080, Val loss 3.519\n",
            "Ep 1 (Step 001615): Train loss 3.116, Val loss 3.521\n",
            "Ep 1 (Step 001620): Train loss 3.011, Val loss 3.515\n",
            "Ep 1 (Step 001625): Train loss 3.098, Val loss 3.496\n",
            "Ep 1 (Step 001630): Train loss 3.228, Val loss 3.493\n",
            "Ep 1 (Step 001635): Train loss 2.982, Val loss 3.500\n",
            "Ep 1 (Step 001640): Train loss 2.921, Val loss 3.496\n",
            "Ep 1 (Step 001645): Train loss 3.092, Val loss 3.494\n",
            "Ep 1 (Step 001650): Train loss 3.077, Val loss 3.502\n",
            "Ep 1 (Step 001655): Train loss 3.059, Val loss 3.513\n",
            "Ep 1 (Step 001660): Train loss 3.380, Val loss 3.512\n",
            "Ep 1 (Step 001665): Train loss 2.950, Val loss 3.515\n",
            "Ep 1 (Step 001670): Train loss 3.342, Val loss 3.515\n",
            "Ep 1 (Step 001675): Train loss 3.175, Val loss 3.507\n",
            "Ep 1 (Step 001680): Train loss 3.158, Val loss 3.498\n",
            "Ep 1 (Step 001685): Train loss 3.086, Val loss 3.501\n",
            "Ep 1 (Step 001690): Train loss 3.219, Val loss 3.498\n",
            "Ep 1 (Step 001695): Train loss 3.077, Val loss 3.501\n",
            "Ep 1 (Step 001700): Train loss 3.067, Val loss 3.507\n",
            "Ep 1 (Step 001705): Train loss 3.130, Val loss 3.511\n",
            "Ep 1 (Step 001710): Train loss 3.237, Val loss 3.524\n",
            "Ep 1 (Step 001715): Train loss 3.071, Val loss 3.514\n",
            "Ep 1 (Step 001720): Train loss 3.109, Val loss 3.515\n",
            "Ep 1 (Step 001725): Train loss 3.078, Val loss 3.505\n",
            "Ep 1 (Step 001730): Train loss 3.134, Val loss 3.514\n",
            "Ep 1 (Step 001735): Train loss 3.096, Val loss 3.507\n",
            "Ep 1 (Step 001740): Train loss 3.055, Val loss 3.506\n",
            "Ep 1 (Step 001745): Train loss 3.176, Val loss 3.492\n",
            "Ep 1 (Step 001750): Train loss 3.261, Val loss 3.491\n",
            "Ep 1 (Step 001755): Train loss 3.112, Val loss 3.500\n",
            "Ep 1 (Step 001760): Train loss 3.219, Val loss 3.502\n",
            "Ep 1 (Step 001765): Train loss 3.260, Val loss 3.516\n",
            "Ep 1 (Step 001770): Train loss 3.197, Val loss 3.529\n",
            "Ep 1 (Step 001775): Train loss 3.323, Val loss 3.510\n",
            "Ep 1 (Step 001780): Train loss 3.123, Val loss 3.499\n",
            "Ep 1 (Step 001785): Train loss 3.245, Val loss 3.499\n",
            "Ep 1 (Step 001790): Train loss 3.251, Val loss 3.507\n",
            "Ep 1 (Step 001795): Train loss 3.053, Val loss 3.502\n",
            "Ep 1 (Step 001800): Train loss 3.441, Val loss 3.514\n",
            "Ep 1 (Step 001805): Train loss 3.030, Val loss 3.519\n",
            "Ep 1 (Step 001810): Train loss 3.121, Val loss 3.518\n",
            "Ep 1 (Step 001815): Train loss 3.235, Val loss 3.507\n",
            "Ep 1 (Step 001820): Train loss 3.133, Val loss 3.500\n",
            "Ep 1 (Step 001825): Train loss 3.250, Val loss 3.496\n",
            "Ep 1 (Step 001830): Train loss 3.069, Val loss 3.498\n",
            "Ep 1 (Step 001835): Train loss 3.203, Val loss 3.493\n",
            "Ep 1 (Step 001840): Train loss 3.187, Val loss 3.489\n",
            "Ep 1 (Step 001845): Train loss 3.149, Val loss 3.488\n",
            "Ep 1 (Step 001850): Train loss 3.142, Val loss 3.504\n",
            "Ep 1 (Step 001855): Train loss 2.951, Val loss 3.492\n",
            "Ep 1 (Step 001860): Train loss 3.240, Val loss 3.491\n",
            "Ep 1 (Step 001865): Train loss 3.209, Val loss 3.498\n",
            "Ep 1 (Step 001870): Train loss 3.285, Val loss 3.497\n",
            "Ep 1 (Step 001875): Train loss 3.022, Val loss 3.506\n",
            "Ep 1 (Step 001880): Train loss 3.256, Val loss 3.517\n",
            "Ep 1 (Step 001885): Train loss 3.046, Val loss 3.508\n",
            "Ep 1 (Step 001890): Train loss 3.071, Val loss 3.513\n",
            "Ep 1 (Step 001895): Train loss 3.176, Val loss 3.504\n",
            "Ep 1 (Step 001900): Train loss 3.051, Val loss 3.505\n",
            "Ep 1 (Step 001905): Train loss 3.004, Val loss 3.497\n",
            "Ep 1 (Step 001910): Train loss 3.146, Val loss 3.490\n",
            "Ep 1 (Step 001915): Train loss 3.171, Val loss 3.492\n",
            "Ep 1 (Step 001920): Train loss 3.159, Val loss 3.505\n",
            "Ep 1 (Step 001925): Train loss 3.233, Val loss 3.507\n",
            "Ep 1 (Step 001930): Train loss 3.171, Val loss 3.502\n",
            "Ep 1 (Step 001935): Train loss 3.144, Val loss 3.499\n",
            "Ep 1 (Step 001940): Train loss 3.105, Val loss 3.502\n",
            "Ep 1 (Step 001945): Train loss 3.145, Val loss 3.507\n",
            "Ep 1 (Step 001950): Train loss 3.182, Val loss 3.505\n",
            "Ep 1 (Step 001955): Train loss 3.377, Val loss 3.497\n",
            "Ep 1 (Step 001960): Train loss 3.278, Val loss 3.485\n",
            "Ep 1 (Step 001965): Train loss 3.277, Val loss 3.491\n",
            "Ep 1 (Step 001970): Train loss 3.129, Val loss 3.494\n",
            "Ep 1 (Step 001975): Train loss 3.101, Val loss 3.505\n",
            "Ep 1 (Step 001980): Train loss 3.182, Val loss 3.488\n",
            "Ep 1 (Step 001985): Train loss 3.160, Val loss 3.499\n",
            "Ep 1 (Step 001990): Train loss 3.145, Val loss 3.504\n",
            "Ep 1 (Step 001995): Train loss 3.214, Val loss 3.505\n",
            "Ep 1 (Step 002000): Train loss 3.194, Val loss 3.495\n",
            "Ep 1 (Step 002005): Train loss 3.241, Val loss 3.507\n",
            "Ep 1 (Step 002010): Train loss 3.117, Val loss 3.491\n",
            "Ep 1 (Step 002015): Train loss 3.128, Val loss 3.478\n",
            "Ep 1 (Step 002020): Train loss 2.992, Val loss 3.482\n",
            "Ep 1 (Step 002025): Train loss 3.119, Val loss 3.476\n",
            "Ep 1 (Step 002030): Train loss 3.272, Val loss 3.470\n",
            "Ep 1 (Step 002035): Train loss 2.877, Val loss 3.462\n",
            "Ep 1 (Step 002040): Train loss 3.234, Val loss 3.483\n",
            "Ep 1 (Step 002045): Train loss 3.131, Val loss 3.482\n",
            "Ep 1 (Step 002050): Train loss 3.238, Val loss 3.477\n",
            "Ep 1 (Step 002055): Train loss 2.940, Val loss 3.480\n",
            "Ep 1 (Step 002060): Train loss 3.232, Val loss 3.494\n",
            "Ep 1 (Step 002065): Train loss 3.129, Val loss 3.499\n",
            "Ep 1 (Step 002070): Train loss 2.899, Val loss 3.485\n",
            "Ep 1 (Step 002075): Train loss 2.989, Val loss 3.483\n",
            "Ep 1 (Step 002080): Train loss 3.085, Val loss 3.480\n",
            "Ep 1 (Step 002085): Train loss 3.119, Val loss 3.479\n",
            "Ep 1 (Step 002090): Train loss 3.178, Val loss 3.469\n",
            "Ep 1 (Step 002095): Train loss 3.158, Val loss 3.457\n",
            "Ep 1 (Step 002100): Train loss 3.137, Val loss 3.471\n",
            "Ep 1 (Step 002105): Train loss 3.126, Val loss 3.476\n",
            "Ep 1 (Step 002110): Train loss 3.210, Val loss 3.476\n",
            "Ep 1 (Step 002115): Train loss 3.103, Val loss 3.478\n",
            "Ep 1 (Step 002120): Train loss 2.966, Val loss 3.470\n",
            "Ep 1 (Step 002125): Train loss 3.124, Val loss 3.473\n",
            "Ep 1 (Step 002130): Train loss 3.223, Val loss 3.473\n",
            "Ep 1 (Step 002135): Train loss 3.070, Val loss 3.473\n",
            "Ep 1 (Step 002140): Train loss 3.035, Val loss 3.470\n",
            "Ep 1 (Step 002145): Train loss 2.867, Val loss 3.470\n",
            "Ep 1 (Step 002150): Train loss 3.304, Val loss 3.481\n",
            "Ep 1 (Step 002155): Train loss 3.028, Val loss 3.482\n",
            "Ep 1 (Step 002160): Train loss 2.928, Val loss 3.471\n",
            "Ep 1 (Step 002165): Train loss 3.229, Val loss 3.475\n",
            "Ep 1 (Step 002170): Train loss 3.249, Val loss 3.476\n",
            "Ep 1 (Step 002175): Train loss 3.047, Val loss 3.477\n",
            "Ep 1 (Step 002180): Train loss 3.038, Val loss 3.477\n",
            "Ep 1 (Step 002185): Train loss 3.191, Val loss 3.474\n",
            "Ep 1 (Step 002190): Train loss 3.080, Val loss 3.478\n",
            "Ep 1 (Step 002195): Train loss 2.901, Val loss 3.484\n",
            "Ep 1 (Step 002200): Train loss 3.096, Val loss 3.472\n",
            "Ep 1 (Step 002205): Train loss 3.077, Val loss 3.469\n",
            "Ep 1 (Step 002210): Train loss 2.892, Val loss 3.473\n",
            "Ep 1 (Step 002215): Train loss 2.922, Val loss 3.483\n",
            "Ep 1 (Step 002220): Train loss 2.983, Val loss 3.481\n",
            "Ep 1 (Step 002225): Train loss 3.010, Val loss 3.471\n",
            "Ep 1 (Step 002230): Train loss 3.120, Val loss 3.464\n",
            "Ep 1 (Step 002235): Train loss 3.081, Val loss 3.471\n",
            "Ep 1 (Step 002240): Train loss 3.052, Val loss 3.468\n",
            "Ep 1 (Step 002245): Train loss 3.245, Val loss 3.461\n",
            "Ep 1 (Step 002250): Train loss 3.288, Val loss 3.458\n",
            "Ep 1 (Step 002255): Train loss 2.826, Val loss 3.466\n",
            "Ep 1 (Step 002260): Train loss 3.151, Val loss 3.469\n",
            "Ep 1 (Step 002265): Train loss 3.185, Val loss 3.461\n",
            "Ep 1 (Step 002270): Train loss 2.886, Val loss 3.452\n",
            "Ep 1 (Step 002275): Train loss 3.065, Val loss 3.456\n",
            "Ep 1 (Step 002280): Train loss 3.228, Val loss 3.449\n",
            "Ep 1 (Step 002285): Train loss 3.066, Val loss 3.457\n",
            "Ep 1 (Step 002290): Train loss 3.359, Val loss 3.459\n",
            "Ep 1 (Step 002295): Train loss 2.895, Val loss 3.466\n",
            "Ep 1 (Step 002300): Train loss 3.243, Val loss 3.462\n",
            "Ep 1 (Step 002305): Train loss 3.071, Val loss 3.466\n",
            "Ep 1 (Step 002310): Train loss 2.919, Val loss 3.473\n",
            "Ep 1 (Step 002315): Train loss 2.929, Val loss 3.469\n",
            "Ep 1 (Step 002320): Train loss 3.082, Val loss 3.449\n",
            "Ep 1 (Step 002325): Train loss 3.224, Val loss 3.452\n",
            "Ep 1 (Step 002330): Train loss 3.214, Val loss 3.455\n",
            "Ep 1 (Step 002335): Train loss 3.088, Val loss 3.446\n",
            "Ep 1 (Step 002340): Train loss 3.242, Val loss 3.450\n",
            "Ep 1 (Step 002345): Train loss 3.157, Val loss 3.451\n",
            "Ep 1 (Step 002350): Train loss 3.199, Val loss 3.447\n",
            "Ep 1 (Step 002355): Train loss 3.000, Val loss 3.435\n",
            "Ep 1 (Step 002360): Train loss 2.895, Val loss 3.420\n",
            "Ep 1 (Step 002365): Train loss 3.154, Val loss 3.427\n",
            "Ep 1 (Step 002370): Train loss 3.095, Val loss 3.442\n",
            "Ep 1 (Step 002375): Train loss 3.052, Val loss 3.431\n",
            "Ep 1 (Step 002380): Train loss 3.154, Val loss 3.434\n",
            "Ep 1 (Step 002385): Train loss 2.916, Val loss 3.451\n",
            "Ep 1 (Step 002390): Train loss 2.960, Val loss 3.439\n",
            "Ep 1 (Step 002395): Train loss 3.062, Val loss 3.425\n",
            "Ep 1 (Step 002400): Train loss 3.026, Val loss 3.421\n",
            "Ep 1 (Step 002405): Train loss 3.018, Val loss 3.443\n",
            "Ep 1 (Step 002410): Train loss 3.001, Val loss 3.429\n",
            "Ep 1 (Step 002415): Train loss 2.905, Val loss 3.436\n",
            "Ep 1 (Step 002420): Train loss 3.363, Val loss 3.443\n",
            "Ep 1 (Step 002425): Train loss 3.141, Val loss 3.443\n",
            "Ep 1 (Step 002430): Train loss 2.969, Val loss 3.425\n",
            "Ep 1 (Step 002435): Train loss 2.980, Val loss 3.425\n",
            "Ep 1 (Step 002440): Train loss 3.165, Val loss 3.436\n",
            "Ep 1 (Step 002445): Train loss 2.996, Val loss 3.433\n",
            "Ep 1 (Step 002450): Train loss 2.923, Val loss 3.430\n",
            "Ep 1 (Step 002455): Train loss 3.076, Val loss 3.424\n",
            "Ep 1 (Step 002460): Train loss 3.017, Val loss 3.434\n",
            "Ep 1 (Step 002465): Train loss 3.019, Val loss 3.458\n",
            "Ep 1 (Step 002470): Train loss 3.164, Val loss 3.447\n",
            "Ep 1 (Step 002475): Train loss 3.010, Val loss 3.435\n",
            "Ep 1 (Step 002480): Train loss 3.131, Val loss 3.439\n",
            "Ep 1 (Step 002485): Train loss 2.777, Val loss 3.432\n",
            "Ep 1 (Step 002490): Train loss 2.922, Val loss 3.431\n",
            "Ep 1 (Step 002495): Train loss 2.831, Val loss 3.430\n",
            "Ep 1 (Step 002500): Train loss 3.104, Val loss 3.431\n",
            "Ep 1 (Step 002505): Train loss 3.017, Val loss 3.430\n",
            "Ep 1 (Step 002510): Train loss 3.173, Val loss 3.438\n",
            "Ep 1 (Step 002515): Train loss 3.176, Val loss 3.430\n",
            "Ep 1 (Step 002520): Train loss 3.081, Val loss 3.426\n",
            "Ep 1 (Step 002525): Train loss 3.280, Val loss 3.438\n",
            "Ep 1 (Step 002530): Train loss 3.270, Val loss 3.432\n",
            "Ep 1 (Step 002535): Train loss 2.973, Val loss 3.429\n",
            "Ep 1 (Step 002540): Train loss 2.948, Val loss 3.434\n",
            "Ep 1 (Step 002545): Train loss 3.099, Val loss 3.434\n",
            "Ep 1 (Step 002550): Train loss 3.051, Val loss 3.437\n",
            "Ep 1 (Step 002555): Train loss 3.021, Val loss 3.428\n",
            "Ep 1 (Step 002560): Train loss 3.055, Val loss 3.428\n",
            "Ep 1 (Step 002565): Train loss 2.932, Val loss 3.428\n",
            "Ep 1 (Step 002570): Train loss 3.167, Val loss 3.432\n",
            "Ep 1 (Step 002575): Train loss 2.908, Val loss 3.426\n",
            "Ep 1 (Step 002580): Train loss 3.058, Val loss 3.429\n",
            "Ep 1 (Step 002585): Train loss 2.920, Val loss 3.433\n",
            "Ep 1 (Step 002590): Train loss 3.042, Val loss 3.425\n",
            "Ep 1 (Step 002595): Train loss 2.997, Val loss 3.426\n",
            "Ep 1 (Step 002600): Train loss 3.136, Val loss 3.420\n",
            "Ep 1 (Step 002605): Train loss 3.186, Val loss 3.424\n",
            "Ep 1 (Step 002610): Train loss 2.978, Val loss 3.418\n",
            "Ep 1 (Step 002615): Train loss 3.065, Val loss 3.419\n",
            "Ep 1 (Step 002620): Train loss 3.057, Val loss 3.431\n",
            "Ep 1 (Step 002625): Train loss 3.029, Val loss 3.429\n",
            "Ep 1 (Step 002630): Train loss 2.906, Val loss 3.416\n",
            "Ep 1 (Step 002635): Train loss 3.106, Val loss 3.413\n",
            "Ep 1 (Step 002640): Train loss 3.074, Val loss 3.418\n",
            "Ep 1 (Step 002645): Train loss 2.999, Val loss 3.429\n",
            "Ep 1 (Step 002650): Train loss 3.077, Val loss 3.426\n",
            "Ep 1 (Step 002655): Train loss 3.118, Val loss 3.419\n",
            "Ep 1 (Step 002660): Train loss 3.021, Val loss 3.421\n",
            "Ep 1 (Step 002665): Train loss 3.103, Val loss 3.431\n",
            "Ep 1 (Step 002670): Train loss 2.954, Val loss 3.422\n",
            "Ep 1 (Step 002675): Train loss 3.098, Val loss 3.423\n",
            "Ep 1 (Step 002680): Train loss 2.987, Val loss 3.418\n",
            "Ep 1 (Step 002685): Train loss 3.072, Val loss 3.416\n",
            "Ep 1 (Step 002690): Train loss 2.972, Val loss 3.420\n",
            "Ep 1 (Step 002695): Train loss 3.151, Val loss 3.425\n",
            "Ep 1 (Step 002700): Train loss 3.070, Val loss 3.423\n",
            "Ep 1 (Step 002705): Train loss 3.131, Val loss 3.420\n",
            "Ep 1 (Step 002710): Train loss 2.848, Val loss 3.420\n",
            "Ep 1 (Step 002715): Train loss 2.891, Val loss 3.429\n",
            "Ep 1 (Step 002720): Train loss 2.885, Val loss 3.433\n",
            "Ep 1 (Step 002725): Train loss 3.078, Val loss 3.435\n",
            "Ep 1 (Step 002730): Train loss 3.024, Val loss 3.442\n",
            "Ep 1 (Step 002735): Train loss 2.991, Val loss 3.445\n",
            "Ep 1 (Step 002740): Train loss 3.290, Val loss 3.427\n",
            "Ep 1 (Step 002745): Train loss 3.104, Val loss 3.418\n",
            "Ep 1 (Step 002750): Train loss 3.067, Val loss 3.422\n",
            "Ep 1 (Step 002755): Train loss 3.091, Val loss 3.422\n",
            "Ep 1 (Step 002760): Train loss 3.043, Val loss 3.410\n",
            "Ep 1 (Step 002765): Train loss 3.091, Val loss 3.418\n",
            "Ep 1 (Step 002770): Train loss 2.929, Val loss 3.416\n",
            "Ep 1 (Step 002775): Train loss 2.930, Val loss 3.412\n",
            "Ep 1 (Step 002780): Train loss 3.128, Val loss 3.416\n",
            "Ep 1 (Step 002785): Train loss 3.075, Val loss 3.411\n",
            "Ep 1 (Step 002790): Train loss 2.817, Val loss 3.408\n",
            "Ep 1 (Step 002795): Train loss 3.079, Val loss 3.412\n",
            "Ep 1 (Step 002800): Train loss 2.979, Val loss 3.408\n",
            "Ep 1 (Step 002805): Train loss 3.081, Val loss 3.414\n",
            "Ep 1 (Step 002810): Train loss 2.984, Val loss 3.402\n",
            "Ep 1 (Step 002815): Train loss 2.889, Val loss 3.403\n",
            "Ep 1 (Step 002820): Train loss 2.833, Val loss 3.404\n",
            "Ep 1 (Step 002825): Train loss 2.897, Val loss 3.406\n",
            "Ep 1 (Step 002830): Train loss 3.107, Val loss 3.418\n",
            "Ep 1 (Step 002835): Train loss 3.380, Val loss 3.414\n",
            "Ep 1 (Step 002840): Train loss 3.119, Val loss 3.417\n",
            "Ep 1 (Step 002845): Train loss 3.160, Val loss 3.422\n",
            "Ep 1 (Step 002850): Train loss 2.950, Val loss 3.420\n",
            "Ep 1 (Step 002855): Train loss 3.068, Val loss 3.416\n",
            "Ep 1 (Step 002860): Train loss 2.954, Val loss 3.406\n",
            "Ep 1 (Step 002865): Train loss 3.076, Val loss 3.397\n",
            "Ep 1 (Step 002870): Train loss 2.941, Val loss 3.399\n",
            "Ep 1 (Step 002875): Train loss 2.918, Val loss 3.402\n",
            "Ep 1 (Step 002880): Train loss 3.137, Val loss 3.409\n",
            "Ep 1 (Step 002885): Train loss 2.826, Val loss 3.424\n",
            "Ep 1 (Step 002890): Train loss 2.823, Val loss 3.424\n",
            "Ep 1 (Step 002895): Train loss 2.915, Val loss 3.418\n",
            "Ep 1 (Step 002900): Train loss 3.146, Val loss 3.408\n",
            "Ep 1 (Step 002905): Train loss 2.794, Val loss 3.412\n",
            "Ep 1 (Step 002910): Train loss 2.930, Val loss 3.402\n",
            "Ep 1 (Step 002915): Train loss 3.067, Val loss 3.394\n",
            "Ep 1 (Step 002920): Train loss 2.972, Val loss 3.392\n",
            "Ep 1 (Step 002925): Train loss 3.048, Val loss 3.402\n",
            "Ep 1 (Step 002930): Train loss 2.914, Val loss 3.400\n",
            "Ep 1 (Step 002935): Train loss 3.121, Val loss 3.400\n",
            "Ep 1 (Step 002940): Train loss 2.975, Val loss 3.409\n",
            "Ep 1 (Step 002945): Train loss 3.147, Val loss 3.417\n",
            "Ep 1 (Step 002950): Train loss 2.835, Val loss 3.410\n",
            "Ep 1 (Step 002955): Train loss 2.835, Val loss 3.402\n",
            "Ep 1 (Step 002960): Train loss 3.148, Val loss 3.402\n",
            "Ep 1 (Step 002965): Train loss 3.138, Val loss 3.391\n",
            "Ep 1 (Step 002970): Train loss 2.810, Val loss 3.392\n",
            "Ep 1 (Step 002975): Train loss 2.950, Val loss 3.388\n",
            "Ep 1 (Step 002980): Train loss 2.931, Val loss 3.384\n",
            "Ep 1 (Step 002985): Train loss 3.162, Val loss 3.386\n",
            "Ep 1 (Step 002990): Train loss 2.960, Val loss 3.385\n",
            "Ep 1 (Step 002995): Train loss 3.053, Val loss 3.389\n",
            "Ep 1 (Step 003000): Train loss 3.015, Val loss 3.383\n",
            "Ep 1 (Step 003005): Train loss 3.200, Val loss 3.377\n",
            "Ep 1 (Step 003010): Train loss 2.858, Val loss 3.386\n",
            "Ep 1 (Step 003015): Train loss 2.945, Val loss 3.384\n",
            "Ep 1 (Step 003020): Train loss 3.152, Val loss 3.377\n",
            "Ep 1 (Step 003025): Train loss 3.089, Val loss 3.379\n",
            "Ep 1 (Step 003030): Train loss 2.946, Val loss 3.397\n",
            "Ep 1 (Step 003035): Train loss 2.917, Val loss 3.397\n",
            "Ep 1 (Step 003040): Train loss 2.733, Val loss 3.396\n",
            "Ep 1 (Step 003045): Train loss 3.020, Val loss 3.394\n",
            "Ep 1 (Step 003050): Train loss 3.083, Val loss 3.391\n",
            "Ep 1 (Step 003055): Train loss 3.169, Val loss 3.382\n",
            "Ep 1 (Step 003060): Train loss 2.846, Val loss 3.380\n",
            "Ep 1 (Step 003065): Train loss 2.719, Val loss 3.374\n",
            "Ep 1 (Step 003070): Train loss 3.060, Val loss 3.379\n",
            "Ep 1 (Step 003075): Train loss 3.140, Val loss 3.387\n",
            "Ep 1 (Step 003080): Train loss 2.972, Val loss 3.389\n",
            "Ep 1 (Step 003085): Train loss 2.907, Val loss 3.381\n",
            "Ep 1 (Step 003090): Train loss 3.013, Val loss 3.387\n",
            "Ep 1 (Step 003095): Train loss 2.923, Val loss 3.379\n",
            "Ep 1 (Step 003100): Train loss 2.967, Val loss 3.372\n",
            "Ep 1 (Step 003105): Train loss 2.953, Val loss 3.375\n",
            "Ep 1 (Step 003110): Train loss 2.960, Val loss 3.379\n",
            "Ep 1 (Step 003115): Train loss 2.987, Val loss 3.377\n",
            "Ep 1 (Step 003120): Train loss 2.955, Val loss 3.375\n",
            "Ep 1 (Step 003125): Train loss 2.994, Val loss 3.380\n",
            "Ep 1 (Step 003130): Train loss 3.023, Val loss 3.366\n",
            "Ep 1 (Step 003135): Train loss 3.047, Val loss 3.371\n",
            "Ep 1 (Step 003140): Train loss 3.063, Val loss 3.370\n",
            "Ep 1 (Step 003145): Train loss 2.977, Val loss 3.377\n",
            "Ep 1 (Step 003150): Train loss 2.885, Val loss 3.374\n",
            "Ep 1 (Step 003155): Train loss 3.110, Val loss 3.375\n",
            "Ep 1 (Step 003160): Train loss 3.128, Val loss 3.375\n",
            "Ep 1 (Step 003165): Train loss 2.933, Val loss 3.384\n",
            "Ep 1 (Step 003170): Train loss 3.087, Val loss 3.386\n",
            "Ep 1 (Step 003175): Train loss 3.095, Val loss 3.382\n",
            "Ep 1 (Step 003180): Train loss 2.972, Val loss 3.381\n",
            "Ep 1 (Step 003185): Train loss 3.019, Val loss 3.378\n",
            "Ep 1 (Step 003190): Train loss 2.924, Val loss 3.371\n",
            "Ep 1 (Step 003195): Train loss 2.899, Val loss 3.376\n",
            "Ep 1 (Step 003200): Train loss 3.143, Val loss 3.383\n",
            "Ep 1 (Step 003205): Train loss 2.938, Val loss 3.372\n",
            "Ep 1 (Step 003210): Train loss 2.931, Val loss 3.373\n",
            "Ep 1 (Step 003215): Train loss 2.965, Val loss 3.386\n",
            "Ep 1 (Step 003220): Train loss 2.957, Val loss 3.380\n",
            "Ep 1 (Step 003225): Train loss 3.068, Val loss 3.384\n",
            "Ep 1 (Step 003230): Train loss 2.955, Val loss 3.379\n",
            "Ep 1 (Step 003235): Train loss 3.113, Val loss 3.383\n",
            "Ep 1 (Step 003240): Train loss 2.994, Val loss 3.395\n",
            "Ep 1 (Step 003245): Train loss 2.898, Val loss 3.394\n",
            "Ep 1 (Step 003250): Train loss 2.950, Val loss 3.386\n",
            "Ep 1 (Step 003255): Train loss 3.170, Val loss 3.378\n",
            "Ep 1 (Step 003260): Train loss 2.958, Val loss 3.373\n",
            "Ep 1 (Step 003265): Train loss 2.887, Val loss 3.380\n",
            "Ep 1 (Step 003270): Train loss 3.021, Val loss 3.377\n",
            "Ep 1 (Step 003275): Train loss 2.832, Val loss 3.372\n",
            "Ep 1 (Step 003280): Train loss 3.084, Val loss 3.372\n",
            "Ep 1 (Step 003285): Train loss 2.854, Val loss 3.376\n",
            "Ep 1 (Step 003290): Train loss 2.981, Val loss 3.372\n",
            "Ep 1 (Step 003295): Train loss 3.199, Val loss 3.369\n",
            "Ep 1 (Step 003300): Train loss 2.899, Val loss 3.369\n",
            "Ep 1 (Step 003305): Train loss 2.966, Val loss 3.377\n",
            "Ep 1 (Step 003310): Train loss 2.908, Val loss 3.377\n",
            "Ep 1 (Step 003315): Train loss 2.872, Val loss 3.367\n",
            "Ep 1 (Step 003320): Train loss 2.936, Val loss 3.366\n",
            "Ep 1 (Step 003325): Train loss 2.921, Val loss 3.373\n",
            "Ep 1 (Step 003330): Train loss 2.949, Val loss 3.364\n",
            "Ep 1 (Step 003335): Train loss 3.018, Val loss 3.356\n",
            "Ep 1 (Step 003340): Train loss 2.751, Val loss 3.368\n",
            "Ep 1 (Step 003345): Train loss 2.692, Val loss 3.369\n",
            "Ep 1 (Step 003350): Train loss 3.080, Val loss 3.364\n",
            "Ep 1 (Step 003355): Train loss 2.688, Val loss 3.361\n",
            "Ep 1 (Step 003360): Train loss 3.045, Val loss 3.358\n",
            "Ep 1 (Step 003365): Train loss 3.112, Val loss 3.368\n",
            "Ep 1 (Step 003370): Train loss 3.190, Val loss 3.379\n",
            "Ep 1 (Step 003375): Train loss 3.115, Val loss 3.377\n",
            "Ep 1 (Step 003380): Train loss 2.898, Val loss 3.361\n",
            "Ep 1 (Step 003385): Train loss 2.852, Val loss 3.359\n",
            "Ep 1 (Step 003390): Train loss 2.796, Val loss 3.371\n",
            "Ep 1 (Step 003395): Train loss 2.853, Val loss 3.375\n",
            "Ep 1 (Step 003400): Train loss 2.915, Val loss 3.370\n",
            "Ep 1 (Step 003405): Train loss 2.904, Val loss 3.372\n",
            "Ep 1 (Step 003410): Train loss 2.962, Val loss 3.370\n",
            "Ep 1 (Step 003415): Train loss 2.976, Val loss 3.382\n",
            "Ep 1 (Step 003420): Train loss 3.017, Val loss 3.379\n",
            "Ep 1 (Step 003425): Train loss 2.922, Val loss 3.373\n",
            "Ep 1 (Step 003430): Train loss 3.119, Val loss 3.378\n",
            "Ep 1 (Step 003435): Train loss 2.978, Val loss 3.377\n",
            "Ep 1 (Step 003440): Train loss 2.927, Val loss 3.381\n",
            "Ep 1 (Step 003445): Train loss 3.003, Val loss 3.383\n",
            "Ep 1 (Step 003450): Train loss 2.905, Val loss 3.390\n",
            "Ep 1 (Step 003455): Train loss 2.877, Val loss 3.388\n",
            "Ep 1 (Step 003460): Train loss 2.871, Val loss 3.382\n",
            "Ep 1 (Step 003465): Train loss 3.295, Val loss 3.384\n",
            "Ep 1 (Step 003470): Train loss 2.809, Val loss 3.380\n",
            "Ep 1 (Step 003475): Train loss 3.041, Val loss 3.372\n",
            "Ep 1 (Step 003480): Train loss 3.013, Val loss 3.376\n",
            "Ep 1 (Step 003485): Train loss 2.936, Val loss 3.368\n",
            "Ep 1 (Step 003490): Train loss 2.928, Val loss 3.369\n",
            "Ep 1 (Step 003495): Train loss 2.952, Val loss 3.376\n",
            "Ep 1 (Step 003500): Train loss 2.944, Val loss 3.383\n",
            "Ep 1 (Step 003505): Train loss 3.040, Val loss 3.383\n",
            "Ep 1 (Step 003510): Train loss 2.694, Val loss 3.367\n",
            "Ep 1 (Step 003515): Train loss 2.886, Val loss 3.367\n",
            "Ep 1 (Step 003520): Train loss 3.010, Val loss 3.369\n",
            "Ep 1 (Step 003525): Train loss 2.935, Val loss 3.366\n",
            "Ep 1 (Step 003530): Train loss 2.879, Val loss 3.364\n",
            "Ep 1 (Step 003535): Train loss 2.807, Val loss 3.372\n",
            "Ep 1 (Step 003540): Train loss 3.101, Val loss 3.383\n",
            "Ep 1 (Step 003545): Train loss 3.088, Val loss 3.373\n",
            "Ep 1 (Step 003550): Train loss 2.929, Val loss 3.372\n",
            "Ep 1 (Step 003555): Train loss 3.117, Val loss 3.375\n",
            "Ep 1 (Step 003560): Train loss 3.018, Val loss 3.364\n",
            "Ep 1 (Step 003565): Train loss 2.969, Val loss 3.355\n",
            "Ep 1 (Step 003570): Train loss 2.943, Val loss 3.369\n",
            "Ep 1 (Step 003575): Train loss 2.860, Val loss 3.379\n",
            "Ep 1 (Step 003580): Train loss 2.878, Val loss 3.375\n",
            "Ep 1 (Step 003585): Train loss 2.991, Val loss 3.376\n",
            "Ep 1 (Step 003590): Train loss 2.948, Val loss 3.369\n",
            "Ep 1 (Step 003595): Train loss 2.803, Val loss 3.362\n",
            "Ep 1 (Step 003600): Train loss 3.103, Val loss 3.366\n",
            "Ep 1 (Step 003605): Train loss 2.931, Val loss 3.362\n",
            "Ep 1 (Step 003610): Train loss 2.936, Val loss 3.369\n",
            "Ep 1 (Step 003615): Train loss 2.754, Val loss 3.369\n",
            "Ep 1 (Step 003620): Train loss 2.953, Val loss 3.368\n",
            "Ep 1 (Step 003625): Train loss 2.842, Val loss 3.357\n",
            "Ep 1 (Step 003630): Train loss 2.962, Val loss 3.355\n",
            "Ep 1 (Step 003635): Train loss 2.901, Val loss 3.361\n",
            "Ep 1 (Step 003640): Train loss 3.073, Val loss 3.366\n",
            "Ep 1 (Step 003645): Train loss 2.817, Val loss 3.360\n",
            "Ep 1 (Step 003650): Train loss 2.892, Val loss 3.346\n",
            "Ep 1 (Step 003655): Train loss 2.962, Val loss 3.348\n",
            "Ep 1 (Step 003660): Train loss 3.072, Val loss 3.351\n",
            "Ep 1 (Step 003665): Train loss 3.016, Val loss 3.344\n",
            "Ep 1 (Step 003670): Train loss 2.977, Val loss 3.333\n",
            "Ep 1 (Step 003675): Train loss 2.885, Val loss 3.327\n",
            "Ep 1 (Step 003680): Train loss 2.918, Val loss 3.336\n",
            "Ep 1 (Step 003685): Train loss 2.906, Val loss 3.343\n",
            "Ep 1 (Step 003690): Train loss 3.055, Val loss 3.351\n",
            "Ep 1 (Step 003695): Train loss 2.778, Val loss 3.358\n",
            "Ep 1 (Step 003700): Train loss 2.988, Val loss 3.347\n",
            "Ep 1 (Step 003705): Train loss 2.809, Val loss 3.343\n",
            "Ep 1 (Step 003710): Train loss 2.889, Val loss 3.334\n",
            "Ep 1 (Step 003715): Train loss 2.871, Val loss 3.336\n",
            "Ep 1 (Step 003720): Train loss 2.781, Val loss 3.340\n",
            "Ep 1 (Step 003725): Train loss 2.865, Val loss 3.332\n",
            "Ep 1 (Step 003730): Train loss 2.820, Val loss 3.332\n",
            "Ep 1 (Step 003735): Train loss 3.124, Val loss 3.326\n",
            "Ep 1 (Step 003740): Train loss 2.975, Val loss 3.332\n",
            "Ep 1 (Step 003745): Train loss 3.006, Val loss 3.332\n",
            "Ep 1 (Step 003750): Train loss 2.968, Val loss 3.329\n",
            "Ep 1 (Step 003755): Train loss 3.153, Val loss 3.338\n",
            "Ep 1 (Step 003760): Train loss 3.042, Val loss 3.341\n",
            "Ep 1 (Step 003765): Train loss 2.860, Val loss 3.345\n",
            "Ep 1 (Step 003770): Train loss 3.027, Val loss 3.341\n",
            "Ep 1 (Step 003775): Train loss 3.011, Val loss 3.338\n",
            "Ep 1 (Step 003780): Train loss 2.869, Val loss 3.344\n",
            "Ep 1 (Step 003785): Train loss 2.734, Val loss 3.348\n",
            "Ep 1 (Step 003790): Train loss 2.836, Val loss 3.339\n",
            "Ep 1 (Step 003795): Train loss 2.727, Val loss 3.341\n",
            "Ep 1 (Step 003800): Train loss 2.994, Val loss 3.342\n",
            "Ep 1 (Step 003805): Train loss 3.054, Val loss 3.348\n",
            "Ep 1 (Step 003810): Train loss 2.999, Val loss 3.338\n",
            "Ep 1 (Step 003815): Train loss 3.057, Val loss 3.343\n",
            "Ep 1 (Step 003820): Train loss 2.786, Val loss 3.345\n",
            "Ep 1 (Step 003825): Train loss 2.893, Val loss 3.347\n",
            "Ep 1 (Step 003830): Train loss 3.023, Val loss 3.348\n",
            "Ep 1 (Step 003835): Train loss 3.151, Val loss 3.357\n",
            "Ep 1 (Step 003840): Train loss 2.893, Val loss 3.349\n",
            "Ep 1 (Step 003845): Train loss 2.609, Val loss 3.346\n",
            "Ep 1 (Step 003850): Train loss 3.107, Val loss 3.345\n",
            "Ep 1 (Step 003855): Train loss 2.834, Val loss 3.346\n",
            "Ep 1 (Step 003860): Train loss 2.916, Val loss 3.343\n",
            "Ep 1 (Step 003865): Train loss 2.814, Val loss 3.344\n",
            "Ep 1 (Step 003870): Train loss 2.893, Val loss 3.348\n",
            "Ep 1 (Step 003875): Train loss 3.024, Val loss 3.345\n",
            "Ep 1 (Step 003880): Train loss 2.807, Val loss 3.344\n",
            "Ep 1 (Step 003885): Train loss 2.728, Val loss 3.339\n",
            "Ep 1 (Step 003890): Train loss 2.869, Val loss 3.337\n",
            "Ep 1 (Step 003895): Train loss 2.884, Val loss 3.338\n",
            "Ep 1 (Step 003900): Train loss 2.810, Val loss 3.347\n",
            "Ep 1 (Step 003905): Train loss 2.975, Val loss 3.337\n",
            "Ep 1 (Step 003910): Train loss 3.005, Val loss 3.335\n",
            "Ep 1 (Step 003915): Train loss 2.860, Val loss 3.339\n",
            "Ep 1 (Step 003920): Train loss 2.843, Val loss 3.337\n",
            "Ep 1 (Step 003925): Train loss 2.958, Val loss 3.336\n",
            "Ep 1 (Step 003930): Train loss 2.820, Val loss 3.343\n",
            "Ep 1 (Step 003935): Train loss 2.852, Val loss 3.332\n",
            "Ep 1 (Step 003940): Train loss 2.987, Val loss 3.325\n",
            "Ep 1 (Step 003945): Train loss 2.872, Val loss 3.333\n",
            "Ep 1 (Step 003950): Train loss 2.929, Val loss 3.336\n",
            "Ep 1 (Step 003955): Train loss 2.723, Val loss 3.335\n",
            "Ep 1 (Step 003960): Train loss 2.973, Val loss 3.324\n",
            "Ep 1 (Step 003965): Train loss 2.948, Val loss 3.327\n",
            "Ep 1 (Step 003970): Train loss 2.725, Val loss 3.339\n",
            "Ep 1 (Step 003975): Train loss 2.970, Val loss 3.335\n",
            "Ep 1 (Step 003980): Train loss 2.930, Val loss 3.330\n",
            "Ep 1 (Step 003985): Train loss 2.828, Val loss 3.326\n",
            "Ep 1 (Step 003990): Train loss 2.869, Val loss 3.325\n",
            "Ep 1 (Step 003995): Train loss 2.896, Val loss 3.328\n",
            "Ep 1 (Step 004000): Train loss 2.857, Val loss 3.327\n",
            "Ep 1 (Step 004005): Train loss 2.947, Val loss 3.331\n",
            "Ep 1 (Step 004010): Train loss 2.829, Val loss 3.344\n",
            "Ep 1 (Step 004015): Train loss 3.051, Val loss 3.348\n",
            "Ep 1 (Step 004020): Train loss 2.736, Val loss 3.336\n",
            "Ep 1 (Step 004025): Train loss 2.884, Val loss 3.325\n",
            "Ep 1 (Step 004030): Train loss 2.842, Val loss 3.328\n",
            "Ep 1 (Step 004035): Train loss 3.006, Val loss 3.326\n",
            "Ep 1 (Step 004040): Train loss 2.826, Val loss 3.329\n",
            "Ep 1 (Step 004045): Train loss 2.941, Val loss 3.332\n",
            "Ep 1 (Step 004050): Train loss 2.935, Val loss 3.342\n",
            "Ep 1 (Step 004055): Train loss 2.872, Val loss 3.332\n",
            "Ep 1 (Step 004060): Train loss 2.863, Val loss 3.337\n",
            "Ep 1 (Step 004065): Train loss 2.875, Val loss 3.334\n",
            "Ep 1 (Step 004070): Train loss 2.868, Val loss 3.341\n",
            "Ep 1 (Step 004075): Train loss 2.859, Val loss 3.337\n",
            "Ep 1 (Step 004080): Train loss 3.108, Val loss 3.333\n",
            "Ep 1 (Step 004085): Train loss 2.943, Val loss 3.342\n",
            "Ep 1 (Step 004090): Train loss 2.796, Val loss 3.332\n",
            "Ep 1 (Step 004095): Train loss 2.973, Val loss 3.329\n",
            "Ep 1 (Step 004100): Train loss 2.807, Val loss 3.325\n",
            "Ep 1 (Step 004105): Train loss 3.042, Val loss 3.321\n",
            "Ep 1 (Step 004110): Train loss 2.831, Val loss 3.313\n",
            "Ep 1 (Step 004115): Train loss 2.877, Val loss 3.319\n",
            "Ep 1 (Step 004120): Train loss 2.787, Val loss 3.311\n",
            "Ep 1 (Step 004125): Train loss 3.058, Val loss 3.316\n",
            "Ep 1 (Step 004130): Train loss 2.816, Val loss 3.323\n",
            "Ep 1 (Step 004135): Train loss 2.983, Val loss 3.326\n",
            "Ep 1 (Step 004140): Train loss 3.004, Val loss 3.321\n",
            "Ep 1 (Step 004145): Train loss 2.762, Val loss 3.330\n",
            "Ep 1 (Step 004150): Train loss 2.923, Val loss 3.333\n",
            "Ep 1 (Step 004155): Train loss 2.832, Val loss 3.332\n",
            "Ep 1 (Step 004160): Train loss 2.871, Val loss 3.330\n",
            "Ep 1 (Step 004165): Train loss 2.716, Val loss 3.345\n",
            "Ep 1 (Step 004170): Train loss 2.919, Val loss 3.358\n",
            "Ep 1 (Step 004175): Train loss 2.961, Val loss 3.336\n",
            "Ep 1 (Step 004180): Train loss 2.694, Val loss 3.330\n",
            "Ep 1 (Step 004185): Train loss 2.836, Val loss 3.326\n",
            "Ep 1 (Step 004190): Train loss 2.762, Val loss 3.322\n",
            "Ep 1 (Step 004195): Train loss 3.061, Val loss 3.330\n",
            "Ep 1 (Step 004200): Train loss 2.829, Val loss 3.336\n",
            "Ep 1 (Step 004205): Train loss 2.976, Val loss 3.336\n",
            "Ep 1 (Step 004210): Train loss 2.953, Val loss 3.336\n",
            "Ep 1 (Step 004215): Train loss 2.856, Val loss 3.336\n",
            "Ep 1 (Step 004220): Train loss 2.850, Val loss 3.344\n",
            "Ep 1 (Step 004225): Train loss 2.991, Val loss 3.345\n",
            "Ep 1 (Step 004230): Train loss 2.782, Val loss 3.347\n",
            "Ep 1 (Step 004235): Train loss 2.940, Val loss 3.343\n",
            "Ep 1 (Step 004240): Train loss 3.101, Val loss 3.343\n",
            "Ep 1 (Step 004245): Train loss 2.830, Val loss 3.343\n",
            "Ep 1 (Step 004250): Train loss 2.730, Val loss 3.343\n",
            "Ep 1 (Step 004255): Train loss 2.964, Val loss 3.350\n",
            "Ep 1 (Step 004260): Train loss 2.835, Val loss 3.339\n",
            "Ep 1 (Step 004265): Train loss 2.909, Val loss 3.331\n",
            "Ep 1 (Step 004270): Train loss 2.795, Val loss 3.332\n",
            "Ep 1 (Step 004275): Train loss 2.874, Val loss 3.339\n",
            "Ep 1 (Step 004280): Train loss 2.978, Val loss 3.337\n",
            "Ep 1 (Step 004285): Train loss 2.917, Val loss 3.347\n",
            "Ep 1 (Step 004290): Train loss 2.834, Val loss 3.356\n",
            "Ep 1 (Step 004295): Train loss 2.980, Val loss 3.354\n",
            "Ep 1 (Step 004300): Train loss 2.918, Val loss 3.350\n",
            "Ep 1 (Step 004305): Train loss 2.898, Val loss 3.345\n",
            "Ep 1 (Step 004310): Train loss 3.160, Val loss 3.342\n",
            "Ep 1 (Step 004315): Train loss 3.050, Val loss 3.334\n",
            "Ep 1 (Step 004320): Train loss 2.802, Val loss 3.327\n",
            "Ep 1 (Step 004325): Train loss 2.807, Val loss 3.328\n",
            "Ep 1 (Step 004330): Train loss 2.812, Val loss 3.329\n",
            "Ep 1 (Step 004335): Train loss 2.826, Val loss 3.334\n",
            "Ep 1 (Step 004340): Train loss 3.007, Val loss 3.339\n",
            "Ep 1 (Step 004345): Train loss 3.121, Val loss 3.338\n",
            "Ep 1 (Step 004350): Train loss 2.715, Val loss 3.328\n",
            "Ep 1 (Step 004355): Train loss 2.968, Val loss 3.323\n",
            "Ep 1 (Step 004360): Train loss 2.777, Val loss 3.323\n",
            "Ep 1 (Step 004365): Train loss 2.882, Val loss 3.326\n",
            "Ep 1 (Step 004370): Train loss 2.726, Val loss 3.339\n",
            "Ep 1 (Step 004375): Train loss 2.778, Val loss 3.339\n",
            "Ep 1 (Step 004380): Train loss 2.920, Val loss 3.334\n",
            "Ep 1 (Step 004385): Train loss 3.034, Val loss 3.324\n",
            "Ep 1 (Step 004390): Train loss 2.906, Val loss 3.314\n",
            "Ep 1 (Step 004395): Train loss 2.708, Val loss 3.316\n",
            "Ep 1 (Step 004400): Train loss 2.834, Val loss 3.317\n",
            "Ep 1 (Step 004405): Train loss 2.872, Val loss 3.305\n",
            "Ep 1 (Step 004410): Train loss 2.823, Val loss 3.306\n",
            "Ep 1 (Step 004415): Train loss 2.949, Val loss 3.308\n",
            "Ep 1 (Step 004420): Train loss 2.853, Val loss 3.320\n",
            "Ep 1 (Step 004425): Train loss 2.920, Val loss 3.317\n",
            "Ep 1 (Step 004430): Train loss 2.813, Val loss 3.310\n",
            "Ep 1 (Step 004435): Train loss 2.925, Val loss 3.307\n",
            "Ep 1 (Step 004440): Train loss 2.801, Val loss 3.318\n",
            "Ep 1 (Step 004445): Train loss 2.700, Val loss 3.315\n",
            "Ep 1 (Step 004450): Train loss 2.979, Val loss 3.309\n",
            "Ep 1 (Step 004455): Train loss 2.916, Val loss 3.301\n",
            "Ep 1 (Step 004460): Train loss 2.903, Val loss 3.305\n",
            "Ep 1 (Step 004465): Train loss 2.891, Val loss 3.303\n",
            "Ep 1 (Step 004470): Train loss 3.107, Val loss 3.303\n",
            "Ep 1 (Step 004475): Train loss 2.680, Val loss 3.313\n",
            "Ep 1 (Step 004480): Train loss 2.978, Val loss 3.307\n",
            "Ep 1 (Step 004485): Train loss 3.082, Val loss 3.312\n",
            "Ep 1 (Step 004490): Train loss 2.911, Val loss 3.321\n",
            "Ep 1 (Step 004495): Train loss 2.710, Val loss 3.329\n",
            "Ep 1 (Step 004500): Train loss 2.920, Val loss 3.325\n",
            "Ep 1 (Step 004505): Train loss 2.799, Val loss 3.330\n",
            "Ep 1 (Step 004510): Train loss 2.851, Val loss 3.323\n",
            "Ep 1 (Step 004515): Train loss 2.799, Val loss 3.316\n",
            "Ep 1 (Step 004520): Train loss 2.924, Val loss 3.319\n",
            "Ep 1 (Step 004525): Train loss 2.597, Val loss 3.312\n",
            "Ep 1 (Step 004530): Train loss 2.750, Val loss 3.311\n",
            "Ep 1 (Step 004535): Train loss 2.684, Val loss 3.311\n",
            "Ep 1 (Step 004540): Train loss 2.706, Val loss 3.313\n",
            "Ep 1 (Step 004545): Train loss 2.934, Val loss 3.310\n",
            "Ep 1 (Step 004550): Train loss 2.891, Val loss 3.304\n",
            "Ep 1 (Step 004555): Train loss 2.944, Val loss 3.308\n",
            "Ep 1 (Step 004560): Train loss 2.773, Val loss 3.312\n",
            "Ep 1 (Step 004565): Train loss 2.898, Val loss 3.303\n",
            "Ep 1 (Step 004570): Train loss 2.854, Val loss 3.300\n",
            "Ep 1 (Step 004575): Train loss 2.999, Val loss 3.308\n",
            "Ep 1 (Step 004580): Train loss 2.992, Val loss 3.318\n",
            "Ep 1 (Step 004585): Train loss 3.057, Val loss 3.299\n",
            "Ep 1 (Step 004590): Train loss 2.891, Val loss 3.290\n",
            "Ep 1 (Step 004595): Train loss 2.787, Val loss 3.297\n",
            "Ep 1 (Step 004600): Train loss 2.739, Val loss 3.305\n",
            "Ep 1 (Step 004605): Train loss 2.937, Val loss 3.306\n",
            "Ep 1 (Step 004610): Train loss 2.958, Val loss 3.314\n",
            "Ep 1 (Step 004615): Train loss 2.890, Val loss 3.312\n",
            "Ep 1 (Step 004620): Train loss 2.981, Val loss 3.311\n",
            "Ep 1 (Step 004625): Train loss 2.829, Val loss 3.308\n",
            "Ep 1 (Step 004630): Train loss 2.852, Val loss 3.303\n",
            "Ep 1 (Step 004635): Train loss 3.072, Val loss 3.309\n",
            "Ep 1 (Step 004640): Train loss 2.823, Val loss 3.308\n",
            "Ep 1 (Step 004645): Train loss 2.839, Val loss 3.309\n",
            "Ep 1 (Step 004650): Train loss 2.926, Val loss 3.308\n",
            "Ep 1 (Step 004655): Train loss 2.969, Val loss 3.308\n",
            "Ep 1 (Step 004660): Train loss 2.876, Val loss 3.301\n",
            "Ep 1 (Step 004665): Train loss 2.679, Val loss 3.303\n",
            "Ep 1 (Step 004670): Train loss 2.893, Val loss 3.306\n",
            "Ep 1 (Step 004675): Train loss 2.974, Val loss 3.318\n",
            "Ep 1 (Step 004680): Train loss 3.030, Val loss 3.319\n",
            "Ep 1 (Step 004685): Train loss 2.820, Val loss 3.310\n",
            "Ep 1 (Step 004690): Train loss 2.830, Val loss 3.308\n",
            "Ep 1 (Step 004695): Train loss 2.671, Val loss 3.312\n",
            "Ep 1 (Step 004700): Train loss 2.716, Val loss 3.306\n",
            "Ep 1 (Step 004705): Train loss 3.033, Val loss 3.302\n",
            "Ep 1 (Step 004710): Train loss 2.736, Val loss 3.299\n",
            "Ep 1 (Step 004715): Train loss 2.604, Val loss 3.303\n",
            "Ep 1 (Step 004720): Train loss 2.894, Val loss 3.302\n",
            "Ep 1 (Step 004725): Train loss 2.833, Val loss 3.311\n",
            "Ep 1 (Step 004730): Train loss 2.996, Val loss 3.304\n",
            "Ep 1 (Step 004735): Train loss 3.076, Val loss 3.308\n",
            "Ep 1 (Step 004740): Train loss 2.826, Val loss 3.307\n",
            "Ep 1 (Step 004745): Train loss 2.800, Val loss 3.308\n",
            "Ep 1 (Step 004750): Train loss 2.928, Val loss 3.313\n",
            "Ep 1 (Step 004755): Train loss 2.937, Val loss 3.309\n",
            "Ep 1 (Step 004760): Train loss 2.963, Val loss 3.310\n",
            "Ep 1 (Step 004765): Train loss 2.896, Val loss 3.304\n",
            "Ep 1 (Step 004770): Train loss 2.812, Val loss 3.303\n",
            "Ep 1 (Step 004775): Train loss 2.802, Val loss 3.299\n",
            "Ep 1 (Step 004780): Train loss 2.501, Val loss 3.304\n",
            "Ep 1 (Step 004785): Train loss 2.911, Val loss 3.308\n",
            "Ep 1 (Step 004790): Train loss 2.692, Val loss 3.304\n",
            "Ep 1 (Step 004795): Train loss 2.859, Val loss 3.295\n",
            "Ep 1 (Step 004800): Train loss 2.986, Val loss 3.298\n",
            "Ep 1 (Step 004805): Train loss 2.553, Val loss 3.316\n",
            "Ep 1 (Step 004810): Train loss 2.827, Val loss 3.308\n",
            "Ep 1 (Step 004815): Train loss 2.780, Val loss 3.292\n",
            "Ep 1 (Step 004820): Train loss 2.761, Val loss 3.286\n",
            "Ep 1 (Step 004825): Train loss 2.949, Val loss 3.297\n",
            "Ep 1 (Step 004830): Train loss 2.888, Val loss 3.300\n",
            "Ep 1 (Step 004835): Train loss 2.958, Val loss 3.299\n",
            "Ep 1 (Step 004840): Train loss 2.739, Val loss 3.295\n",
            "Ep 1 (Step 004845): Train loss 2.858, Val loss 3.292\n",
            "Ep 1 (Step 004850): Train loss 2.778, Val loss 3.289\n",
            "Ep 1 (Step 004855): Train loss 2.802, Val loss 3.292\n",
            "Ep 1 (Step 004860): Train loss 2.769, Val loss 3.288\n",
            "Ep 1 (Step 004865): Train loss 2.803, Val loss 3.295\n",
            "Ep 1 (Step 004870): Train loss 3.031, Val loss 3.293\n",
            "Ep 1 (Step 004875): Train loss 3.023, Val loss 3.295\n",
            "Ep 1 (Step 004880): Train loss 2.851, Val loss 3.298\n",
            "Ep 1 (Step 004885): Train loss 2.715, Val loss 3.293\n",
            "Ep 1 (Step 004890): Train loss 2.834, Val loss 3.286\n",
            "Ep 1 (Step 004895): Train loss 2.915, Val loss 3.281\n",
            "Ep 1 (Step 004900): Train loss 2.857, Val loss 3.287\n",
            "Ep 1 (Step 004905): Train loss 2.814, Val loss 3.296\n",
            "Ep 1 (Step 004910): Train loss 2.902, Val loss 3.300\n",
            "Ep 1 (Step 004915): Train loss 3.094, Val loss 3.306\n",
            "Ep 1 (Step 004920): Train loss 2.882, Val loss 3.291\n",
            "Ep 1 (Step 004925): Train loss 2.801, Val loss 3.283\n",
            "Ep 1 (Step 004930): Train loss 2.892, Val loss 3.276\n",
            "Ep 1 (Step 004935): Train loss 2.849, Val loss 3.289\n",
            "Ep 1 (Step 004940): Train loss 2.991, Val loss 3.289\n",
            "Ep 1 (Step 004945): Train loss 2.865, Val loss 3.302\n",
            "Ep 1 (Step 004950): Train loss 2.811, Val loss 3.299\n",
            "Ep 1 (Step 004955): Train loss 2.726, Val loss 3.289\n",
            "Ep 1 (Step 004960): Train loss 2.939, Val loss 3.287\n",
            "Ep 1 (Step 004965): Train loss 2.731, Val loss 3.293\n",
            "Ep 1 (Step 004970): Train loss 2.935, Val loss 3.293\n",
            "Ep 1 (Step 004975): Train loss 2.977, Val loss 3.285\n",
            "Ep 1 (Step 004980): Train loss 2.786, Val loss 3.283\n",
            "Ep 1 (Step 004985): Train loss 3.013, Val loss 3.286\n",
            "Ep 1 (Step 004990): Train loss 2.797, Val loss 3.290\n",
            "Ep 1 (Step 004995): Train loss 2.908, Val loss 3.291\n",
            "Ep 1 (Step 005000): Train loss 2.821, Val loss 3.289\n",
            "Ep 1 (Step 005005): Train loss 2.696, Val loss 3.288\n",
            "Ep 1 (Step 005010): Train loss 2.586, Val loss 3.291\n",
            "Ep 1 (Step 005015): Train loss 2.746, Val loss 3.296\n",
            "Ep 1 (Step 005020): Train loss 2.650, Val loss 3.305\n",
            "Ep 1 (Step 005025): Train loss 2.605, Val loss 3.300\n",
            "Ep 1 (Step 005030): Train loss 2.816, Val loss 3.291\n",
            "Ep 1 (Step 005035): Train loss 2.653, Val loss 3.284\n",
            "Ep 1 (Step 005040): Train loss 2.718, Val loss 3.281\n",
            "Ep 1 (Step 005045): Train loss 2.735, Val loss 3.293\n",
            "Ep 1 (Step 005050): Train loss 2.838, Val loss 3.299\n",
            "Ep 1 (Step 005055): Train loss 2.783, Val loss 3.294\n",
            "Ep 1 (Step 005060): Train loss 2.582, Val loss 3.290\n",
            "Ep 1 (Step 005065): Train loss 2.915, Val loss 3.285\n",
            "Ep 1 (Step 005070): Train loss 2.729, Val loss 3.285\n",
            "Ep 1 (Step 005075): Train loss 2.668, Val loss 3.293\n",
            "Ep 1 (Step 005080): Train loss 2.756, Val loss 3.297\n",
            "Ep 1 (Step 005085): Train loss 2.887, Val loss 3.296\n",
            "Ep 1 (Step 005090): Train loss 2.941, Val loss 3.292\n",
            "Ep 1 (Step 005095): Train loss 2.818, Val loss 3.297\n",
            "Ep 1 (Step 005100): Train loss 2.846, Val loss 3.293\n",
            "Ep 1 (Step 005105): Train loss 2.684, Val loss 3.294\n",
            "Ep 1 (Step 005110): Train loss 2.813, Val loss 3.294\n",
            "Ep 1 (Step 005115): Train loss 2.734, Val loss 3.291\n",
            "Ep 1 (Step 005120): Train loss 2.773, Val loss 3.286\n",
            "Ep 1 (Step 005125): Train loss 2.631, Val loss 3.281\n",
            "Ep 1 (Step 005130): Train loss 2.620, Val loss 3.287\n",
            "Ep 1 (Step 005135): Train loss 3.038, Val loss 3.292\n",
            "Ep 1 (Step 005140): Train loss 2.733, Val loss 3.288\n",
            "Ep 1 (Step 005145): Train loss 2.708, Val loss 3.283\n",
            "Ep 1 (Step 005150): Train loss 2.830, Val loss 3.285\n",
            "Ep 1 (Step 005155): Train loss 2.681, Val loss 3.291\n",
            "Ep 1 (Step 005160): Train loss 2.897, Val loss 3.287\n",
            "Ep 1 (Step 005165): Train loss 2.946, Val loss 3.284\n",
            "Ep 1 (Step 005170): Train loss 2.805, Val loss 3.288\n",
            "Ep 1 (Step 005175): Train loss 2.788, Val loss 3.282\n",
            "Ep 1 (Step 005180): Train loss 2.738, Val loss 3.295\n",
            "Ep 1 (Step 005185): Train loss 2.959, Val loss 3.301\n",
            "Ep 1 (Step 005190): Train loss 2.776, Val loss 3.287\n",
            "Ep 1 (Step 005195): Train loss 2.798, Val loss 3.293\n",
            "Ep 1 (Step 005200): Train loss 2.782, Val loss 3.290\n",
            "Ep 1 (Step 005205): Train loss 2.795, Val loss 3.287\n",
            "Ep 1 (Step 005210): Train loss 2.784, Val loss 3.288\n",
            "Ep 1 (Step 005215): Train loss 2.799, Val loss 3.283\n",
            "Ep 1 (Step 005220): Train loss 2.819, Val loss 3.280\n",
            "Ep 1 (Step 005225): Train loss 2.848, Val loss 3.283\n",
            "Ep 1 (Step 005230): Train loss 2.764, Val loss 3.283\n",
            "Ep 1 (Step 005235): Train loss 2.986, Val loss 3.289\n",
            "Ep 1 (Step 005240): Train loss 3.009, Val loss 3.290\n",
            "Ep 1 (Step 005245): Train loss 2.741, Val loss 3.288\n",
            "Ep 1 (Step 005250): Train loss 2.970, Val loss 3.279\n",
            "Ep 1 (Step 005255): Train loss 2.984, Val loss 3.281\n",
            "Ep 1 (Step 005260): Train loss 2.783, Val loss 3.293\n",
            "Ep 1 (Step 005265): Train loss 2.870, Val loss 3.283\n",
            "Ep 1 (Step 005270): Train loss 2.800, Val loss 3.281\n",
            "Ep 1 (Step 005275): Train loss 3.021, Val loss 3.300\n",
            "Ep 1 (Step 005280): Train loss 2.913, Val loss 3.292\n",
            "Ep 1 (Step 005285): Train loss 2.956, Val loss 3.288\n",
            "Ep 1 (Step 005290): Train loss 2.886, Val loss 3.283\n",
            "Ep 1 (Step 005295): Train loss 2.768, Val loss 3.290\n",
            "Ep 1 (Step 005300): Train loss 2.768, Val loss 3.296\n",
            "Ep 1 (Step 005305): Train loss 2.762, Val loss 3.294\n",
            "Ep 1 (Step 005310): Train loss 2.636, Val loss 3.293\n",
            "Ep 1 (Step 005315): Train loss 2.878, Val loss 3.298\n",
            "Ep 1 (Step 005320): Train loss 2.578, Val loss 3.286\n",
            "Ep 1 (Step 005325): Train loss 2.808, Val loss 3.276\n",
            "Ep 1 (Step 005330): Train loss 2.851, Val loss 3.286\n",
            "Ep 1 (Step 005335): Train loss 2.859, Val loss 3.303\n",
            "Ep 1 (Step 005340): Train loss 2.939, Val loss 3.291\n",
            "Ep 1 (Step 005345): Train loss 2.716, Val loss 3.276\n",
            "Ep 1 (Step 005350): Train loss 2.934, Val loss 3.276\n",
            "Ep 1 (Step 005355): Train loss 2.696, Val loss 3.276\n",
            "Ep 1 (Step 005360): Train loss 2.759, Val loss 3.270\n",
            "Ep 1 (Step 005365): Train loss 2.818, Val loss 3.277\n",
            "Ep 1 (Step 005370): Train loss 2.803, Val loss 3.268\n",
            "Ep 1 (Step 005375): Train loss 2.754, Val loss 3.260\n",
            "Ep 1 (Step 005380): Train loss 2.922, Val loss 3.264\n",
            "Ep 1 (Step 005385): Train loss 2.737, Val loss 3.273\n",
            "Ep 1 (Step 005390): Train loss 2.797, Val loss 3.281\n",
            "Ep 1 (Step 005395): Train loss 2.859, Val loss 3.278\n",
            "Ep 1 (Step 005400): Train loss 2.828, Val loss 3.278\n",
            "Ep 1 (Step 005405): Train loss 2.871, Val loss 3.279\n",
            "Ep 1 (Step 005410): Train loss 2.893, Val loss 3.275\n",
            "Ep 1 (Step 005415): Train loss 2.889, Val loss 3.266\n",
            "Ep 1 (Step 005420): Train loss 2.670, Val loss 3.257\n",
            "Ep 1 (Step 005425): Train loss 2.827, Val loss 3.263\n",
            "Ep 1 (Step 005430): Train loss 2.822, Val loss 3.267\n",
            "Ep 1 (Step 005435): Train loss 2.919, Val loss 3.262\n",
            "Ep 1 (Step 005440): Train loss 2.780, Val loss 3.266\n",
            "Ep 1 (Step 005445): Train loss 2.698, Val loss 3.257\n",
            "Ep 1 (Step 005450): Train loss 2.782, Val loss 3.267\n",
            "Ep 1 (Step 005455): Train loss 2.850, Val loss 3.265\n",
            "Ep 1 (Step 005460): Train loss 2.537, Val loss 3.258\n",
            "Ep 1 (Step 005465): Train loss 2.943, Val loss 3.258\n",
            "Ep 1 (Step 005470): Train loss 2.801, Val loss 3.263\n",
            "Ep 1 (Step 005475): Train loss 2.601, Val loss 3.264\n",
            "Ep 1 (Step 005480): Train loss 2.660, Val loss 3.265\n",
            "Ep 1 (Step 005485): Train loss 2.703, Val loss 3.260\n",
            "Ep 1 (Step 005490): Train loss 2.663, Val loss 3.259\n",
            "Ep 1 (Step 005495): Train loss 2.596, Val loss 3.262\n",
            "Ep 1 (Step 005500): Train loss 2.885, Val loss 3.268\n",
            "Ep 1 (Step 005505): Train loss 2.924, Val loss 3.284\n",
            "Ep 1 (Step 005510): Train loss 2.906, Val loss 3.276\n",
            "Ep 1 (Step 005515): Train loss 2.970, Val loss 3.268\n",
            "Ep 1 (Step 005520): Train loss 3.038, Val loss 3.263\n",
            "Ep 1 (Step 005525): Train loss 2.561, Val loss 3.268\n",
            "Ep 1 (Step 005530): Train loss 2.856, Val loss 3.265\n",
            "Ep 1 (Step 005535): Train loss 2.667, Val loss 3.265\n",
            "Ep 1 (Step 005540): Train loss 2.721, Val loss 3.265\n",
            "Ep 1 (Step 005545): Train loss 2.753, Val loss 3.266\n",
            "Ep 1 (Step 005550): Train loss 2.775, Val loss 3.266\n",
            "Ep 1 (Step 005555): Train loss 2.816, Val loss 3.263\n",
            "Ep 1 (Step 005560): Train loss 2.695, Val loss 3.257\n",
            "Ep 1 (Step 005565): Train loss 2.817, Val loss 3.265\n",
            "Ep 1 (Step 005570): Train loss 2.757, Val loss 3.263\n",
            "Ep 1 (Step 005575): Train loss 2.709, Val loss 3.262\n",
            "Ep 1 (Step 005580): Train loss 2.854, Val loss 3.260\n",
            "Ep 1 (Step 005585): Train loss 2.822, Val loss 3.257\n",
            "Ep 1 (Step 005590): Train loss 2.685, Val loss 3.269\n",
            "Ep 1 (Step 005595): Train loss 2.710, Val loss 3.248\n",
            "Ep 1 (Step 005600): Train loss 2.676, Val loss 3.246\n",
            "Ep 1 (Step 005605): Train loss 2.631, Val loss 3.257\n",
            "Ep 1 (Step 005610): Train loss 2.958, Val loss 3.264\n",
            "Ep 1 (Step 005615): Train loss 2.877, Val loss 3.250\n",
            "Ep 1 (Step 005620): Train loss 2.842, Val loss 3.244\n",
            "Ep 1 (Step 005625): Train loss 2.647, Val loss 3.248\n",
            "Ep 1 (Step 005630): Train loss 2.575, Val loss 3.260\n",
            "Ep 1 (Step 005635): Train loss 2.637, Val loss 3.254\n",
            "Ep 1 (Step 005640): Train loss 2.859, Val loss 3.245\n",
            "Ep 1 (Step 005645): Train loss 2.655, Val loss 3.243\n",
            "Ep 1 (Step 005650): Train loss 2.822, Val loss 3.247\n",
            "Ep 1 (Step 005655): Train loss 2.863, Val loss 3.250\n",
            "Ep 1 (Step 005660): Train loss 2.820, Val loss 3.242\n",
            "Ep 1 (Step 005665): Train loss 2.699, Val loss 3.235\n",
            "Ep 1 (Step 005670): Train loss 2.788, Val loss 3.241\n",
            "Ep 1 (Step 005675): Train loss 2.798, Val loss 3.249\n",
            "Ep 1 (Step 005680): Train loss 2.576, Val loss 3.253\n",
            "Ep 1 (Step 005685): Train loss 2.691, Val loss 3.250\n",
            "Ep 1 (Step 005690): Train loss 2.782, Val loss 3.243\n",
            "Ep 1 (Step 005695): Train loss 2.841, Val loss 3.244\n",
            "Ep 1 (Step 005700): Train loss 2.523, Val loss 3.246\n",
            "Ep 1 (Step 005705): Train loss 3.096, Val loss 3.243\n",
            "Ep 1 (Step 005710): Train loss 2.538, Val loss 3.253\n",
            "Ep 1 (Step 005715): Train loss 2.727, Val loss 3.253\n",
            "Ep 1 (Step 005720): Train loss 2.808, Val loss 3.255\n",
            "Ep 1 (Step 005725): Train loss 2.762, Val loss 3.242\n",
            "Ep 1 (Step 005730): Train loss 2.877, Val loss 3.241\n",
            "Ep 1 (Step 005735): Train loss 3.107, Val loss 3.240\n",
            "Ep 1 (Step 005740): Train loss 2.802, Val loss 3.242\n",
            "Ep 1 (Step 005745): Train loss 2.866, Val loss 3.243\n",
            "Ep 1 (Step 005750): Train loss 2.691, Val loss 3.238\n",
            "Ep 1 (Step 005755): Train loss 2.732, Val loss 3.247\n",
            "Ep 1 (Step 005760): Train loss 2.827, Val loss 3.247\n",
            "Ep 1 (Step 005765): Train loss 2.563, Val loss 3.236\n",
            "Ep 1 (Step 005770): Train loss 2.885, Val loss 3.237\n",
            "Ep 1 (Step 005775): Train loss 2.576, Val loss 3.242\n",
            "Ep 1 (Step 005780): Train loss 2.671, Val loss 3.237\n",
            "Ep 1 (Step 005785): Train loss 2.821, Val loss 3.243\n",
            "Ep 1 (Step 005790): Train loss 2.702, Val loss 3.245\n",
            "Ep 1 (Step 005795): Train loss 2.799, Val loss 3.247\n",
            "Ep 1 (Step 005800): Train loss 2.599, Val loss 3.256\n",
            "Ep 1 (Step 005805): Train loss 2.936, Val loss 3.247\n",
            "Ep 1 (Step 005810): Train loss 2.783, Val loss 3.253\n",
            "Ep 1 (Step 005815): Train loss 2.922, Val loss 3.253\n",
            "Ep 1 (Step 005820): Train loss 2.650, Val loss 3.248\n",
            "Ep 1 (Step 005825): Train loss 2.804, Val loss 3.246\n",
            "Ep 1 (Step 005830): Train loss 2.957, Val loss 3.254\n",
            "Ep 1 (Step 005835): Train loss 2.717, Val loss 3.260\n",
            "Ep 1 (Step 005840): Train loss 2.780, Val loss 3.244\n",
            "Ep 1 (Step 005845): Train loss 2.867, Val loss 3.250\n",
            "Ep 1 (Step 005850): Train loss 2.826, Val loss 3.257\n",
            "Ep 1 (Step 005855): Train loss 2.869, Val loss 3.251\n",
            "Ep 1 (Step 005860): Train loss 2.744, Val loss 3.250\n",
            "Ep 1 (Step 005865): Train loss 2.585, Val loss 3.244\n",
            "Ep 1 (Step 005870): Train loss 2.667, Val loss 3.248\n",
            "Ep 1 (Step 005875): Train loss 2.767, Val loss 3.261\n",
            "Ep 1 (Step 005880): Train loss 3.013, Val loss 3.263\n",
            "Ep 1 (Step 005885): Train loss 2.527, Val loss 3.262\n",
            "Ep 1 (Step 005890): Train loss 2.824, Val loss 3.253\n",
            "Ep 1 (Step 005895): Train loss 2.632, Val loss 3.256\n",
            "Ep 1 (Step 005900): Train loss 2.858, Val loss 3.263\n",
            "Ep 1 (Step 005905): Train loss 2.827, Val loss 3.252\n",
            "Ep 1 (Step 005910): Train loss 2.748, Val loss 3.253\n",
            "Ep 1 (Step 005915): Train loss 2.809, Val loss 3.250\n",
            "Ep 1 (Step 005920): Train loss 2.777, Val loss 3.253\n",
            "Ep 1 (Step 005925): Train loss 2.880, Val loss 3.253\n",
            "Ep 1 (Step 005930): Train loss 2.706, Val loss 3.255\n",
            "Ep 1 (Step 005935): Train loss 2.903, Val loss 3.253\n",
            "Ep 1 (Step 005940): Train loss 2.813, Val loss 3.249\n",
            "Ep 1 (Step 005945): Train loss 2.715, Val loss 3.254\n",
            "Ep 1 (Step 005950): Train loss 2.654, Val loss 3.253\n",
            "Ep 1 (Step 005955): Train loss 2.791, Val loss 3.250\n",
            "Ep 1 (Step 005960): Train loss 2.756, Val loss 3.247\n",
            "Ep 1 (Step 005965): Train loss 2.621, Val loss 3.252\n",
            "Ep 1 (Step 005970): Train loss 2.749, Val loss 3.254\n",
            "Ep 1 (Step 005975): Train loss 2.613, Val loss 3.253\n",
            "Ep 1 (Step 005980): Train loss 2.740, Val loss 3.249\n",
            "Ep 1 (Step 005985): Train loss 2.817, Val loss 3.254\n",
            "Ep 1 (Step 005990): Train loss 2.734, Val loss 3.248\n",
            "Ep 1 (Step 005995): Train loss 2.962, Val loss 3.243\n",
            "Ep 1 (Step 006000): Train loss 2.601, Val loss 3.243\n",
            "Ep 1 (Step 006005): Train loss 2.753, Val loss 3.252\n",
            "Ep 1 (Step 006010): Train loss 2.545, Val loss 3.257\n",
            "Ep 1 (Step 006015): Train loss 2.908, Val loss 3.246\n",
            "Ep 1 (Step 006020): Train loss 2.997, Val loss 3.237\n",
            "Ep 1 (Step 006025): Train loss 2.719, Val loss 3.236\n",
            "Ep 1 (Step 006030): Train loss 2.759, Val loss 3.246\n",
            "Ep 1 (Step 006035): Train loss 2.819, Val loss 3.255\n",
            "Ep 1 (Step 006040): Train loss 3.030, Val loss 3.255\n",
            "Ep 1 (Step 006045): Train loss 2.686, Val loss 3.246\n",
            "Ep 1 (Step 006050): Train loss 2.968, Val loss 3.239\n",
            "Ep 1 (Step 006055): Train loss 2.881, Val loss 3.242\n",
            "Ep 1 (Step 006060): Train loss 2.885, Val loss 3.254\n",
            "Ep 1 (Step 006065): Train loss 2.748, Val loss 3.250\n",
            "Ep 1 (Step 006070): Train loss 2.908, Val loss 3.242\n",
            "Ep 1 (Step 006075): Train loss 2.905, Val loss 3.239\n",
            "Ep 1 (Step 006080): Train loss 2.839, Val loss 3.241\n",
            "Ep 1 (Step 006085): Train loss 2.775, Val loss 3.247\n",
            "Ep 1 (Step 006090): Train loss 2.765, Val loss 3.247\n",
            "Ep 1 (Step 006095): Train loss 2.767, Val loss 3.244\n",
            "Ep 1 (Step 006100): Train loss 2.630, Val loss 3.235\n",
            "Ep 1 (Step 006105): Train loss 2.749, Val loss 3.237\n",
            "Ep 1 (Step 006110): Train loss 2.819, Val loss 3.241\n",
            "Ep 1 (Step 006115): Train loss 2.807, Val loss 3.240\n",
            "Ep 1 (Step 006120): Train loss 3.035, Val loss 3.237\n",
            "Ep 1 (Step 006125): Train loss 2.853, Val loss 3.241\n",
            "Ep 1 (Step 006130): Train loss 2.916, Val loss 3.246\n",
            "Ep 1 (Step 006135): Train loss 2.817, Val loss 3.250\n",
            "Ep 1 (Step 006140): Train loss 2.686, Val loss 3.244\n",
            "Ep 1 (Step 006145): Train loss 2.777, Val loss 3.246\n",
            "Ep 1 (Step 006150): Train loss 2.733, Val loss 3.242\n",
            "Ep 1 (Step 006155): Train loss 2.842, Val loss 3.237\n",
            "Ep 1 (Step 006160): Train loss 2.864, Val loss 3.236\n",
            "Ep 1 (Step 006165): Train loss 2.580, Val loss 3.236\n",
            "Ep 1 (Step 006170): Train loss 2.902, Val loss 3.235\n",
            "Ep 1 (Step 006175): Train loss 2.641, Val loss 3.235\n",
            "Ep 1 (Step 006180): Train loss 2.992, Val loss 3.248\n",
            "Ep 1 (Step 006185): Train loss 2.683, Val loss 3.253\n",
            "Ep 1 (Step 006190): Train loss 2.743, Val loss 3.239\n",
            "Ep 1 (Step 006195): Train loss 2.643, Val loss 3.233\n",
            "Ep 1 (Step 006200): Train loss 2.669, Val loss 3.232\n",
            "Ep 1 (Step 006205): Train loss 2.841, Val loss 3.245\n",
            "Ep 1 (Step 006210): Train loss 2.707, Val loss 3.243\n",
            "Ep 1 (Step 006215): Train loss 2.882, Val loss 3.244\n",
            "Ep 1 (Step 006220): Train loss 2.670, Val loss 3.243\n",
            "Ep 1 (Step 006225): Train loss 2.670, Val loss 3.246\n",
            "Ep 1 (Step 006230): Train loss 2.750, Val loss 3.253\n",
            "Ep 1 (Step 006235): Train loss 2.746, Val loss 3.252\n",
            "Ep 1 (Step 006240): Train loss 2.749, Val loss 3.257\n",
            "Ep 1 (Step 006245): Train loss 2.807, Val loss 3.254\n",
            "Ep 1 (Step 006250): Train loss 2.696, Val loss 3.245\n",
            "Ep 1 (Step 006255): Train loss 2.804, Val loss 3.249\n",
            "Ep 1 (Step 006260): Train loss 2.651, Val loss 3.248\n",
            "Ep 1 (Step 006265): Train loss 2.709, Val loss 3.248\n",
            "Ep 1 (Step 006270): Train loss 2.826, Val loss 3.251\n",
            "Ep 1 (Step 006275): Train loss 2.751, Val loss 3.239\n",
            "Ep 1 (Step 006280): Train loss 2.643, Val loss 3.237\n",
            "Ep 1 (Step 006285): Train loss 2.747, Val loss 3.241\n",
            "Ep 1 (Step 006290): Train loss 2.722, Val loss 3.245\n",
            "Ep 1 (Step 006295): Train loss 2.747, Val loss 3.236\n",
            "Ep 1 (Step 006300): Train loss 2.712, Val loss 3.233\n",
            "Ep 1 (Step 006305): Train loss 2.928, Val loss 3.233\n",
            "Ep 1 (Step 006310): Train loss 2.825, Val loss 3.237\n",
            "Ep 1 (Step 006315): Train loss 2.726, Val loss 3.234\n",
            "Ep 1 (Step 006320): Train loss 2.823, Val loss 3.236\n",
            "Ep 1 (Step 006325): Train loss 2.893, Val loss 3.236\n",
            "Ep 1 (Step 006330): Train loss 2.824, Val loss 3.247\n",
            "Ep 1 (Step 006335): Train loss 2.755, Val loss 3.242\n",
            "Ep 1 (Step 006340): Train loss 2.807, Val loss 3.247\n",
            "Ep 1 (Step 006345): Train loss 2.871, Val loss 3.246\n",
            "Ep 1 (Step 006350): Train loss 2.604, Val loss 3.238\n",
            "Ep 1 (Step 006355): Train loss 2.835, Val loss 3.228\n",
            "Ep 1 (Step 006360): Train loss 2.753, Val loss 3.226\n",
            "Ep 1 (Step 006365): Train loss 2.773, Val loss 3.226\n",
            "Ep 1 (Step 006370): Train loss 2.676, Val loss 3.230\n",
            "Ep 1 (Step 006375): Train loss 2.615, Val loss 3.237\n",
            "Ep 1 (Step 006380): Train loss 2.549, Val loss 3.230\n",
            "Ep 1 (Step 006385): Train loss 2.560, Val loss 3.232\n",
            "Ep 1 (Step 006390): Train loss 2.687, Val loss 3.237\n",
            "Ep 1 (Step 006395): Train loss 2.807, Val loss 3.238\n",
            "Ep 1 (Step 006400): Train loss 2.778, Val loss 3.239\n",
            "Ep 1 (Step 006405): Train loss 2.952, Val loss 3.236\n",
            "Ep 1 (Step 006410): Train loss 2.729, Val loss 3.238\n",
            "Ep 1 (Step 006415): Train loss 2.570, Val loss 3.237\n",
            "Ep 1 (Step 006420): Train loss 2.802, Val loss 3.237\n",
            "Ep 1 (Step 006425): Train loss 2.738, Val loss 3.232\n",
            "Ep 1 (Step 006430): Train loss 2.874, Val loss 3.231\n",
            "Ep 1 (Step 006435): Train loss 2.764, Val loss 3.234\n",
            "Ep 1 (Step 006440): Train loss 2.739, Val loss 3.250\n",
            "Ep 1 (Step 006445): Train loss 2.699, Val loss 3.248\n",
            "Ep 1 (Step 006450): Train loss 2.777, Val loss 3.241\n",
            "Ep 1 (Step 006455): Train loss 2.671, Val loss 3.236\n",
            "Ep 1 (Step 006460): Train loss 2.682, Val loss 3.228\n",
            "Ep 1 (Step 006465): Train loss 2.771, Val loss 3.236\n",
            "Ep 1 (Step 006470): Train loss 2.652, Val loss 3.239\n",
            "Ep 1 (Step 006475): Train loss 2.683, Val loss 3.238\n",
            "Ep 1 (Step 006480): Train loss 2.773, Val loss 3.234\n",
            "Ep 1 (Step 006485): Train loss 2.606, Val loss 3.236\n",
            "Ep 1 (Step 006490): Train loss 2.682, Val loss 3.234\n",
            "Ep 1 (Step 006495): Train loss 2.730, Val loss 3.232\n",
            "Ep 1 (Step 006500): Train loss 2.804, Val loss 3.229\n",
            "Ep 1 (Step 006505): Train loss 2.781, Val loss 3.226\n",
            "Ep 1 (Step 006510): Train loss 2.732, Val loss 3.230\n",
            "Ep 1 (Step 006515): Train loss 2.992, Val loss 3.230\n",
            "Ep 1 (Step 006520): Train loss 2.548, Val loss 3.230\n",
            "Ep 1 (Step 006525): Train loss 2.707, Val loss 3.231\n",
            "Ep 1 (Step 006530): Train loss 2.651, Val loss 3.228\n",
            "Ep 1 (Step 006535): Train loss 2.632, Val loss 3.224\n",
            "Ep 1 (Step 006540): Train loss 2.752, Val loss 3.223\n",
            "Ep 1 (Step 006545): Train loss 2.876, Val loss 3.226\n",
            "Ep 1 (Step 006550): Train loss 2.723, Val loss 3.230\n",
            "Ep 1 (Step 006555): Train loss 2.732, Val loss 3.225\n",
            "Ep 1 (Step 006560): Train loss 2.830, Val loss 3.229\n",
            "Ep 1 (Step 006565): Train loss 2.906, Val loss 3.229\n",
            "Ep 1 (Step 006570): Train loss 2.790, Val loss 3.232\n",
            "Ep 1 (Step 006575): Train loss 2.432, Val loss 3.229\n",
            "Ep 1 (Step 006580): Train loss 2.652, Val loss 3.227\n",
            "Ep 1 (Step 006585): Train loss 2.778, Val loss 3.223\n",
            "Ep 1 (Step 006590): Train loss 2.843, Val loss 3.218\n",
            "Ep 1 (Step 006595): Train loss 2.724, Val loss 3.218\n",
            "Ep 1 (Step 006600): Train loss 2.686, Val loss 3.216\n",
            "Ep 1 (Step 006605): Train loss 2.865, Val loss 3.214\n",
            "Ep 1 (Step 006610): Train loss 2.620, Val loss 3.229\n",
            "Ep 1 (Step 006615): Train loss 2.663, Val loss 3.235\n",
            "Ep 1 (Step 006620): Train loss 2.599, Val loss 3.245\n",
            "Ep 1 (Step 006625): Train loss 2.750, Val loss 3.243\n",
            "Ep 1 (Step 006630): Train loss 2.726, Val loss 3.235\n",
            "Ep 1 (Step 006635): Train loss 2.743, Val loss 3.230\n",
            "Ep 1 (Step 006640): Train loss 2.847, Val loss 3.233\n",
            "Ep 1 (Step 006645): Train loss 2.478, Val loss 3.227\n",
            "Ep 1 (Step 006650): Train loss 2.541, Val loss 3.227\n",
            "Ep 1 (Step 006655): Train loss 2.878, Val loss 3.226\n",
            "Ep 1 (Step 006660): Train loss 2.716, Val loss 3.233\n",
            "Ep 1 (Step 006665): Train loss 2.913, Val loss 3.235\n",
            "Ep 1 (Step 006670): Train loss 3.071, Val loss 3.222\n",
            "Ep 1 (Step 006675): Train loss 2.691, Val loss 3.221\n",
            "Ep 1 (Step 006680): Train loss 2.678, Val loss 3.236\n",
            "Ep 1 (Step 006685): Train loss 2.676, Val loss 3.238\n",
            "Ep 1 (Step 006690): Train loss 2.705, Val loss 3.233\n",
            "Ep 1 (Step 006695): Train loss 2.809, Val loss 3.238\n",
            "Ep 1 (Step 006700): Train loss 2.800, Val loss 3.233\n",
            "Ep 1 (Step 006705): Train loss 2.753, Val loss 3.227\n",
            "Ep 1 (Step 006710): Train loss 2.943, Val loss 3.239\n",
            "Ep 1 (Step 006715): Train loss 2.585, Val loss 3.240\n",
            "Ep 1 (Step 006720): Train loss 2.755, Val loss 3.244\n",
            "Ep 1 (Step 006725): Train loss 2.743, Val loss 3.237\n",
            "Ep 1 (Step 006730): Train loss 2.816, Val loss 3.233\n",
            "Ep 1 (Step 006735): Train loss 2.860, Val loss 3.237\n",
            "Ep 1 (Step 006740): Train loss 2.796, Val loss 3.254\n",
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस । # # # प्रतिक्रिया : थप रूपमा, इनपुटमा जनावरको सबैभन्दा प्रभावशाली विशेषताहरू मध्ये <|unk|>सूचीबद्ध गर्नुहोस् # # # इनपुट : जनावरः पेन्गुइन # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
            "Ep 2 (Step 006745): Train loss 2.811, Val loss 3.242\n",
            "Ep 2 (Step 006750): Train loss 2.698, Val loss 3.240\n",
            "Ep 2 (Step 006755): Train loss 2.764, Val loss 3.232\n",
            "Ep 2 (Step 006760): Train loss 2.640, Val loss 3.228\n",
            "Ep 2 (Step 006765): Train loss 2.908, Val loss 3.231\n",
            "Ep 2 (Step 006770): Train loss 2.629, Val loss 3.239\n",
            "Ep 2 (Step 006775): Train loss 2.579, Val loss 3.238\n",
            "Ep 2 (Step 006780): Train loss 2.462, Val loss 3.239\n",
            "Ep 2 (Step 006785): Train loss 2.759, Val loss 3.238\n",
            "Ep 2 (Step 006790): Train loss 2.579, Val loss 3.226\n",
            "Ep 2 (Step 006795): Train loss 2.435, Val loss 3.221\n",
            "Ep 2 (Step 006800): Train loss 2.681, Val loss 3.229\n",
            "Ep 2 (Step 006805): Train loss 2.674, Val loss 3.232\n",
            "Ep 2 (Step 006810): Train loss 2.711, Val loss 3.235\n",
            "Ep 2 (Step 006815): Train loss 2.798, Val loss 3.230\n",
            "Ep 2 (Step 006820): Train loss 2.699, Val loss 3.225\n",
            "Ep 2 (Step 006825): Train loss 2.937, Val loss 3.226\n",
            "Ep 2 (Step 006830): Train loss 2.677, Val loss 3.232\n",
            "Ep 2 (Step 006835): Train loss 2.839, Val loss 3.236\n",
            "Ep 2 (Step 006840): Train loss 2.697, Val loss 3.234\n",
            "Ep 2 (Step 006845): Train loss 2.683, Val loss 3.231\n",
            "Ep 2 (Step 006850): Train loss 2.736, Val loss 3.226\n",
            "Ep 2 (Step 006855): Train loss 2.647, Val loss 3.225\n",
            "Ep 2 (Step 006860): Train loss 2.745, Val loss 3.224\n",
            "Ep 2 (Step 006865): Train loss 2.690, Val loss 3.219\n",
            "Ep 2 (Step 006870): Train loss 2.885, Val loss 3.217\n",
            "Ep 2 (Step 006875): Train loss 2.681, Val loss 3.224\n",
            "Ep 2 (Step 006880): Train loss 2.651, Val loss 3.229\n",
            "Ep 2 (Step 006885): Train loss 2.767, Val loss 3.227\n",
            "Ep 2 (Step 006890): Train loss 2.776, Val loss 3.225\n",
            "Ep 2 (Step 006895): Train loss 2.735, Val loss 3.220\n",
            "Ep 2 (Step 006900): Train loss 2.746, Val loss 3.213\n",
            "Ep 2 (Step 006905): Train loss 2.767, Val loss 3.215\n",
            "Ep 2 (Step 006910): Train loss 2.607, Val loss 3.221\n",
            "Ep 2 (Step 006915): Train loss 2.834, Val loss 3.214\n",
            "Ep 2 (Step 006920): Train loss 2.724, Val loss 3.220\n",
            "Ep 2 (Step 006925): Train loss 2.886, Val loss 3.219\n",
            "Ep 2 (Step 006930): Train loss 2.677, Val loss 3.221\n",
            "Ep 2 (Step 006935): Train loss 2.826, Val loss 3.224\n",
            "Ep 2 (Step 006940): Train loss 2.707, Val loss 3.226\n",
            "Ep 2 (Step 006945): Train loss 2.612, Val loss 3.222\n",
            "Ep 2 (Step 006950): Train loss 2.681, Val loss 3.225\n",
            "Ep 2 (Step 006955): Train loss 2.665, Val loss 3.234\n",
            "Ep 2 (Step 006960): Train loss 2.753, Val loss 3.234\n",
            "Ep 2 (Step 006965): Train loss 2.538, Val loss 3.220\n",
            "Ep 2 (Step 006970): Train loss 2.706, Val loss 3.215\n",
            "Ep 2 (Step 006975): Train loss 2.798, Val loss 3.228\n",
            "Ep 2 (Step 006980): Train loss 2.731, Val loss 3.232\n",
            "Ep 2 (Step 006985): Train loss 2.630, Val loss 3.221\n",
            "Ep 2 (Step 006990): Train loss 2.617, Val loss 3.220\n",
            "Ep 2 (Step 006995): Train loss 2.656, Val loss 3.231\n",
            "Ep 2 (Step 007000): Train loss 2.697, Val loss 3.233\n",
            "Ep 2 (Step 007005): Train loss 2.704, Val loss 3.228\n",
            "Ep 2 (Step 007010): Train loss 2.822, Val loss 3.230\n",
            "Ep 2 (Step 007015): Train loss 2.652, Val loss 3.230\n",
            "Ep 2 (Step 007020): Train loss 2.797, Val loss 3.232\n",
            "Ep 2 (Step 007025): Train loss 2.584, Val loss 3.228\n",
            "Ep 2 (Step 007030): Train loss 2.692, Val loss 3.228\n",
            "Ep 2 (Step 007035): Train loss 2.805, Val loss 3.225\n",
            "Ep 2 (Step 007040): Train loss 2.709, Val loss 3.218\n",
            "Ep 2 (Step 007045): Train loss 2.861, Val loss 3.211\n",
            "Ep 2 (Step 007050): Train loss 2.776, Val loss 3.208\n",
            "Ep 2 (Step 007055): Train loss 2.825, Val loss 3.216\n",
            "Ep 2 (Step 007060): Train loss 2.518, Val loss 3.228\n",
            "Ep 2 (Step 007065): Train loss 2.819, Val loss 3.220\n",
            "Ep 2 (Step 007070): Train loss 2.787, Val loss 3.214\n",
            "Ep 2 (Step 007075): Train loss 2.703, Val loss 3.216\n",
            "Ep 2 (Step 007080): Train loss 2.924, Val loss 3.223\n",
            "Ep 2 (Step 007085): Train loss 2.745, Val loss 3.220\n",
            "Ep 2 (Step 007090): Train loss 2.833, Val loss 3.211\n",
            "Ep 2 (Step 007095): Train loss 2.628, Val loss 3.213\n",
            "Ep 2 (Step 007100): Train loss 2.596, Val loss 3.220\n",
            "Ep 2 (Step 007105): Train loss 2.585, Val loss 3.223\n",
            "Ep 2 (Step 007110): Train loss 2.712, Val loss 3.217\n",
            "Ep 2 (Step 007115): Train loss 2.546, Val loss 3.226\n",
            "Ep 2 (Step 007120): Train loss 2.546, Val loss 3.223\n",
            "Ep 2 (Step 007125): Train loss 2.923, Val loss 3.222\n",
            "Ep 2 (Step 007130): Train loss 2.491, Val loss 3.218\n",
            "Ep 2 (Step 007135): Train loss 2.636, Val loss 3.217\n",
            "Ep 2 (Step 007140): Train loss 2.679, Val loss 3.219\n",
            "Ep 2 (Step 007145): Train loss 2.739, Val loss 3.219\n",
            "Ep 2 (Step 007150): Train loss 2.613, Val loss 3.216\n",
            "Ep 2 (Step 007155): Train loss 2.465, Val loss 3.217\n",
            "Ep 2 (Step 007160): Train loss 2.620, Val loss 3.220\n",
            "Ep 2 (Step 007165): Train loss 2.699, Val loss 3.219\n",
            "Ep 2 (Step 007170): Train loss 2.781, Val loss 3.214\n",
            "Ep 2 (Step 007175): Train loss 2.802, Val loss 3.212\n",
            "Ep 2 (Step 007180): Train loss 3.003, Val loss 3.212\n",
            "Ep 2 (Step 007185): Train loss 2.602, Val loss 3.210\n",
            "Ep 2 (Step 007190): Train loss 2.656, Val loss 3.206\n",
            "Ep 2 (Step 007195): Train loss 2.838, Val loss 3.207\n",
            "Ep 2 (Step 007200): Train loss 2.494, Val loss 3.210\n",
            "Ep 2 (Step 007205): Train loss 2.609, Val loss 3.217\n",
            "Ep 2 (Step 007210): Train loss 2.609, Val loss 3.219\n",
            "Ep 2 (Step 007215): Train loss 2.709, Val loss 3.220\n",
            "Ep 2 (Step 007220): Train loss 2.707, Val loss 3.218\n",
            "Ep 2 (Step 007225): Train loss 2.706, Val loss 3.214\n",
            "Ep 2 (Step 007230): Train loss 2.676, Val loss 3.218\n",
            "Ep 2 (Step 007235): Train loss 2.579, Val loss 3.222\n",
            "Ep 2 (Step 007240): Train loss 2.777, Val loss 3.222\n",
            "Ep 2 (Step 007245): Train loss 2.506, Val loss 3.222\n",
            "Ep 2 (Step 007250): Train loss 2.723, Val loss 3.217\n",
            "Ep 2 (Step 007255): Train loss 2.556, Val loss 3.224\n",
            "Ep 2 (Step 007260): Train loss 2.802, Val loss 3.222\n",
            "Ep 2 (Step 007265): Train loss 2.919, Val loss 3.222\n",
            "Ep 2 (Step 007270): Train loss 2.595, Val loss 3.229\n",
            "Ep 2 (Step 007275): Train loss 2.642, Val loss 3.226\n",
            "Ep 2 (Step 007280): Train loss 2.492, Val loss 3.224\n",
            "Ep 2 (Step 007285): Train loss 2.678, Val loss 3.228\n",
            "Ep 2 (Step 007290): Train loss 2.676, Val loss 3.222\n",
            "Ep 2 (Step 007295): Train loss 2.658, Val loss 3.222\n",
            "Ep 2 (Step 007300): Train loss 2.609, Val loss 3.230\n",
            "Ep 2 (Step 007305): Train loss 2.873, Val loss 3.230\n",
            "Ep 2 (Step 007310): Train loss 2.671, Val loss 3.228\n",
            "Ep 2 (Step 007315): Train loss 2.717, Val loss 3.217\n",
            "Ep 2 (Step 007320): Train loss 2.873, Val loss 3.218\n",
            "Ep 2 (Step 007325): Train loss 2.721, Val loss 3.220\n",
            "Ep 2 (Step 007330): Train loss 2.589, Val loss 3.214\n",
            "Ep 2 (Step 007335): Train loss 2.719, Val loss 3.214\n",
            "Ep 2 (Step 007340): Train loss 2.561, Val loss 3.209\n",
            "Ep 2 (Step 007345): Train loss 2.665, Val loss 3.209\n",
            "Ep 2 (Step 007350): Train loss 2.615, Val loss 3.208\n",
            "Ep 2 (Step 007355): Train loss 2.666, Val loss 3.216\n",
            "Ep 2 (Step 007360): Train loss 2.531, Val loss 3.210\n",
            "Ep 2 (Step 007365): Train loss 2.812, Val loss 3.211\n",
            "Ep 2 (Step 007370): Train loss 2.625, Val loss 3.210\n",
            "Ep 2 (Step 007375): Train loss 2.771, Val loss 3.219\n",
            "Ep 2 (Step 007380): Train loss 2.525, Val loss 3.217\n",
            "Ep 2 (Step 007385): Train loss 2.709, Val loss 3.212\n",
            "Ep 2 (Step 007390): Train loss 2.754, Val loss 3.215\n",
            "Ep 2 (Step 007395): Train loss 2.612, Val loss 3.213\n",
            "Ep 2 (Step 007400): Train loss 2.779, Val loss 3.210\n",
            "Ep 2 (Step 007405): Train loss 2.869, Val loss 3.205\n",
            "Ep 2 (Step 007410): Train loss 2.518, Val loss 3.203\n",
            "Ep 2 (Step 007415): Train loss 2.698, Val loss 3.216\n",
            "Ep 2 (Step 007420): Train loss 2.722, Val loss 3.217\n",
            "Ep 2 (Step 007425): Train loss 2.596, Val loss 3.211\n",
            "Ep 2 (Step 007430): Train loss 2.708, Val loss 3.212\n",
            "Ep 2 (Step 007435): Train loss 2.671, Val loss 3.233\n",
            "Ep 2 (Step 007440): Train loss 2.735, Val loss 3.232\n",
            "Ep 2 (Step 007445): Train loss 2.709, Val loss 3.225\n",
            "Ep 2 (Step 007450): Train loss 2.824, Val loss 3.216\n",
            "Ep 2 (Step 007455): Train loss 2.654, Val loss 3.210\n",
            "Ep 2 (Step 007460): Train loss 2.598, Val loss 3.209\n",
            "Ep 2 (Step 007465): Train loss 2.704, Val loss 3.216\n",
            "Ep 2 (Step 007470): Train loss 2.859, Val loss 3.216\n",
            "Ep 2 (Step 007475): Train loss 2.604, Val loss 3.216\n",
            "Ep 2 (Step 007480): Train loss 2.553, Val loss 3.218\n",
            "Ep 2 (Step 007485): Train loss 2.759, Val loss 3.213\n",
            "Ep 2 (Step 007490): Train loss 2.487, Val loss 3.205\n",
            "Ep 2 (Step 007495): Train loss 2.818, Val loss 3.203\n",
            "Ep 2 (Step 007500): Train loss 2.538, Val loss 3.208\n",
            "Ep 2 (Step 007505): Train loss 2.875, Val loss 3.211\n",
            "Ep 2 (Step 007510): Train loss 2.492, Val loss 3.213\n",
            "Ep 2 (Step 007515): Train loss 2.734, Val loss 3.213\n",
            "Ep 2 (Step 007520): Train loss 2.761, Val loss 3.214\n",
            "Ep 2 (Step 007525): Train loss 2.646, Val loss 3.222\n",
            "Ep 2 (Step 007530): Train loss 2.693, Val loss 3.212\n",
            "Ep 2 (Step 007535): Train loss 2.675, Val loss 3.212\n",
            "Ep 2 (Step 007540): Train loss 2.605, Val loss 3.214\n",
            "Ep 2 (Step 007545): Train loss 2.781, Val loss 3.219\n",
            "Ep 2 (Step 007550): Train loss 2.813, Val loss 3.218\n",
            "Ep 2 (Step 007555): Train loss 2.565, Val loss 3.224\n",
            "Ep 2 (Step 007560): Train loss 2.645, Val loss 3.226\n",
            "Ep 2 (Step 007565): Train loss 2.801, Val loss 3.220\n",
            "Ep 2 (Step 007570): Train loss 2.616, Val loss 3.219\n",
            "Ep 2 (Step 007575): Train loss 2.694, Val loss 3.220\n",
            "Ep 2 (Step 007580): Train loss 2.659, Val loss 3.220\n",
            "Ep 2 (Step 007585): Train loss 2.591, Val loss 3.218\n",
            "Ep 2 (Step 007590): Train loss 2.704, Val loss 3.221\n",
            "Ep 2 (Step 007595): Train loss 2.668, Val loss 3.218\n",
            "Ep 2 (Step 007600): Train loss 2.985, Val loss 3.212\n",
            "Ep 2 (Step 007605): Train loss 2.758, Val loss 3.208\n",
            "Ep 2 (Step 007610): Train loss 2.488, Val loss 3.208\n",
            "Ep 2 (Step 007615): Train loss 2.556, Val loss 3.212\n",
            "Ep 2 (Step 007620): Train loss 2.705, Val loss 3.218\n",
            "Ep 2 (Step 007625): Train loss 2.701, Val loss 3.213\n",
            "Ep 2 (Step 007630): Train loss 2.838, Val loss 3.204\n",
            "Ep 2 (Step 007635): Train loss 2.847, Val loss 3.201\n",
            "Ep 2 (Step 007640): Train loss 2.610, Val loss 3.204\n",
            "Ep 2 (Step 007645): Train loss 2.727, Val loss 3.212\n",
            "Ep 2 (Step 007650): Train loss 2.635, Val loss 3.223\n",
            "Ep 2 (Step 007655): Train loss 2.747, Val loss 3.212\n",
            "Ep 2 (Step 007660): Train loss 2.632, Val loss 3.207\n",
            "Ep 2 (Step 007665): Train loss 2.844, Val loss 3.199\n",
            "Ep 2 (Step 007670): Train loss 2.468, Val loss 3.201\n",
            "Ep 2 (Step 007675): Train loss 2.781, Val loss 3.204\n",
            "Ep 2 (Step 007680): Train loss 2.698, Val loss 3.216\n",
            "Ep 2 (Step 007685): Train loss 2.571, Val loss 3.211\n",
            "Ep 2 (Step 007690): Train loss 2.799, Val loss 3.204\n",
            "Ep 2 (Step 007695): Train loss 2.617, Val loss 3.204\n",
            "Ep 2 (Step 007700): Train loss 2.769, Val loss 3.202\n",
            "Ep 2 (Step 007705): Train loss 2.766, Val loss 3.201\n",
            "Ep 2 (Step 007710): Train loss 2.485, Val loss 3.206\n",
            "Ep 2 (Step 007715): Train loss 2.860, Val loss 3.206\n",
            "Ep 2 (Step 007720): Train loss 2.742, Val loss 3.196\n",
            "Ep 2 (Step 007725): Train loss 2.536, Val loss 3.194\n",
            "Ep 2 (Step 007730): Train loss 2.725, Val loss 3.198\n",
            "Ep 2 (Step 007735): Train loss 2.820, Val loss 3.205\n",
            "Ep 2 (Step 007740): Train loss 2.845, Val loss 3.204\n",
            "Ep 2 (Step 007745): Train loss 2.665, Val loss 3.204\n",
            "Ep 2 (Step 007750): Train loss 2.786, Val loss 3.208\n",
            "Ep 2 (Step 007755): Train loss 2.734, Val loss 3.203\n",
            "Ep 2 (Step 007760): Train loss 2.799, Val loss 3.210\n",
            "Ep 2 (Step 007765): Train loss 2.796, Val loss 3.211\n",
            "Ep 2 (Step 007770): Train loss 2.596, Val loss 3.208\n",
            "Ep 2 (Step 007775): Train loss 2.703, Val loss 3.200\n",
            "Ep 2 (Step 007780): Train loss 2.630, Val loss 3.201\n",
            "Ep 2 (Step 007785): Train loss 2.667, Val loss 3.198\n",
            "Ep 2 (Step 007790): Train loss 2.663, Val loss 3.194\n",
            "Ep 2 (Step 007795): Train loss 2.421, Val loss 3.197\n",
            "Ep 2 (Step 007800): Train loss 2.804, Val loss 3.190\n",
            "Ep 2 (Step 007805): Train loss 2.848, Val loss 3.185\n",
            "Ep 2 (Step 007810): Train loss 2.852, Val loss 3.187\n",
            "Ep 2 (Step 007815): Train loss 2.735, Val loss 3.193\n",
            "Ep 2 (Step 007820): Train loss 2.823, Val loss 3.199\n",
            "Ep 2 (Step 007825): Train loss 2.734, Val loss 3.192\n",
            "Ep 2 (Step 007830): Train loss 2.833, Val loss 3.193\n",
            "Ep 2 (Step 007835): Train loss 2.640, Val loss 3.193\n",
            "Ep 2 (Step 007840): Train loss 2.730, Val loss 3.195\n",
            "Ep 2 (Step 007845): Train loss 2.615, Val loss 3.197\n",
            "Ep 2 (Step 007850): Train loss 2.803, Val loss 3.202\n",
            "Ep 2 (Step 007855): Train loss 2.659, Val loss 3.197\n",
            "Ep 2 (Step 007860): Train loss 2.791, Val loss 3.193\n",
            "Ep 2 (Step 007865): Train loss 2.551, Val loss 3.192\n",
            "Ep 2 (Step 007870): Train loss 2.624, Val loss 3.200\n",
            "Ep 2 (Step 007875): Train loss 2.801, Val loss 3.200\n",
            "Ep 2 (Step 007880): Train loss 2.571, Val loss 3.193\n",
            "Ep 2 (Step 007885): Train loss 2.756, Val loss 3.193\n",
            "Ep 2 (Step 007890): Train loss 2.655, Val loss 3.196\n",
            "Ep 2 (Step 007895): Train loss 2.696, Val loss 3.197\n",
            "Ep 2 (Step 007900): Train loss 2.728, Val loss 3.202\n",
            "Ep 2 (Step 007905): Train loss 2.867, Val loss 3.198\n",
            "Ep 2 (Step 007910): Train loss 2.629, Val loss 3.195\n",
            "Ep 2 (Step 007915): Train loss 2.698, Val loss 3.200\n",
            "Ep 2 (Step 007920): Train loss 2.445, Val loss 3.206\n",
            "Ep 2 (Step 007925): Train loss 2.704, Val loss 3.204\n",
            "Ep 2 (Step 007930): Train loss 2.515, Val loss 3.199\n",
            "Ep 2 (Step 007935): Train loss 2.704, Val loss 3.199\n",
            "Ep 2 (Step 007940): Train loss 2.619, Val loss 3.199\n",
            "Ep 2 (Step 007945): Train loss 2.607, Val loss 3.201\n",
            "Ep 2 (Step 007950): Train loss 2.515, Val loss 3.200\n",
            "Ep 2 (Step 007955): Train loss 2.759, Val loss 3.207\n",
            "Ep 2 (Step 007960): Train loss 2.448, Val loss 3.210\n",
            "Ep 2 (Step 007965): Train loss 2.684, Val loss 3.199\n",
            "Ep 2 (Step 007970): Train loss 2.776, Val loss 3.197\n",
            "Ep 2 (Step 007975): Train loss 2.596, Val loss 3.197\n",
            "Ep 2 (Step 007980): Train loss 2.510, Val loss 3.198\n",
            "Ep 2 (Step 007985): Train loss 2.556, Val loss 3.201\n",
            "Ep 2 (Step 007990): Train loss 2.765, Val loss 3.198\n",
            "Ep 2 (Step 007995): Train loss 2.797, Val loss 3.198\n",
            "Ep 2 (Step 008000): Train loss 2.644, Val loss 3.201\n",
            "Ep 2 (Step 008005): Train loss 2.757, Val loss 3.205\n",
            "Ep 2 (Step 008010): Train loss 2.699, Val loss 3.209\n",
            "Ep 2 (Step 008015): Train loss 2.507, Val loss 3.216\n",
            "Ep 2 (Step 008020): Train loss 2.701, Val loss 3.218\n",
            "Ep 2 (Step 008025): Train loss 2.677, Val loss 3.216\n",
            "Ep 2 (Step 008030): Train loss 2.715, Val loss 3.211\n",
            "Ep 2 (Step 008035): Train loss 2.700, Val loss 3.211\n",
            "Ep 2 (Step 008040): Train loss 2.838, Val loss 3.219\n",
            "Ep 2 (Step 008045): Train loss 2.669, Val loss 3.217\n",
            "Ep 2 (Step 008050): Train loss 2.656, Val loss 3.207\n",
            "Ep 2 (Step 008055): Train loss 2.708, Val loss 3.212\n",
            "Ep 2 (Step 008060): Train loss 2.636, Val loss 3.215\n",
            "Ep 2 (Step 008065): Train loss 2.560, Val loss 3.214\n",
            "Ep 2 (Step 008070): Train loss 2.438, Val loss 3.213\n",
            "Ep 2 (Step 008075): Train loss 2.680, Val loss 3.211\n",
            "Ep 2 (Step 008080): Train loss 2.737, Val loss 3.206\n",
            "Ep 2 (Step 008085): Train loss 2.700, Val loss 3.202\n",
            "Ep 2 (Step 008090): Train loss 2.602, Val loss 3.205\n",
            "Ep 2 (Step 008095): Train loss 2.634, Val loss 3.207\n",
            "Ep 2 (Step 008100): Train loss 2.818, Val loss 3.205\n",
            "Ep 2 (Step 008105): Train loss 2.637, Val loss 3.202\n",
            "Ep 2 (Step 008110): Train loss 2.643, Val loss 3.201\n",
            "Ep 2 (Step 008115): Train loss 2.574, Val loss 3.200\n",
            "Ep 2 (Step 008120): Train loss 2.655, Val loss 3.208\n",
            "Ep 2 (Step 008125): Train loss 2.699, Val loss 3.219\n",
            "Ep 2 (Step 008130): Train loss 2.662, Val loss 3.223\n",
            "Ep 2 (Step 008135): Train loss 2.489, Val loss 3.214\n",
            "Ep 2 (Step 008140): Train loss 2.652, Val loss 3.205\n",
            "Ep 2 (Step 008145): Train loss 2.694, Val loss 3.200\n",
            "Ep 2 (Step 008150): Train loss 2.802, Val loss 3.203\n",
            "Ep 2 (Step 008155): Train loss 2.560, Val loss 3.203\n",
            "Ep 2 (Step 008160): Train loss 2.634, Val loss 3.205\n",
            "Ep 2 (Step 008165): Train loss 2.692, Val loss 3.204\n",
            "Ep 2 (Step 008170): Train loss 2.689, Val loss 3.204\n",
            "Ep 2 (Step 008175): Train loss 2.765, Val loss 3.211\n",
            "Ep 2 (Step 008180): Train loss 2.715, Val loss 3.206\n",
            "Ep 2 (Step 008185): Train loss 2.659, Val loss 3.201\n",
            "Ep 2 (Step 008190): Train loss 2.526, Val loss 3.192\n",
            "Ep 2 (Step 008195): Train loss 2.570, Val loss 3.196\n",
            "Ep 2 (Step 008200): Train loss 2.800, Val loss 3.204\n",
            "Ep 2 (Step 008205): Train loss 2.553, Val loss 3.201\n",
            "Ep 2 (Step 008210): Train loss 2.535, Val loss 3.198\n",
            "Ep 2 (Step 008215): Train loss 2.564, Val loss 3.196\n",
            "Ep 2 (Step 008220): Train loss 2.584, Val loss 3.195\n",
            "Ep 2 (Step 008225): Train loss 2.777, Val loss 3.193\n",
            "Ep 2 (Step 008230): Train loss 2.740, Val loss 3.194\n",
            "Ep 2 (Step 008235): Train loss 2.646, Val loss 3.193\n",
            "Ep 2 (Step 008240): Train loss 2.497, Val loss 3.195\n",
            "Ep 2 (Step 008245): Train loss 2.536, Val loss 3.194\n",
            "Ep 2 (Step 008250): Train loss 2.500, Val loss 3.197\n",
            "Ep 2 (Step 008255): Train loss 2.504, Val loss 3.204\n",
            "Ep 2 (Step 008260): Train loss 2.703, Val loss 3.205\n",
            "Ep 2 (Step 008265): Train loss 2.579, Val loss 3.206\n",
            "Ep 2 (Step 008270): Train loss 2.480, Val loss 3.205\n",
            "Ep 2 (Step 008275): Train loss 2.715, Val loss 3.204\n",
            "Ep 2 (Step 008280): Train loss 2.654, Val loss 3.194\n",
            "Ep 2 (Step 008285): Train loss 2.694, Val loss 3.191\n",
            "Ep 2 (Step 008290): Train loss 2.674, Val loss 3.195\n",
            "Ep 2 (Step 008295): Train loss 2.482, Val loss 3.198\n",
            "Ep 2 (Step 008300): Train loss 2.629, Val loss 3.206\n",
            "Ep 2 (Step 008305): Train loss 2.733, Val loss 3.202\n",
            "Ep 2 (Step 008310): Train loss 2.509, Val loss 3.190\n",
            "Ep 2 (Step 008315): Train loss 2.652, Val loss 3.188\n",
            "Ep 2 (Step 008320): Train loss 2.593, Val loss 3.198\n",
            "Ep 2 (Step 008325): Train loss 2.767, Val loss 3.203\n",
            "Ep 2 (Step 008330): Train loss 2.891, Val loss 3.187\n",
            "Ep 2 (Step 008335): Train loss 2.774, Val loss 3.182\n",
            "Ep 2 (Step 008340): Train loss 2.511, Val loss 3.186\n",
            "Ep 2 (Step 008345): Train loss 2.684, Val loss 3.188\n",
            "Ep 2 (Step 008350): Train loss 2.735, Val loss 3.190\n",
            "Ep 2 (Step 008355): Train loss 2.572, Val loss 3.201\n",
            "Ep 2 (Step 008360): Train loss 2.698, Val loss 3.198\n",
            "Ep 2 (Step 008365): Train loss 2.685, Val loss 3.187\n",
            "Ep 2 (Step 008370): Train loss 2.625, Val loss 3.181\n",
            "Ep 2 (Step 008375): Train loss 2.394, Val loss 3.188\n",
            "Ep 2 (Step 008380): Train loss 2.577, Val loss 3.191\n",
            "Ep 2 (Step 008385): Train loss 2.573, Val loss 3.190\n",
            "Ep 2 (Step 008390): Train loss 2.637, Val loss 3.188\n",
            "Ep 2 (Step 008395): Train loss 2.447, Val loss 3.186\n",
            "Ep 2 (Step 008400): Train loss 2.609, Val loss 3.184\n",
            "Ep 2 (Step 008405): Train loss 2.712, Val loss 3.188\n",
            "Ep 2 (Step 008410): Train loss 2.503, Val loss 3.187\n",
            "Ep 2 (Step 008415): Train loss 2.447, Val loss 3.186\n",
            "Ep 2 (Step 008420): Train loss 2.470, Val loss 3.187\n",
            "Ep 2 (Step 008425): Train loss 2.687, Val loss 3.192\n",
            "Ep 2 (Step 008430): Train loss 2.702, Val loss 3.202\n",
            "Ep 2 (Step 008435): Train loss 2.599, Val loss 3.200\n",
            "Ep 2 (Step 008440): Train loss 2.637, Val loss 3.205\n",
            "Ep 2 (Step 008445): Train loss 2.685, Val loss 3.210\n",
            "Ep 2 (Step 008450): Train loss 2.627, Val loss 3.212\n",
            "Ep 2 (Step 008455): Train loss 2.490, Val loss 3.207\n",
            "Ep 2 (Step 008460): Train loss 2.582, Val loss 3.205\n",
            "Ep 2 (Step 008465): Train loss 2.621, Val loss 3.207\n",
            "Ep 2 (Step 008470): Train loss 2.422, Val loss 3.202\n",
            "Ep 2 (Step 008475): Train loss 2.477, Val loss 3.206\n",
            "Ep 2 (Step 008480): Train loss 2.564, Val loss 3.196\n",
            "Ep 2 (Step 008485): Train loss 2.567, Val loss 3.207\n",
            "Ep 2 (Step 008490): Train loss 2.328, Val loss 3.208\n",
            "Ep 2 (Step 008495): Train loss 2.689, Val loss 3.202\n",
            "Ep 2 (Step 008500): Train loss 2.718, Val loss 3.200\n",
            "Ep 2 (Step 008505): Train loss 2.428, Val loss 3.197\n",
            "Ep 2 (Step 008510): Train loss 2.593, Val loss 3.196\n",
            "Ep 2 (Step 008515): Train loss 2.745, Val loss 3.197\n",
            "Ep 2 (Step 008520): Train loss 2.676, Val loss 3.201\n",
            "Ep 2 (Step 008525): Train loss 2.797, Val loss 3.198\n",
            "Ep 2 (Step 008530): Train loss 2.721, Val loss 3.198\n",
            "Ep 2 (Step 008535): Train loss 2.762, Val loss 3.195\n",
            "Ep 2 (Step 008540): Train loss 2.971, Val loss 3.198\n",
            "Ep 2 (Step 008545): Train loss 2.834, Val loss 3.196\n",
            "Ep 2 (Step 008550): Train loss 2.528, Val loss 3.208\n",
            "Ep 2 (Step 008555): Train loss 2.672, Val loss 3.218\n",
            "Ep 2 (Step 008560): Train loss 2.651, Val loss 3.213\n",
            "Ep 2 (Step 008565): Train loss 2.753, Val loss 3.205\n",
            "Ep 2 (Step 008570): Train loss 2.655, Val loss 3.208\n",
            "Ep 2 (Step 008575): Train loss 2.561, Val loss 3.220\n",
            "Ep 2 (Step 008580): Train loss 2.655, Val loss 3.211\n",
            "Ep 2 (Step 008585): Train loss 2.575, Val loss 3.197\n",
            "Ep 2 (Step 008590): Train loss 2.628, Val loss 3.201\n",
            "Ep 2 (Step 008595): Train loss 2.457, Val loss 3.205\n",
            "Ep 2 (Step 008600): Train loss 2.679, Val loss 3.204\n",
            "Ep 2 (Step 008605): Train loss 2.636, Val loss 3.203\n",
            "Ep 2 (Step 008610): Train loss 2.850, Val loss 3.204\n",
            "Ep 2 (Step 008615): Train loss 2.494, Val loss 3.201\n",
            "Ep 2 (Step 008620): Train loss 2.832, Val loss 3.199\n",
            "Ep 2 (Step 008625): Train loss 2.691, Val loss 3.194\n",
            "Ep 2 (Step 008630): Train loss 2.560, Val loss 3.198\n",
            "Ep 2 (Step 008635): Train loss 2.640, Val loss 3.199\n",
            "Ep 2 (Step 008640): Train loss 2.630, Val loss 3.208\n",
            "Ep 2 (Step 008645): Train loss 2.547, Val loss 3.212\n",
            "Ep 2 (Step 008650): Train loss 2.623, Val loss 3.203\n",
            "Ep 2 (Step 008655): Train loss 2.436, Val loss 3.196\n",
            "Ep 2 (Step 008660): Train loss 2.566, Val loss 3.192\n",
            "Ep 2 (Step 008665): Train loss 2.612, Val loss 3.193\n",
            "Ep 2 (Step 008670): Train loss 2.514, Val loss 3.205\n",
            "Ep 2 (Step 008675): Train loss 2.827, Val loss 3.214\n",
            "Ep 2 (Step 008680): Train loss 2.526, Val loss 3.205\n",
            "Ep 2 (Step 008685): Train loss 2.782, Val loss 3.199\n",
            "Ep 2 (Step 008690): Train loss 2.761, Val loss 3.200\n",
            "Ep 2 (Step 008695): Train loss 2.709, Val loss 3.200\n",
            "Ep 2 (Step 008700): Train loss 2.519, Val loss 3.197\n",
            "Ep 2 (Step 008705): Train loss 2.437, Val loss 3.196\n",
            "Ep 2 (Step 008710): Train loss 2.705, Val loss 3.196\n",
            "Ep 2 (Step 008715): Train loss 2.566, Val loss 3.194\n",
            "Ep 2 (Step 008720): Train loss 2.824, Val loss 3.194\n",
            "Ep 2 (Step 008725): Train loss 2.735, Val loss 3.196\n",
            "Ep 2 (Step 008730): Train loss 2.364, Val loss 3.196\n",
            "Ep 2 (Step 008735): Train loss 2.564, Val loss 3.200\n",
            "Ep 2 (Step 008740): Train loss 2.620, Val loss 3.193\n",
            "Ep 2 (Step 008745): Train loss 2.603, Val loss 3.200\n",
            "Ep 2 (Step 008750): Train loss 2.367, Val loss 3.206\n",
            "Ep 2 (Step 008755): Train loss 2.658, Val loss 3.201\n",
            "Ep 2 (Step 008760): Train loss 2.527, Val loss 3.208\n",
            "Ep 2 (Step 008765): Train loss 2.768, Val loss 3.197\n",
            "Ep 2 (Step 008770): Train loss 2.466, Val loss 3.196\n",
            "Ep 2 (Step 008775): Train loss 2.596, Val loss 3.198\n",
            "Ep 2 (Step 008780): Train loss 2.475, Val loss 3.204\n",
            "Ep 2 (Step 008785): Train loss 2.788, Val loss 3.194\n",
            "Ep 2 (Step 008790): Train loss 2.606, Val loss 3.201\n",
            "Ep 2 (Step 008795): Train loss 2.797, Val loss 3.200\n",
            "Ep 2 (Step 008800): Train loss 2.793, Val loss 3.200\n",
            "Ep 2 (Step 008805): Train loss 2.655, Val loss 3.211\n",
            "Ep 2 (Step 008810): Train loss 2.645, Val loss 3.211\n",
            "Ep 2 (Step 008815): Train loss 2.458, Val loss 3.214\n",
            "Ep 2 (Step 008820): Train loss 2.667, Val loss 3.200\n",
            "Ep 2 (Step 008825): Train loss 2.640, Val loss 3.190\n",
            "Ep 2 (Step 008830): Train loss 2.734, Val loss 3.191\n",
            "Ep 2 (Step 008835): Train loss 2.499, Val loss 3.203\n",
            "Ep 2 (Step 008840): Train loss 2.711, Val loss 3.198\n",
            "Ep 2 (Step 008845): Train loss 2.729, Val loss 3.201\n",
            "Ep 2 (Step 008850): Train loss 2.623, Val loss 3.200\n",
            "Ep 2 (Step 008855): Train loss 2.688, Val loss 3.198\n",
            "Ep 2 (Step 008860): Train loss 2.619, Val loss 3.203\n",
            "Ep 2 (Step 008865): Train loss 2.610, Val loss 3.213\n",
            "Ep 2 (Step 008870): Train loss 2.702, Val loss 3.209\n",
            "Ep 2 (Step 008875): Train loss 2.684, Val loss 3.208\n",
            "Ep 2 (Step 008880): Train loss 2.682, Val loss 3.200\n",
            "Ep 2 (Step 008885): Train loss 2.620, Val loss 3.192\n",
            "Ep 2 (Step 008890): Train loss 2.681, Val loss 3.196\n",
            "Ep 2 (Step 008895): Train loss 2.644, Val loss 3.196\n",
            "Ep 2 (Step 008900): Train loss 2.546, Val loss 3.199\n",
            "Ep 2 (Step 008905): Train loss 2.642, Val loss 3.207\n",
            "Ep 2 (Step 008910): Train loss 2.688, Val loss 3.202\n",
            "Ep 2 (Step 008915): Train loss 2.743, Val loss 3.199\n",
            "Ep 2 (Step 008920): Train loss 2.458, Val loss 3.195\n",
            "Ep 2 (Step 008925): Train loss 2.541, Val loss 3.192\n",
            "Ep 2 (Step 008930): Train loss 2.684, Val loss 3.190\n",
            "Ep 2 (Step 008935): Train loss 2.592, Val loss 3.194\n",
            "Ep 2 (Step 008940): Train loss 2.662, Val loss 3.198\n",
            "Ep 2 (Step 008945): Train loss 2.611, Val loss 3.205\n",
            "Ep 2 (Step 008950): Train loss 2.566, Val loss 3.211\n",
            "Ep 2 (Step 008955): Train loss 2.649, Val loss 3.205\n",
            "Ep 2 (Step 008960): Train loss 2.744, Val loss 3.210\n",
            "Ep 2 (Step 008965): Train loss 2.775, Val loss 3.209\n",
            "Ep 2 (Step 008970): Train loss 2.577, Val loss 3.200\n",
            "Ep 2 (Step 008975): Train loss 2.448, Val loss 3.192\n",
            "Ep 2 (Step 008980): Train loss 2.543, Val loss 3.191\n",
            "Ep 2 (Step 008985): Train loss 2.640, Val loss 3.190\n",
            "Ep 2 (Step 008990): Train loss 2.223, Val loss 3.191\n",
            "Ep 2 (Step 008995): Train loss 2.522, Val loss 3.190\n",
            "Ep 2 (Step 009000): Train loss 2.534, Val loss 3.190\n",
            "Ep 2 (Step 009005): Train loss 2.664, Val loss 3.194\n",
            "Ep 2 (Step 009010): Train loss 2.723, Val loss 3.197\n",
            "Ep 2 (Step 009015): Train loss 2.709, Val loss 3.196\n",
            "Ep 2 (Step 009020): Train loss 2.593, Val loss 3.191\n",
            "Ep 2 (Step 009025): Train loss 2.682, Val loss 3.192\n",
            "Ep 2 (Step 009030): Train loss 2.877, Val loss 3.194\n",
            "Ep 2 (Step 009035): Train loss 2.574, Val loss 3.205\n",
            "Ep 2 (Step 009040): Train loss 2.565, Val loss 3.202\n",
            "Ep 2 (Step 009045): Train loss 2.494, Val loss 3.195\n",
            "Ep 2 (Step 009050): Train loss 2.639, Val loss 3.193\n",
            "Ep 2 (Step 009055): Train loss 2.638, Val loss 3.197\n",
            "Ep 2 (Step 009060): Train loss 2.651, Val loss 3.196\n",
            "Ep 2 (Step 009065): Train loss 2.570, Val loss 3.192\n",
            "Ep 2 (Step 009070): Train loss 2.639, Val loss 3.184\n",
            "Ep 2 (Step 009075): Train loss 2.844, Val loss 3.185\n",
            "Ep 2 (Step 009080): Train loss 2.537, Val loss 3.182\n",
            "Ep 2 (Step 009085): Train loss 2.572, Val loss 3.182\n",
            "Ep 2 (Step 009090): Train loss 2.555, Val loss 3.192\n",
            "Ep 2 (Step 009095): Train loss 2.728, Val loss 3.188\n",
            "Ep 2 (Step 009100): Train loss 2.481, Val loss 3.183\n",
            "Ep 2 (Step 009105): Train loss 2.774, Val loss 3.177\n",
            "Ep 2 (Step 009110): Train loss 2.544, Val loss 3.184\n",
            "Ep 2 (Step 009115): Train loss 2.461, Val loss 3.195\n",
            "Ep 2 (Step 009120): Train loss 2.442, Val loss 3.190\n",
            "Ep 2 (Step 009125): Train loss 2.753, Val loss 3.186\n",
            "Ep 2 (Step 009130): Train loss 2.582, Val loss 3.180\n",
            "Ep 2 (Step 009135): Train loss 2.701, Val loss 3.181\n",
            "Ep 2 (Step 009140): Train loss 2.565, Val loss 3.185\n",
            "Ep 2 (Step 009145): Train loss 2.517, Val loss 3.181\n",
            "Ep 2 (Step 009150): Train loss 2.653, Val loss 3.187\n",
            "Ep 2 (Step 009155): Train loss 2.776, Val loss 3.191\n",
            "Ep 2 (Step 009160): Train loss 2.747, Val loss 3.191\n",
            "Ep 2 (Step 009165): Train loss 2.611, Val loss 3.185\n",
            "Ep 2 (Step 009170): Train loss 2.636, Val loss 3.187\n",
            "Ep 2 (Step 009175): Train loss 2.868, Val loss 3.185\n",
            "Ep 2 (Step 009180): Train loss 2.813, Val loss 3.182\n",
            "Ep 2 (Step 009185): Train loss 2.805, Val loss 3.179\n",
            "Ep 2 (Step 009190): Train loss 2.793, Val loss 3.182\n",
            "Ep 2 (Step 009195): Train loss 2.534, Val loss 3.187\n",
            "Ep 2 (Step 009200): Train loss 2.552, Val loss 3.184\n",
            "Ep 2 (Step 009205): Train loss 2.847, Val loss 3.186\n",
            "Ep 2 (Step 009210): Train loss 2.849, Val loss 3.188\n",
            "Ep 2 (Step 009215): Train loss 2.500, Val loss 3.190\n",
            "Ep 2 (Step 009220): Train loss 2.684, Val loss 3.188\n",
            "Ep 2 (Step 009225): Train loss 2.708, Val loss 3.181\n",
            "Ep 2 (Step 009230): Train loss 2.639, Val loss 3.185\n",
            "Ep 2 (Step 009235): Train loss 2.638, Val loss 3.186\n",
            "Ep 2 (Step 009240): Train loss 2.523, Val loss 3.186\n",
            "Ep 2 (Step 009245): Train loss 2.709, Val loss 3.185\n",
            "Ep 2 (Step 009250): Train loss 2.423, Val loss 3.191\n",
            "Ep 2 (Step 009255): Train loss 2.511, Val loss 3.194\n",
            "Ep 2 (Step 009260): Train loss 2.513, Val loss 3.189\n",
            "Ep 2 (Step 009265): Train loss 2.641, Val loss 3.184\n",
            "Ep 2 (Step 009270): Train loss 2.620, Val loss 3.176\n",
            "Ep 2 (Step 009275): Train loss 2.486, Val loss 3.172\n",
            "Ep 2 (Step 009280): Train loss 2.558, Val loss 3.167\n",
            "Ep 2 (Step 009285): Train loss 2.736, Val loss 3.174\n",
            "Ep 2 (Step 009290): Train loss 2.516, Val loss 3.187\n",
            "Ep 2 (Step 009295): Train loss 2.589, Val loss 3.186\n",
            "Ep 2 (Step 009300): Train loss 2.847, Val loss 3.179\n",
            "Ep 2 (Step 009305): Train loss 2.644, Val loss 3.186\n",
            "Ep 2 (Step 009310): Train loss 2.554, Val loss 3.191\n",
            "Ep 2 (Step 009315): Train loss 2.566, Val loss 3.190\n",
            "Ep 2 (Step 009320): Train loss 2.663, Val loss 3.189\n",
            "Ep 2 (Step 009325): Train loss 2.569, Val loss 3.187\n",
            "Ep 2 (Step 009330): Train loss 2.713, Val loss 3.188\n",
            "Ep 2 (Step 009335): Train loss 2.580, Val loss 3.191\n",
            "Ep 2 (Step 009340): Train loss 2.626, Val loss 3.184\n",
            "Ep 2 (Step 009345): Train loss 2.692, Val loss 3.183\n",
            "Ep 2 (Step 009350): Train loss 2.606, Val loss 3.185\n",
            "Ep 2 (Step 009355): Train loss 2.425, Val loss 3.190\n",
            "Ep 2 (Step 009360): Train loss 2.615, Val loss 3.194\n",
            "Ep 2 (Step 009365): Train loss 2.512, Val loss 3.191\n",
            "Ep 2 (Step 009370): Train loss 2.757, Val loss 3.185\n",
            "Ep 2 (Step 009375): Train loss 2.691, Val loss 3.188\n",
            "Ep 2 (Step 009380): Train loss 2.415, Val loss 3.190\n",
            "Ep 2 (Step 009385): Train loss 2.618, Val loss 3.188\n",
            "Ep 2 (Step 009390): Train loss 2.717, Val loss 3.189\n",
            "Ep 2 (Step 009395): Train loss 2.820, Val loss 3.190\n",
            "Ep 2 (Step 009400): Train loss 2.490, Val loss 3.182\n",
            "Ep 2 (Step 009405): Train loss 2.533, Val loss 3.182\n",
            "Ep 2 (Step 009410): Train loss 2.447, Val loss 3.182\n",
            "Ep 2 (Step 009415): Train loss 2.679, Val loss 3.185\n",
            "Ep 2 (Step 009420): Train loss 2.541, Val loss 3.188\n",
            "Ep 2 (Step 009425): Train loss 2.529, Val loss 3.184\n",
            "Ep 2 (Step 009430): Train loss 2.560, Val loss 3.188\n",
            "Ep 2 (Step 009435): Train loss 2.664, Val loss 3.191\n",
            "Ep 2 (Step 009440): Train loss 2.540, Val loss 3.188\n",
            "Ep 2 (Step 009445): Train loss 2.303, Val loss 3.182\n",
            "Ep 2 (Step 009450): Train loss 2.472, Val loss 3.179\n",
            "Ep 2 (Step 009455): Train loss 2.629, Val loss 3.179\n",
            "Ep 2 (Step 009460): Train loss 2.497, Val loss 3.185\n",
            "Ep 2 (Step 009465): Train loss 2.629, Val loss 3.179\n",
            "Ep 2 (Step 009470): Train loss 2.609, Val loss 3.171\n",
            "Ep 2 (Step 009475): Train loss 2.517, Val loss 3.175\n",
            "Ep 2 (Step 009480): Train loss 2.851, Val loss 3.187\n",
            "Ep 2 (Step 009485): Train loss 2.747, Val loss 3.182\n",
            "Ep 2 (Step 009490): Train loss 2.732, Val loss 3.176\n",
            "Ep 2 (Step 009495): Train loss 2.546, Val loss 3.175\n",
            "Ep 2 (Step 009500): Train loss 2.551, Val loss 3.178\n",
            "Ep 2 (Step 009505): Train loss 2.870, Val loss 3.179\n",
            "Ep 2 (Step 009510): Train loss 2.521, Val loss 3.172\n",
            "Ep 2 (Step 009515): Train loss 2.642, Val loss 3.169\n",
            "Ep 2 (Step 009520): Train loss 2.697, Val loss 3.170\n",
            "Ep 2 (Step 009525): Train loss 2.791, Val loss 3.176\n",
            "Ep 2 (Step 009530): Train loss 2.617, Val loss 3.180\n",
            "Ep 2 (Step 009535): Train loss 2.753, Val loss 3.177\n",
            "Ep 2 (Step 009540): Train loss 2.761, Val loss 3.172\n",
            "Ep 2 (Step 009545): Train loss 2.729, Val loss 3.173\n",
            "Ep 2 (Step 009550): Train loss 2.586, Val loss 3.175\n",
            "Ep 2 (Step 009555): Train loss 2.754, Val loss 3.176\n",
            "Ep 2 (Step 009560): Train loss 2.586, Val loss 3.182\n",
            "Ep 2 (Step 009565): Train loss 2.597, Val loss 3.179\n",
            "Ep 2 (Step 009570): Train loss 2.533, Val loss 3.180\n",
            "Ep 2 (Step 009575): Train loss 2.628, Val loss 3.178\n",
            "Ep 2 (Step 009580): Train loss 2.598, Val loss 3.172\n",
            "Ep 2 (Step 009585): Train loss 2.653, Val loss 3.170\n",
            "Ep 2 (Step 009590): Train loss 2.550, Val loss 3.187\n",
            "Ep 2 (Step 009595): Train loss 2.711, Val loss 3.188\n",
            "Ep 2 (Step 009600): Train loss 2.547, Val loss 3.173\n",
            "Ep 2 (Step 009605): Train loss 2.609, Val loss 3.171\n",
            "Ep 2 (Step 009610): Train loss 2.701, Val loss 3.175\n",
            "Ep 2 (Step 009615): Train loss 2.619, Val loss 3.179\n",
            "Ep 2 (Step 009620): Train loss 2.475, Val loss 3.173\n",
            "Ep 2 (Step 009625): Train loss 2.657, Val loss 3.183\n",
            "Ep 2 (Step 009630): Train loss 2.585, Val loss 3.187\n",
            "Ep 2 (Step 009635): Train loss 2.542, Val loss 3.181\n",
            "Ep 2 (Step 009640): Train loss 2.555, Val loss 3.180\n",
            "Ep 2 (Step 009645): Train loss 2.657, Val loss 3.183\n",
            "Ep 2 (Step 009650): Train loss 2.655, Val loss 3.190\n",
            "Ep 2 (Step 009655): Train loss 2.759, Val loss 3.187\n",
            "Ep 2 (Step 009660): Train loss 2.678, Val loss 3.180\n",
            "Ep 2 (Step 009665): Train loss 2.567, Val loss 3.179\n",
            "Ep 2 (Step 009670): Train loss 2.529, Val loss 3.177\n",
            "Ep 2 (Step 009675): Train loss 2.559, Val loss 3.181\n",
            "Ep 2 (Step 009680): Train loss 2.551, Val loss 3.178\n",
            "Ep 2 (Step 009685): Train loss 2.608, Val loss 3.171\n",
            "Ep 2 (Step 009690): Train loss 2.424, Val loss 3.173\n",
            "Ep 2 (Step 009695): Train loss 2.884, Val loss 3.178\n",
            "Ep 2 (Step 009700): Train loss 2.416, Val loss 3.175\n",
            "Ep 2 (Step 009705): Train loss 2.531, Val loss 3.177\n",
            "Ep 2 (Step 009710): Train loss 2.576, Val loss 3.171\n",
            "Ep 2 (Step 009715): Train loss 2.428, Val loss 3.175\n",
            "Ep 2 (Step 009720): Train loss 2.642, Val loss 3.172\n",
            "Ep 2 (Step 009725): Train loss 2.650, Val loss 3.184\n",
            "Ep 2 (Step 009730): Train loss 2.417, Val loss 3.192\n",
            "Ep 2 (Step 009735): Train loss 2.508, Val loss 3.179\n",
            "Ep 2 (Step 009740): Train loss 2.575, Val loss 3.169\n",
            "Ep 2 (Step 009745): Train loss 2.712, Val loss 3.173\n",
            "Ep 2 (Step 009750): Train loss 2.876, Val loss 3.177\n",
            "Ep 2 (Step 009755): Train loss 2.621, Val loss 3.177\n",
            "Ep 2 (Step 009760): Train loss 2.518, Val loss 3.169\n",
            "Ep 2 (Step 009765): Train loss 2.536, Val loss 3.167\n",
            "Ep 2 (Step 009770): Train loss 2.596, Val loss 3.170\n",
            "Ep 2 (Step 009775): Train loss 2.700, Val loss 3.167\n",
            "Ep 2 (Step 009780): Train loss 2.547, Val loss 3.162\n",
            "Ep 2 (Step 009785): Train loss 2.634, Val loss 3.160\n",
            "Ep 2 (Step 009790): Train loss 2.528, Val loss 3.168\n",
            "Ep 2 (Step 009795): Train loss 2.519, Val loss 3.181\n",
            "Ep 2 (Step 009800): Train loss 2.380, Val loss 3.176\n",
            "Ep 2 (Step 009805): Train loss 2.543, Val loss 3.182\n",
            "Ep 2 (Step 009810): Train loss 2.486, Val loss 3.175\n",
            "Ep 2 (Step 009815): Train loss 2.641, Val loss 3.171\n",
            "Ep 2 (Step 009820): Train loss 2.665, Val loss 3.164\n",
            "Ep 2 (Step 009825): Train loss 2.706, Val loss 3.165\n",
            "Ep 2 (Step 009830): Train loss 2.477, Val loss 3.175\n",
            "Ep 2 (Step 009835): Train loss 2.609, Val loss 3.176\n",
            "Ep 2 (Step 009840): Train loss 2.679, Val loss 3.169\n",
            "Ep 2 (Step 009845): Train loss 2.515, Val loss 3.180\n",
            "Ep 2 (Step 009850): Train loss 2.577, Val loss 3.170\n",
            "Ep 2 (Step 009855): Train loss 2.534, Val loss 3.162\n",
            "Ep 2 (Step 009860): Train loss 2.463, Val loss 3.167\n",
            "Ep 2 (Step 009865): Train loss 2.701, Val loss 3.178\n",
            "Ep 2 (Step 009870): Train loss 2.642, Val loss 3.169\n",
            "Ep 2 (Step 009875): Train loss 2.586, Val loss 3.180\n",
            "Ep 2 (Step 009880): Train loss 2.658, Val loss 3.182\n",
            "Ep 2 (Step 009885): Train loss 2.568, Val loss 3.174\n",
            "Ep 2 (Step 009890): Train loss 2.458, Val loss 3.172\n",
            "Ep 2 (Step 009895): Train loss 2.640, Val loss 3.173\n",
            "Ep 2 (Step 009900): Train loss 2.516, Val loss 3.181\n",
            "Ep 2 (Step 009905): Train loss 2.553, Val loss 3.178\n",
            "Ep 2 (Step 009910): Train loss 2.544, Val loss 3.174\n",
            "Ep 2 (Step 009915): Train loss 2.709, Val loss 3.171\n",
            "Ep 2 (Step 009920): Train loss 2.703, Val loss 3.171\n",
            "Ep 2 (Step 009925): Train loss 2.564, Val loss 3.177\n",
            "Ep 2 (Step 009930): Train loss 2.627, Val loss 3.181\n",
            "Ep 2 (Step 009935): Train loss 2.776, Val loss 3.178\n",
            "Ep 2 (Step 009940): Train loss 2.672, Val loss 3.185\n",
            "Ep 2 (Step 009945): Train loss 2.592, Val loss 3.190\n",
            "Ep 2 (Step 009950): Train loss 2.686, Val loss 3.185\n",
            "Ep 2 (Step 009955): Train loss 2.714, Val loss 3.177\n",
            "Ep 2 (Step 009960): Train loss 2.608, Val loss 3.181\n",
            "Ep 2 (Step 009965): Train loss 2.272, Val loss 3.174\n",
            "Ep 2 (Step 009970): Train loss 2.490, Val loss 3.174\n",
            "Ep 2 (Step 009975): Train loss 2.733, Val loss 3.178\n",
            "Ep 2 (Step 009980): Train loss 2.470, Val loss 3.182\n",
            "Ep 2 (Step 009985): Train loss 2.622, Val loss 3.185\n",
            "Ep 2 (Step 009990): Train loss 2.777, Val loss 3.182\n",
            "Ep 2 (Step 009995): Train loss 2.599, Val loss 3.177\n",
            "Ep 2 (Step 010000): Train loss 2.558, Val loss 3.170\n",
            "Ep 2 (Step 010005): Train loss 2.567, Val loss 3.170\n",
            "Ep 2 (Step 010010): Train loss 2.588, Val loss 3.170\n",
            "Ep 2 (Step 010015): Train loss 2.450, Val loss 3.177\n",
            "Ep 2 (Step 010020): Train loss 2.631, Val loss 3.177\n",
            "Ep 2 (Step 010025): Train loss 2.465, Val loss 3.174\n",
            "Ep 2 (Step 010030): Train loss 2.743, Val loss 3.167\n",
            "Ep 2 (Step 010035): Train loss 2.231, Val loss 3.174\n",
            "Ep 2 (Step 010040): Train loss 2.607, Val loss 3.174\n",
            "Ep 2 (Step 010045): Train loss 2.529, Val loss 3.180\n",
            "Ep 2 (Step 010050): Train loss 2.763, Val loss 3.177\n",
            "Ep 2 (Step 010055): Train loss 2.720, Val loss 3.169\n",
            "Ep 2 (Step 010060): Train loss 2.452, Val loss 3.161\n",
            "Ep 2 (Step 010065): Train loss 2.515, Val loss 3.160\n",
            "Ep 2 (Step 010070): Train loss 2.485, Val loss 3.164\n",
            "Ep 2 (Step 010075): Train loss 2.545, Val loss 3.167\n",
            "Ep 2 (Step 010080): Train loss 2.392, Val loss 3.165\n",
            "Ep 2 (Step 010085): Train loss 2.594, Val loss 3.171\n",
            "Ep 2 (Step 010090): Train loss 2.524, Val loss 3.172\n",
            "Ep 2 (Step 010095): Train loss 2.565, Val loss 3.172\n",
            "Ep 2 (Step 010100): Train loss 2.540, Val loss 3.170\n",
            "Ep 2 (Step 010105): Train loss 2.551, Val loss 3.169\n",
            "Ep 2 (Step 010110): Train loss 2.598, Val loss 3.179\n",
            "Ep 2 (Step 010115): Train loss 2.382, Val loss 3.184\n",
            "Ep 2 (Step 010120): Train loss 2.463, Val loss 3.187\n",
            "Ep 2 (Step 010125): Train loss 2.779, Val loss 3.185\n",
            "Ep 2 (Step 010130): Train loss 2.242, Val loss 3.170\n",
            "Ep 2 (Step 010135): Train loss 2.742, Val loss 3.163\n",
            "Ep 2 (Step 010140): Train loss 2.662, Val loss 3.167\n",
            "Ep 2 (Step 010145): Train loss 2.427, Val loss 3.171\n",
            "Ep 2 (Step 010150): Train loss 2.533, Val loss 3.170\n",
            "Ep 2 (Step 010155): Train loss 2.569, Val loss 3.169\n",
            "Ep 2 (Step 010160): Train loss 2.470, Val loss 3.164\n",
            "Ep 2 (Step 010165): Train loss 2.481, Val loss 3.171\n",
            "Ep 2 (Step 010170): Train loss 2.495, Val loss 3.173\n",
            "Ep 2 (Step 010175): Train loss 2.387, Val loss 3.172\n",
            "Ep 2 (Step 010180): Train loss 2.546, Val loss 3.169\n",
            "Ep 2 (Step 010185): Train loss 2.679, Val loss 3.169\n",
            "Ep 2 (Step 010190): Train loss 2.572, Val loss 3.167\n",
            "Ep 2 (Step 010195): Train loss 2.537, Val loss 3.173\n",
            "Ep 2 (Step 010200): Train loss 2.655, Val loss 3.171\n",
            "Ep 2 (Step 010205): Train loss 2.653, Val loss 3.175\n",
            "Ep 2 (Step 010210): Train loss 2.737, Val loss 3.171\n",
            "Ep 2 (Step 010215): Train loss 2.606, Val loss 3.170\n",
            "Ep 2 (Step 010220): Train loss 2.496, Val loss 3.172\n",
            "Ep 2 (Step 010225): Train loss 2.519, Val loss 3.173\n",
            "Ep 2 (Step 010230): Train loss 2.536, Val loss 3.162\n",
            "Ep 2 (Step 010235): Train loss 2.492, Val loss 3.152\n",
            "Ep 2 (Step 010240): Train loss 2.619, Val loss 3.156\n",
            "Ep 2 (Step 010245): Train loss 2.589, Val loss 3.172\n",
            "Ep 2 (Step 010250): Train loss 2.494, Val loss 3.179\n",
            "Ep 2 (Step 010255): Train loss 2.791, Val loss 3.180\n",
            "Ep 2 (Step 010260): Train loss 2.758, Val loss 3.180\n",
            "Ep 2 (Step 010265): Train loss 2.743, Val loss 3.176\n",
            "Ep 2 (Step 010270): Train loss 2.571, Val loss 3.172\n",
            "Ep 2 (Step 010275): Train loss 2.654, Val loss 3.166\n",
            "Ep 2 (Step 010280): Train loss 2.603, Val loss 3.165\n",
            "Ep 2 (Step 010285): Train loss 2.515, Val loss 3.165\n",
            "Ep 2 (Step 010290): Train loss 2.367, Val loss 3.167\n",
            "Ep 2 (Step 010295): Train loss 2.526, Val loss 3.167\n",
            "Ep 2 (Step 010300): Train loss 2.592, Val loss 3.160\n",
            "Ep 2 (Step 010305): Train loss 2.496, Val loss 3.164\n",
            "Ep 2 (Step 010310): Train loss 2.453, Val loss 3.164\n",
            "Ep 2 (Step 010315): Train loss 2.548, Val loss 3.159\n",
            "Ep 2 (Step 010320): Train loss 2.456, Val loss 3.161\n",
            "Ep 2 (Step 010325): Train loss 2.358, Val loss 3.163\n",
            "Ep 2 (Step 010330): Train loss 2.622, Val loss 3.167\n",
            "Ep 2 (Step 010335): Train loss 2.485, Val loss 3.168\n",
            "Ep 2 (Step 010340): Train loss 2.526, Val loss 3.166\n",
            "Ep 2 (Step 010345): Train loss 2.717, Val loss 3.168\n",
            "Ep 2 (Step 010350): Train loss 2.437, Val loss 3.169\n",
            "Ep 2 (Step 010355): Train loss 2.570, Val loss 3.167\n",
            "Ep 2 (Step 010360): Train loss 2.563, Val loss 3.168\n",
            "Ep 2 (Step 010365): Train loss 2.605, Val loss 3.166\n",
            "Ep 2 (Step 010370): Train loss 2.454, Val loss 3.168\n",
            "Ep 2 (Step 010375): Train loss 2.472, Val loss 3.162\n",
            "Ep 2 (Step 010380): Train loss 2.505, Val loss 3.159\n",
            "Ep 2 (Step 010385): Train loss 2.651, Val loss 3.149\n",
            "Ep 2 (Step 010390): Train loss 2.489, Val loss 3.145\n",
            "Ep 2 (Step 010395): Train loss 2.685, Val loss 3.148\n",
            "Ep 2 (Step 010400): Train loss 2.634, Val loss 3.151\n",
            "Ep 2 (Step 010405): Train loss 2.616, Val loss 3.155\n",
            "Ep 2 (Step 010410): Train loss 2.656, Val loss 3.156\n",
            "Ep 2 (Step 010415): Train loss 2.481, Val loss 3.156\n",
            "Ep 2 (Step 010420): Train loss 2.479, Val loss 3.163\n",
            "Ep 2 (Step 010425): Train loss 2.803, Val loss 3.157\n",
            "Ep 2 (Step 010430): Train loss 2.416, Val loss 3.148\n",
            "Ep 2 (Step 010435): Train loss 2.355, Val loss 3.149\n",
            "Ep 2 (Step 010440): Train loss 2.764, Val loss 3.155\n",
            "Ep 2 (Step 010445): Train loss 2.366, Val loss 3.148\n",
            "Ep 2 (Step 010450): Train loss 2.539, Val loss 3.152\n",
            "Ep 2 (Step 010455): Train loss 2.704, Val loss 3.152\n",
            "Ep 2 (Step 010460): Train loss 2.654, Val loss 3.148\n",
            "Ep 2 (Step 010465): Train loss 2.675, Val loss 3.153\n",
            "Ep 2 (Step 010470): Train loss 2.511, Val loss 3.152\n",
            "Ep 2 (Step 010475): Train loss 2.587, Val loss 3.163\n",
            "Ep 2 (Step 010480): Train loss 2.472, Val loss 3.150\n",
            "Ep 2 (Step 010485): Train loss 2.379, Val loss 3.145\n",
            "Ep 2 (Step 010490): Train loss 2.576, Val loss 3.141\n",
            "Ep 2 (Step 010495): Train loss 2.404, Val loss 3.137\n",
            "Ep 2 (Step 010500): Train loss 2.625, Val loss 3.136\n",
            "Ep 2 (Step 010505): Train loss 2.699, Val loss 3.147\n",
            "Ep 2 (Step 010510): Train loss 2.641, Val loss 3.157\n",
            "Ep 2 (Step 010515): Train loss 2.465, Val loss 3.151\n",
            "Ep 2 (Step 010520): Train loss 2.480, Val loss 3.144\n",
            "Ep 2 (Step 010525): Train loss 2.644, Val loss 3.143\n",
            "Ep 2 (Step 010530): Train loss 2.519, Val loss 3.142\n",
            "Ep 2 (Step 010535): Train loss 2.589, Val loss 3.139\n",
            "Ep 2 (Step 010540): Train loss 2.542, Val loss 3.139\n",
            "Ep 2 (Step 010545): Train loss 2.565, Val loss 3.149\n",
            "Ep 2 (Step 010550): Train loss 2.591, Val loss 3.157\n",
            "Ep 2 (Step 010555): Train loss 2.474, Val loss 3.153\n",
            "Ep 2 (Step 010560): Train loss 2.707, Val loss 3.152\n",
            "Ep 2 (Step 010565): Train loss 2.502, Val loss 3.157\n",
            "Ep 2 (Step 010570): Train loss 2.544, Val loss 3.162\n",
            "Ep 2 (Step 010575): Train loss 2.443, Val loss 3.153\n",
            "Ep 2 (Step 010580): Train loss 2.669, Val loss 3.148\n",
            "Ep 2 (Step 010585): Train loss 2.393, Val loss 3.153\n",
            "Ep 2 (Step 010590): Train loss 2.424, Val loss 3.152\n",
            "Ep 2 (Step 010595): Train loss 2.662, Val loss 3.146\n",
            "Ep 2 (Step 010600): Train loss 2.527, Val loss 3.140\n",
            "Ep 2 (Step 010605): Train loss 2.537, Val loss 3.142\n",
            "Ep 2 (Step 010610): Train loss 2.693, Val loss 3.140\n",
            "Ep 2 (Step 010615): Train loss 2.379, Val loss 3.148\n",
            "Ep 2 (Step 010620): Train loss 2.633, Val loss 3.151\n",
            "Ep 2 (Step 010625): Train loss 2.337, Val loss 3.147\n",
            "Ep 2 (Step 010630): Train loss 2.570, Val loss 3.150\n",
            "Ep 2 (Step 010635): Train loss 2.571, Val loss 3.159\n",
            "Ep 2 (Step 010640): Train loss 2.310, Val loss 3.150\n",
            "Ep 2 (Step 010645): Train loss 2.450, Val loss 3.151\n",
            "Ep 2 (Step 010650): Train loss 2.679, Val loss 3.153\n",
            "Ep 2 (Step 010655): Train loss 2.605, Val loss 3.150\n",
            "Ep 2 (Step 010660): Train loss 2.672, Val loss 3.151\n",
            "Ep 2 (Step 010665): Train loss 2.739, Val loss 3.145\n",
            "Ep 2 (Step 010670): Train loss 2.566, Val loss 3.142\n",
            "Ep 2 (Step 010675): Train loss 2.767, Val loss 3.137\n",
            "Ep 2 (Step 010680): Train loss 2.599, Val loss 3.136\n",
            "Ep 2 (Step 010685): Train loss 2.924, Val loss 3.146\n",
            "Ep 2 (Step 010690): Train loss 2.600, Val loss 3.157\n",
            "Ep 2 (Step 010695): Train loss 2.575, Val loss 3.154\n",
            "Ep 2 (Step 010700): Train loss 2.371, Val loss 3.150\n",
            "Ep 2 (Step 010705): Train loss 2.603, Val loss 3.148\n",
            "Ep 2 (Step 010710): Train loss 2.556, Val loss 3.159\n",
            "Ep 2 (Step 010715): Train loss 2.318, Val loss 3.151\n",
            "Ep 2 (Step 010720): Train loss 2.527, Val loss 3.149\n",
            "Ep 2 (Step 010725): Train loss 2.577, Val loss 3.152\n",
            "Ep 2 (Step 010730): Train loss 2.599, Val loss 3.157\n",
            "Ep 2 (Step 010735): Train loss 2.456, Val loss 3.148\n",
            "Ep 2 (Step 010740): Train loss 2.535, Val loss 3.147\n",
            "Ep 2 (Step 010745): Train loss 2.612, Val loss 3.149\n",
            "Ep 2 (Step 010750): Train loss 2.445, Val loss 3.154\n",
            "Ep 2 (Step 010755): Train loss 2.589, Val loss 3.148\n",
            "Ep 2 (Step 010760): Train loss 2.641, Val loss 3.149\n",
            "Ep 2 (Step 010765): Train loss 2.439, Val loss 3.150\n",
            "Ep 2 (Step 010770): Train loss 2.621, Val loss 3.147\n",
            "Ep 2 (Step 010775): Train loss 2.665, Val loss 3.151\n",
            "Ep 2 (Step 010780): Train loss 2.418, Val loss 3.151\n",
            "Ep 2 (Step 010785): Train loss 2.559, Val loss 3.155\n",
            "Ep 2 (Step 010790): Train loss 2.661, Val loss 3.164\n",
            "Ep 2 (Step 010795): Train loss 2.628, Val loss 3.164\n",
            "Ep 2 (Step 010800): Train loss 2.546, Val loss 3.149\n",
            "Ep 2 (Step 010805): Train loss 2.590, Val loss 3.152\n",
            "Ep 2 (Step 010810): Train loss 2.629, Val loss 3.167\n",
            "Ep 2 (Step 010815): Train loss 2.552, Val loss 3.168\n",
            "Ep 2 (Step 010820): Train loss 2.560, Val loss 3.166\n",
            "Ep 2 (Step 010825): Train loss 2.426, Val loss 3.165\n",
            "Ep 2 (Step 010830): Train loss 2.537, Val loss 3.164\n",
            "Ep 2 (Step 010835): Train loss 2.512, Val loss 3.163\n",
            "Ep 2 (Step 010840): Train loss 2.700, Val loss 3.169\n",
            "Ep 2 (Step 010845): Train loss 2.465, Val loss 3.166\n",
            "Ep 2 (Step 010850): Train loss 2.350, Val loss 3.155\n",
            "Ep 2 (Step 010855): Train loss 2.398, Val loss 3.150\n",
            "Ep 2 (Step 010860): Train loss 2.501, Val loss 3.160\n",
            "Ep 2 (Step 010865): Train loss 2.735, Val loss 3.164\n",
            "Ep 2 (Step 010870): Train loss 2.804, Val loss 3.157\n",
            "Ep 2 (Step 010875): Train loss 2.408, Val loss 3.154\n",
            "Ep 2 (Step 010880): Train loss 2.494, Val loss 3.153\n",
            "Ep 2 (Step 010885): Train loss 2.464, Val loss 3.149\n",
            "Ep 2 (Step 010890): Train loss 2.287, Val loss 3.154\n",
            "Ep 2 (Step 010895): Train loss 2.215, Val loss 3.149\n",
            "Ep 2 (Step 010900): Train loss 2.471, Val loss 3.151\n",
            "Ep 2 (Step 010905): Train loss 2.509, Val loss 3.151\n",
            "Ep 2 (Step 010910): Train loss 2.698, Val loss 3.150\n",
            "Ep 2 (Step 010915): Train loss 2.411, Val loss 3.157\n",
            "Ep 2 (Step 010920): Train loss 2.688, Val loss 3.157\n",
            "Ep 2 (Step 010925): Train loss 2.418, Val loss 3.158\n",
            "Ep 2 (Step 010930): Train loss 2.530, Val loss 3.161\n",
            "Ep 2 (Step 010935): Train loss 2.453, Val loss 3.153\n",
            "Ep 2 (Step 010940): Train loss 2.608, Val loss 3.151\n",
            "Ep 2 (Step 010945): Train loss 2.737, Val loss 3.143\n",
            "Ep 2 (Step 010950): Train loss 2.480, Val loss 3.150\n",
            "Ep 2 (Step 010955): Train loss 2.464, Val loss 3.150\n",
            "Ep 2 (Step 010960): Train loss 2.509, Val loss 3.163\n",
            "Ep 2 (Step 010965): Train loss 2.314, Val loss 3.162\n",
            "Ep 2 (Step 010970): Train loss 2.312, Val loss 3.154\n",
            "Ep 2 (Step 010975): Train loss 2.395, Val loss 3.153\n",
            "Ep 2 (Step 010980): Train loss 2.570, Val loss 3.149\n",
            "Ep 2 (Step 010985): Train loss 2.648, Val loss 3.149\n",
            "Ep 2 (Step 010990): Train loss 2.578, Val loss 3.146\n",
            "Ep 2 (Step 010995): Train loss 2.460, Val loss 3.145\n",
            "Ep 2 (Step 011000): Train loss 2.564, Val loss 3.142\n",
            "Ep 2 (Step 011005): Train loss 2.464, Val loss 3.154\n",
            "Ep 2 (Step 011010): Train loss 2.734, Val loss 3.147\n",
            "Ep 2 (Step 011015): Train loss 2.609, Val loss 3.143\n",
            "Ep 2 (Step 011020): Train loss 2.451, Val loss 3.146\n",
            "Ep 2 (Step 011025): Train loss 2.562, Val loss 3.147\n",
            "Ep 2 (Step 011030): Train loss 2.487, Val loss 3.147\n",
            "Ep 2 (Step 011035): Train loss 2.608, Val loss 3.146\n",
            "Ep 2 (Step 011040): Train loss 2.437, Val loss 3.150\n",
            "Ep 2 (Step 011045): Train loss 2.568, Val loss 3.160\n",
            "Ep 2 (Step 011050): Train loss 2.625, Val loss 3.163\n",
            "Ep 2 (Step 011055): Train loss 2.631, Val loss 3.159\n",
            "Ep 2 (Step 011060): Train loss 2.512, Val loss 3.154\n",
            "Ep 2 (Step 011065): Train loss 2.572, Val loss 3.151\n",
            "Ep 2 (Step 011070): Train loss 2.541, Val loss 3.149\n",
            "Ep 2 (Step 011075): Train loss 2.534, Val loss 3.149\n",
            "Ep 2 (Step 011080): Train loss 2.600, Val loss 3.149\n",
            "Ep 2 (Step 011085): Train loss 2.326, Val loss 3.147\n",
            "Ep 2 (Step 011090): Train loss 2.753, Val loss 3.144\n",
            "Ep 2 (Step 011095): Train loss 2.481, Val loss 3.148\n",
            "Ep 2 (Step 011100): Train loss 2.615, Val loss 3.147\n",
            "Ep 2 (Step 011105): Train loss 2.400, Val loss 3.154\n",
            "Ep 2 (Step 011110): Train loss 2.660, Val loss 3.159\n",
            "Ep 2 (Step 011115): Train loss 2.449, Val loss 3.165\n",
            "Ep 2 (Step 011120): Train loss 2.498, Val loss 3.163\n",
            "Ep 2 (Step 011125): Train loss 2.313, Val loss 3.157\n",
            "Ep 2 (Step 011130): Train loss 2.445, Val loss 3.152\n",
            "Ep 2 (Step 011135): Train loss 2.573, Val loss 3.163\n",
            "Ep 2 (Step 011140): Train loss 2.569, Val loss 3.164\n",
            "Ep 2 (Step 011145): Train loss 2.407, Val loss 3.161\n",
            "Ep 2 (Step 011150): Train loss 2.460, Val loss 3.159\n",
            "Ep 2 (Step 011155): Train loss 2.566, Val loss 3.152\n",
            "Ep 2 (Step 011160): Train loss 2.473, Val loss 3.147\n",
            "Ep 2 (Step 011165): Train loss 2.364, Val loss 3.151\n",
            "Ep 2 (Step 011170): Train loss 2.518, Val loss 3.153\n",
            "Ep 2 (Step 011175): Train loss 2.463, Val loss 3.156\n",
            "Ep 2 (Step 011180): Train loss 2.487, Val loss 3.147\n",
            "Ep 2 (Step 011185): Train loss 2.433, Val loss 3.145\n",
            "Ep 2 (Step 011190): Train loss 2.392, Val loss 3.148\n",
            "Ep 2 (Step 011195): Train loss 2.560, Val loss 3.149\n",
            "Ep 2 (Step 011200): Train loss 2.404, Val loss 3.147\n",
            "Ep 2 (Step 011205): Train loss 2.663, Val loss 3.146\n",
            "Ep 2 (Step 011210): Train loss 2.419, Val loss 3.150\n",
            "Ep 2 (Step 011215): Train loss 2.627, Val loss 3.147\n",
            "Ep 2 (Step 011220): Train loss 2.481, Val loss 3.143\n",
            "Ep 2 (Step 011225): Train loss 2.368, Val loss 3.139\n",
            "Ep 2 (Step 011230): Train loss 2.461, Val loss 3.140\n",
            "Ep 2 (Step 011235): Train loss 2.448, Val loss 3.137\n",
            "Ep 2 (Step 011240): Train loss 2.510, Val loss 3.141\n",
            "Ep 2 (Step 011245): Train loss 2.409, Val loss 3.146\n",
            "Ep 2 (Step 011250): Train loss 2.566, Val loss 3.149\n",
            "Ep 2 (Step 011255): Train loss 2.443, Val loss 3.142\n",
            "Ep 2 (Step 011260): Train loss 2.576, Val loss 3.145\n",
            "Ep 2 (Step 011265): Train loss 2.391, Val loss 3.140\n",
            "Ep 2 (Step 011270): Train loss 2.432, Val loss 3.151\n",
            "Ep 2 (Step 011275): Train loss 2.500, Val loss 3.150\n",
            "Ep 2 (Step 011280): Train loss 2.417, Val loss 3.138\n",
            "Ep 2 (Step 011285): Train loss 2.663, Val loss 3.137\n",
            "Ep 2 (Step 011290): Train loss 2.556, Val loss 3.142\n",
            "Ep 2 (Step 011295): Train loss 2.526, Val loss 3.145\n",
            "Ep 2 (Step 011300): Train loss 2.528, Val loss 3.147\n",
            "Ep 2 (Step 011305): Train loss 2.501, Val loss 3.140\n",
            "Ep 2 (Step 011310): Train loss 2.356, Val loss 3.145\n",
            "Ep 2 (Step 011315): Train loss 2.489, Val loss 3.148\n",
            "Ep 2 (Step 011320): Train loss 2.576, Val loss 3.142\n",
            "Ep 2 (Step 011325): Train loss 2.477, Val loss 3.146\n",
            "Ep 2 (Step 011330): Train loss 2.569, Val loss 3.148\n",
            "Ep 2 (Step 011335): Train loss 2.342, Val loss 3.152\n",
            "Ep 2 (Step 011340): Train loss 2.640, Val loss 3.151\n",
            "Ep 2 (Step 011345): Train loss 2.715, Val loss 3.146\n",
            "Ep 2 (Step 011350): Train loss 2.412, Val loss 3.141\n",
            "Ep 2 (Step 011355): Train loss 2.406, Val loss 3.137\n",
            "Ep 2 (Step 011360): Train loss 2.555, Val loss 3.139\n",
            "Ep 2 (Step 011365): Train loss 2.570, Val loss 3.134\n",
            "Ep 2 (Step 011370): Train loss 2.559, Val loss 3.133\n",
            "Ep 2 (Step 011375): Train loss 2.571, Val loss 3.140\n",
            "Ep 2 (Step 011380): Train loss 2.446, Val loss 3.144\n",
            "Ep 2 (Step 011385): Train loss 2.510, Val loss 3.140\n",
            "Ep 2 (Step 011390): Train loss 2.600, Val loss 3.142\n",
            "Ep 2 (Step 011395): Train loss 2.165, Val loss 3.145\n",
            "Ep 2 (Step 011400): Train loss 2.500, Val loss 3.148\n",
            "Ep 2 (Step 011405): Train loss 2.566, Val loss 3.153\n",
            "Ep 2 (Step 011410): Train loss 2.473, Val loss 3.158\n",
            "Ep 2 (Step 011415): Train loss 2.767, Val loss 3.141\n",
            "Ep 2 (Step 011420): Train loss 2.560, Val loss 3.132\n",
            "Ep 2 (Step 011425): Train loss 2.454, Val loss 3.131\n",
            "Ep 2 (Step 011430): Train loss 2.401, Val loss 3.131\n",
            "Ep 2 (Step 011435): Train loss 2.617, Val loss 3.130\n",
            "Ep 2 (Step 011440): Train loss 2.486, Val loss 3.134\n",
            "Ep 2 (Step 011445): Train loss 2.465, Val loss 3.138\n",
            "Ep 2 (Step 011450): Train loss 2.551, Val loss 3.136\n",
            "Ep 2 (Step 011455): Train loss 2.398, Val loss 3.134\n",
            "Ep 2 (Step 011460): Train loss 2.561, Val loss 3.128\n",
            "Ep 2 (Step 011465): Train loss 2.493, Val loss 3.129\n",
            "Ep 2 (Step 011470): Train loss 2.462, Val loss 3.127\n",
            "Ep 2 (Step 011475): Train loss 2.612, Val loss 3.128\n",
            "Ep 2 (Step 011480): Train loss 2.440, Val loss 3.129\n",
            "Ep 2 (Step 011485): Train loss 2.384, Val loss 3.130\n",
            "Ep 2 (Step 011490): Train loss 2.405, Val loss 3.135\n",
            "Ep 2 (Step 011495): Train loss 2.631, Val loss 3.139\n",
            "Ep 2 (Step 011500): Train loss 2.528, Val loss 3.139\n",
            "Ep 2 (Step 011505): Train loss 2.602, Val loss 3.137\n",
            "Ep 2 (Step 011510): Train loss 2.337, Val loss 3.134\n",
            "Ep 2 (Step 011515): Train loss 2.774, Val loss 3.138\n",
            "Ep 2 (Step 011520): Train loss 2.618, Val loss 3.140\n",
            "Ep 2 (Step 011525): Train loss 2.645, Val loss 3.142\n",
            "Ep 2 (Step 011530): Train loss 2.508, Val loss 3.139\n",
            "Ep 2 (Step 011535): Train loss 2.453, Val loss 3.140\n",
            "Ep 2 (Step 011540): Train loss 2.527, Val loss 3.140\n",
            "Ep 2 (Step 011545): Train loss 2.659, Val loss 3.136\n",
            "Ep 2 (Step 011550): Train loss 2.706, Val loss 3.133\n",
            "Ep 2 (Step 011555): Train loss 2.579, Val loss 3.140\n",
            "Ep 2 (Step 011560): Train loss 2.357, Val loss 3.135\n",
            "Ep 2 (Step 011565): Train loss 2.433, Val loss 3.138\n",
            "Ep 2 (Step 011570): Train loss 2.694, Val loss 3.142\n",
            "Ep 2 (Step 011575): Train loss 2.459, Val loss 3.143\n",
            "Ep 2 (Step 011580): Train loss 2.424, Val loss 3.141\n",
            "Ep 2 (Step 011585): Train loss 2.705, Val loss 3.143\n",
            "Ep 2 (Step 011590): Train loss 2.579, Val loss 3.147\n",
            "Ep 2 (Step 011595): Train loss 2.587, Val loss 3.143\n",
            "Ep 2 (Step 011600): Train loss 2.381, Val loss 3.141\n",
            "Ep 2 (Step 011605): Train loss 2.520, Val loss 3.137\n",
            "Ep 2 (Step 011610): Train loss 2.686, Val loss 3.145\n",
            "Ep 2 (Step 011615): Train loss 2.530, Val loss 3.153\n",
            "Ep 2 (Step 011620): Train loss 2.766, Val loss 3.147\n",
            "Ep 2 (Step 011625): Train loss 2.288, Val loss 3.138\n",
            "Ep 2 (Step 011630): Train loss 2.538, Val loss 3.141\n",
            "Ep 2 (Step 011635): Train loss 2.415, Val loss 3.147\n",
            "Ep 2 (Step 011640): Train loss 2.395, Val loss 3.152\n",
            "Ep 2 (Step 011645): Train loss 2.399, Val loss 3.148\n",
            "Ep 2 (Step 011650): Train loss 2.203, Val loss 3.148\n",
            "Ep 2 (Step 011655): Train loss 2.422, Val loss 3.145\n",
            "Ep 2 (Step 011660): Train loss 2.554, Val loss 3.147\n",
            "Ep 2 (Step 011665): Train loss 2.647, Val loss 3.149\n",
            "Ep 2 (Step 011670): Train loss 2.614, Val loss 3.139\n",
            "Ep 2 (Step 011675): Train loss 2.568, Val loss 3.134\n",
            "Ep 2 (Step 011680): Train loss 2.802, Val loss 3.132\n",
            "Ep 2 (Step 011685): Train loss 2.756, Val loss 3.135\n",
            "Ep 2 (Step 011690): Train loss 2.594, Val loss 3.142\n",
            "Ep 2 (Step 011695): Train loss 2.419, Val loss 3.149\n",
            "Ep 2 (Step 011700): Train loss 2.609, Val loss 3.150\n",
            "Ep 2 (Step 011705): Train loss 2.424, Val loss 3.145\n",
            "Ep 2 (Step 011710): Train loss 2.418, Val loss 3.141\n",
            "Ep 2 (Step 011715): Train loss 2.262, Val loss 3.135\n",
            "Ep 2 (Step 011720): Train loss 2.581, Val loss 3.134\n",
            "Ep 2 (Step 011725): Train loss 2.479, Val loss 3.128\n",
            "Ep 2 (Step 011730): Train loss 2.640, Val loss 3.134\n",
            "Ep 2 (Step 011735): Train loss 2.490, Val loss 3.135\n",
            "Ep 2 (Step 011740): Train loss 2.399, Val loss 3.142\n",
            "Ep 2 (Step 011745): Train loss 2.639, Val loss 3.146\n",
            "Ep 2 (Step 011750): Train loss 2.647, Val loss 3.140\n",
            "Ep 2 (Step 011755): Train loss 2.410, Val loss 3.137\n",
            "Ep 2 (Step 011760): Train loss 2.687, Val loss 3.137\n",
            "Ep 2 (Step 011765): Train loss 2.547, Val loss 3.143\n",
            "Ep 2 (Step 011770): Train loss 2.497, Val loss 3.143\n",
            "Ep 2 (Step 011775): Train loss 2.630, Val loss 3.134\n",
            "Ep 2 (Step 011780): Train loss 2.262, Val loss 3.140\n",
            "Ep 2 (Step 011785): Train loss 2.731, Val loss 3.144\n",
            "Ep 2 (Step 011790): Train loss 2.444, Val loss 3.147\n",
            "Ep 2 (Step 011795): Train loss 2.782, Val loss 3.138\n",
            "Ep 2 (Step 011800): Train loss 2.495, Val loss 3.137\n",
            "Ep 2 (Step 011805): Train loss 2.688, Val loss 3.133\n",
            "Ep 2 (Step 011810): Train loss 2.416, Val loss 3.135\n",
            "Ep 2 (Step 011815): Train loss 2.368, Val loss 3.142\n",
            "Ep 2 (Step 011820): Train loss 2.617, Val loss 3.140\n",
            "Ep 2 (Step 011825): Train loss 2.544, Val loss 3.143\n",
            "Ep 2 (Step 011830): Train loss 2.493, Val loss 3.138\n",
            "Ep 2 (Step 011835): Train loss 2.636, Val loss 3.127\n",
            "Ep 2 (Step 011840): Train loss 2.471, Val loss 3.124\n",
            "Ep 2 (Step 011845): Train loss 2.246, Val loss 3.130\n",
            "Ep 2 (Step 011850): Train loss 2.438, Val loss 3.133\n",
            "Ep 2 (Step 011855): Train loss 2.385, Val loss 3.132\n",
            "Ep 2 (Step 011860): Train loss 2.497, Val loss 3.125\n",
            "Ep 2 (Step 011865): Train loss 2.421, Val loss 3.127\n",
            "Ep 2 (Step 011870): Train loss 2.374, Val loss 3.138\n",
            "Ep 2 (Step 011875): Train loss 2.505, Val loss 3.138\n",
            "Ep 2 (Step 011880): Train loss 2.528, Val loss 3.125\n",
            "Ep 2 (Step 011885): Train loss 2.484, Val loss 3.119\n",
            "Ep 2 (Step 011890): Train loss 2.458, Val loss 3.126\n",
            "Ep 2 (Step 011895): Train loss 2.533, Val loss 3.131\n",
            "Ep 2 (Step 011900): Train loss 2.240, Val loss 3.130\n",
            "Ep 2 (Step 011905): Train loss 2.465, Val loss 3.128\n",
            "Ep 2 (Step 011910): Train loss 2.395, Val loss 3.123\n",
            "Ep 2 (Step 011915): Train loss 2.626, Val loss 3.126\n",
            "Ep 2 (Step 011920): Train loss 2.625, Val loss 3.131\n",
            "Ep 2 (Step 011925): Train loss 2.547, Val loss 3.130\n",
            "Ep 2 (Step 011930): Train loss 2.485, Val loss 3.125\n",
            "Ep 2 (Step 011935): Train loss 2.453, Val loss 3.130\n",
            "Ep 2 (Step 011940): Train loss 2.411, Val loss 3.123\n",
            "Ep 2 (Step 011945): Train loss 2.502, Val loss 3.118\n",
            "Ep 2 (Step 011950): Train loss 2.384, Val loss 3.124\n",
            "Ep 2 (Step 011955): Train loss 2.318, Val loss 3.122\n",
            "Ep 2 (Step 011960): Train loss 2.415, Val loss 3.133\n",
            "Ep 2 (Step 011965): Train loss 2.622, Val loss 3.141\n",
            "Ep 2 (Step 011970): Train loss 2.413, Val loss 3.139\n",
            "Ep 2 (Step 011975): Train loss 2.434, Val loss 3.126\n",
            "Ep 2 (Step 011980): Train loss 2.498, Val loss 3.125\n",
            "Ep 2 (Step 011985): Train loss 2.562, Val loss 3.123\n",
            "Ep 2 (Step 011990): Train loss 2.643, Val loss 3.133\n",
            "Ep 2 (Step 011995): Train loss 2.544, Val loss 3.128\n",
            "Ep 2 (Step 012000): Train loss 2.460, Val loss 3.121\n",
            "Ep 2 (Step 012005): Train loss 2.325, Val loss 3.119\n",
            "Ep 2 (Step 012010): Train loss 2.548, Val loss 3.124\n",
            "Ep 2 (Step 012015): Train loss 2.416, Val loss 3.126\n",
            "Ep 2 (Step 012020): Train loss 2.580, Val loss 3.126\n",
            "Ep 2 (Step 012025): Train loss 2.619, Val loss 3.131\n",
            "Ep 2 (Step 012030): Train loss 2.555, Val loss 3.128\n",
            "Ep 2 (Step 012035): Train loss 2.554, Val loss 3.121\n",
            "Ep 2 (Step 012040): Train loss 2.506, Val loss 3.119\n",
            "Ep 2 (Step 012045): Train loss 2.369, Val loss 3.130\n",
            "Ep 2 (Step 012050): Train loss 2.546, Val loss 3.129\n",
            "Ep 2 (Step 012055): Train loss 2.392, Val loss 3.121\n",
            "Ep 2 (Step 012060): Train loss 2.548, Val loss 3.118\n",
            "Ep 2 (Step 012065): Train loss 2.707, Val loss 3.118\n",
            "Ep 2 (Step 012070): Train loss 2.225, Val loss 3.118\n",
            "Ep 2 (Step 012075): Train loss 2.540, Val loss 3.121\n",
            "Ep 2 (Step 012080): Train loss 2.546, Val loss 3.131\n",
            "Ep 2 (Step 012085): Train loss 2.533, Val loss 3.134\n",
            "Ep 2 (Step 012090): Train loss 2.430, Val loss 3.134\n",
            "Ep 2 (Step 012095): Train loss 2.524, Val loss 3.130\n",
            "Ep 2 (Step 012100): Train loss 2.459, Val loss 3.130\n",
            "Ep 2 (Step 012105): Train loss 2.339, Val loss 3.131\n",
            "Ep 2 (Step 012110): Train loss 2.620, Val loss 3.132\n",
            "Ep 2 (Step 012115): Train loss 2.389, Val loss 3.134\n",
            "Ep 2 (Step 012120): Train loss 2.534, Val loss 3.126\n",
            "Ep 2 (Step 012125): Train loss 2.732, Val loss 3.122\n",
            "Ep 2 (Step 012130): Train loss 2.543, Val loss 3.126\n",
            "Ep 2 (Step 012135): Train loss 2.619, Val loss 3.131\n",
            "Ep 2 (Step 012140): Train loss 2.573, Val loss 3.138\n",
            "Ep 2 (Step 012145): Train loss 2.499, Val loss 3.134\n",
            "Ep 2 (Step 012150): Train loss 2.422, Val loss 3.126\n",
            "Ep 2 (Step 012155): Train loss 2.450, Val loss 3.122\n",
            "Ep 2 (Step 012160): Train loss 2.415, Val loss 3.121\n",
            "Ep 2 (Step 012165): Train loss 2.546, Val loss 3.129\n",
            "Ep 2 (Step 012170): Train loss 2.430, Val loss 3.136\n",
            "Ep 2 (Step 012175): Train loss 2.183, Val loss 3.129\n",
            "Ep 2 (Step 012180): Train loss 2.414, Val loss 3.125\n",
            "Ep 2 (Step 012185): Train loss 2.379, Val loss 3.125\n",
            "Ep 2 (Step 012190): Train loss 2.609, Val loss 3.126\n",
            "Ep 2 (Step 012195): Train loss 2.434, Val loss 3.129\n",
            "Ep 2 (Step 012200): Train loss 2.548, Val loss 3.128\n",
            "Ep 2 (Step 012205): Train loss 2.629, Val loss 3.126\n",
            "Ep 2 (Step 012210): Train loss 2.408, Val loss 3.125\n",
            "Ep 2 (Step 012215): Train loss 2.372, Val loss 3.128\n",
            "Ep 2 (Step 012220): Train loss 2.486, Val loss 3.131\n",
            "Ep 2 (Step 012225): Train loss 2.580, Val loss 3.125\n",
            "Ep 2 (Step 012230): Train loss 2.539, Val loss 3.121\n",
            "Ep 2 (Step 012235): Train loss 2.430, Val loss 3.123\n",
            "Ep 2 (Step 012240): Train loss 2.624, Val loss 3.116\n",
            "Ep 2 (Step 012245): Train loss 2.581, Val loss 3.116\n",
            "Ep 2 (Step 012250): Train loss 2.594, Val loss 3.113\n",
            "Ep 2 (Step 012255): Train loss 2.537, Val loss 3.118\n",
            "Ep 2 (Step 012260): Train loss 2.394, Val loss 3.119\n",
            "Ep 2 (Step 012265): Train loss 2.661, Val loss 3.116\n",
            "Ep 2 (Step 012270): Train loss 2.621, Val loss 3.112\n",
            "Ep 2 (Step 012275): Train loss 2.770, Val loss 3.114\n",
            "Ep 2 (Step 012280): Train loss 2.572, Val loss 3.125\n",
            "Ep 2 (Step 012285): Train loss 2.562, Val loss 3.130\n",
            "Ep 2 (Step 012290): Train loss 2.536, Val loss 3.130\n",
            "Ep 2 (Step 012295): Train loss 2.472, Val loss 3.126\n",
            "Ep 2 (Step 012300): Train loss 2.311, Val loss 3.128\n",
            "Ep 2 (Step 012305): Train loss 2.450, Val loss 3.123\n",
            "Ep 2 (Step 012310): Train loss 2.486, Val loss 3.108\n",
            "Ep 2 (Step 012315): Train loss 2.360, Val loss 3.098\n",
            "Ep 2 (Step 012320): Train loss 2.306, Val loss 3.106\n",
            "Ep 2 (Step 012325): Train loss 2.449, Val loss 3.117\n",
            "Ep 2 (Step 012330): Train loss 2.475, Val loss 3.125\n",
            "Ep 2 (Step 012335): Train loss 2.461, Val loss 3.116\n",
            "Ep 2 (Step 012340): Train loss 2.240, Val loss 3.107\n",
            "Ep 2 (Step 012345): Train loss 2.386, Val loss 3.109\n",
            "Ep 2 (Step 012350): Train loss 2.302, Val loss 3.112\n",
            "Ep 2 (Step 012355): Train loss 2.478, Val loss 3.118\n",
            "Ep 2 (Step 012360): Train loss 2.463, Val loss 3.112\n",
            "Ep 2 (Step 012365): Train loss 2.580, Val loss 3.116\n",
            "Ep 2 (Step 012370): Train loss 2.656, Val loss 3.118\n",
            "Ep 2 (Step 012375): Train loss 2.630, Val loss 3.122\n",
            "Ep 2 (Step 012380): Train loss 2.454, Val loss 3.123\n",
            "Ep 2 (Step 012385): Train loss 2.299, Val loss 3.119\n",
            "Ep 2 (Step 012390): Train loss 2.311, Val loss 3.127\n",
            "Ep 2 (Step 012395): Train loss 2.445, Val loss 3.123\n",
            "Ep 2 (Step 012400): Train loss 2.420, Val loss 3.121\n",
            "Ep 2 (Step 012405): Train loss 2.432, Val loss 3.125\n",
            "Ep 2 (Step 012410): Train loss 2.456, Val loss 3.121\n",
            "Ep 2 (Step 012415): Train loss 2.591, Val loss 3.125\n",
            "Ep 2 (Step 012420): Train loss 2.438, Val loss 3.122\n",
            "Ep 2 (Step 012425): Train loss 2.754, Val loss 3.117\n",
            "Ep 2 (Step 012430): Train loss 2.289, Val loss 3.122\n",
            "Ep 2 (Step 012435): Train loss 2.385, Val loss 3.118\n",
            "Ep 2 (Step 012440): Train loss 2.619, Val loss 3.122\n",
            "Ep 2 (Step 012445): Train loss 2.582, Val loss 3.124\n",
            "Ep 2 (Step 012450): Train loss 2.249, Val loss 3.120\n",
            "Ep 2 (Step 012455): Train loss 2.549, Val loss 3.116\n",
            "Ep 2 (Step 012460): Train loss 2.563, Val loss 3.121\n",
            "Ep 2 (Step 012465): Train loss 2.443, Val loss 3.119\n",
            "Ep 2 (Step 012470): Train loss 2.495, Val loss 3.120\n",
            "Ep 2 (Step 012475): Train loss 2.372, Val loss 3.115\n",
            "Ep 2 (Step 012480): Train loss 2.648, Val loss 3.114\n",
            "Ep 2 (Step 012485): Train loss 2.465, Val loss 3.120\n",
            "Ep 2 (Step 012490): Train loss 2.531, Val loss 3.119\n",
            "Ep 2 (Step 012495): Train loss 2.435, Val loss 3.112\n",
            "Ep 2 (Step 012500): Train loss 2.655, Val loss 3.113\n",
            "Ep 2 (Step 012505): Train loss 2.667, Val loss 3.118\n",
            "Ep 2 (Step 012510): Train loss 2.462, Val loss 3.113\n",
            "Ep 2 (Step 012515): Train loss 2.443, Val loss 3.110\n",
            "Ep 2 (Step 012520): Train loss 2.465, Val loss 3.111\n",
            "Ep 2 (Step 012525): Train loss 2.474, Val loss 3.114\n",
            "Ep 2 (Step 012530): Train loss 2.543, Val loss 3.114\n",
            "Ep 2 (Step 012535): Train loss 2.664, Val loss 3.111\n",
            "Ep 2 (Step 012540): Train loss 2.352, Val loss 3.109\n",
            "Ep 2 (Step 012545): Train loss 2.467, Val loss 3.117\n",
            "Ep 2 (Step 012550): Train loss 2.587, Val loss 3.123\n",
            "Ep 2 (Step 012555): Train loss 2.454, Val loss 3.124\n",
            "Ep 2 (Step 012560): Train loss 2.589, Val loss 3.119\n",
            "Ep 2 (Step 012565): Train loss 2.419, Val loss 3.110\n",
            "Ep 2 (Step 012570): Train loss 2.552, Val loss 3.111\n",
            "Ep 2 (Step 012575): Train loss 2.542, Val loss 3.114\n",
            "Ep 2 (Step 012580): Train loss 2.534, Val loss 3.115\n",
            "Ep 2 (Step 012585): Train loss 2.508, Val loss 3.114\n",
            "Ep 2 (Step 012590): Train loss 2.451, Val loss 3.111\n",
            "Ep 2 (Step 012595): Train loss 2.642, Val loss 3.110\n",
            "Ep 2 (Step 012600): Train loss 2.500, Val loss 3.114\n",
            "Ep 2 (Step 012605): Train loss 2.455, Val loss 3.119\n",
            "Ep 2 (Step 012610): Train loss 2.588, Val loss 3.123\n",
            "Ep 2 (Step 012615): Train loss 2.283, Val loss 3.123\n",
            "Ep 2 (Step 012620): Train loss 2.360, Val loss 3.121\n",
            "Ep 2 (Step 012625): Train loss 2.379, Val loss 3.119\n",
            "Ep 2 (Step 012630): Train loss 2.580, Val loss 3.121\n",
            "Ep 2 (Step 012635): Train loss 2.489, Val loss 3.118\n",
            "Ep 2 (Step 012640): Train loss 2.516, Val loss 3.114\n",
            "Ep 2 (Step 012645): Train loss 2.559, Val loss 3.116\n",
            "Ep 2 (Step 012650): Train loss 2.437, Val loss 3.116\n",
            "Ep 2 (Step 012655): Train loss 2.316, Val loss 3.119\n",
            "Ep 2 (Step 012660): Train loss 2.322, Val loss 3.122\n",
            "Ep 2 (Step 012665): Train loss 2.574, Val loss 3.123\n",
            "Ep 2 (Step 012670): Train loss 2.537, Val loss 3.121\n",
            "Ep 2 (Step 012675): Train loss 2.555, Val loss 3.118\n",
            "Ep 2 (Step 012680): Train loss 2.449, Val loss 3.120\n",
            "Ep 2 (Step 012685): Train loss 2.571, Val loss 3.126\n",
            "Ep 2 (Step 012690): Train loss 2.245, Val loss 3.126\n",
            "Ep 2 (Step 012695): Train loss 2.532, Val loss 3.114\n",
            "Ep 2 (Step 012700): Train loss 2.328, Val loss 3.112\n",
            "Ep 2 (Step 012705): Train loss 2.409, Val loss 3.115\n",
            "Ep 2 (Step 012710): Train loss 2.332, Val loss 3.120\n",
            "Ep 2 (Step 012715): Train loss 2.545, Val loss 3.112\n",
            "Ep 2 (Step 012720): Train loss 2.336, Val loss 3.108\n",
            "Ep 2 (Step 012725): Train loss 2.363, Val loss 3.110\n",
            "Ep 2 (Step 012730): Train loss 2.385, Val loss 3.118\n",
            "Ep 2 (Step 012735): Train loss 2.634, Val loss 3.126\n",
            "Ep 2 (Step 012740): Train loss 2.376, Val loss 3.128\n",
            "Ep 2 (Step 012745): Train loss 2.674, Val loss 3.129\n",
            "Ep 2 (Step 012750): Train loss 2.815, Val loss 3.122\n",
            "Ep 2 (Step 012755): Train loss 2.605, Val loss 3.126\n",
            "Ep 2 (Step 012760): Train loss 2.624, Val loss 3.123\n",
            "Ep 2 (Step 012765): Train loss 2.425, Val loss 3.126\n",
            "Ep 2 (Step 012770): Train loss 2.470, Val loss 3.125\n",
            "Ep 2 (Step 012775): Train loss 2.626, Val loss 3.120\n",
            "Ep 2 (Step 012780): Train loss 2.447, Val loss 3.118\n",
            "Ep 2 (Step 012785): Train loss 2.285, Val loss 3.125\n",
            "Ep 2 (Step 012790): Train loss 2.427, Val loss 3.124\n",
            "Ep 2 (Step 012795): Train loss 2.517, Val loss 3.120\n",
            "Ep 2 (Step 012800): Train loss 2.384, Val loss 3.124\n",
            "Ep 2 (Step 012805): Train loss 2.360, Val loss 3.125\n",
            "Ep 2 (Step 012810): Train loss 2.386, Val loss 3.125\n",
            "Ep 2 (Step 012815): Train loss 2.307, Val loss 3.131\n",
            "Ep 2 (Step 012820): Train loss 2.473, Val loss 3.133\n",
            "Ep 2 (Step 012825): Train loss 2.443, Val loss 3.128\n",
            "Ep 2 (Step 012830): Train loss 2.684, Val loss 3.131\n",
            "Ep 2 (Step 012835): Train loss 2.529, Val loss 3.123\n",
            "Ep 2 (Step 012840): Train loss 2.210, Val loss 3.124\n",
            "Ep 2 (Step 012845): Train loss 2.485, Val loss 3.128\n",
            "Ep 2 (Step 012850): Train loss 2.360, Val loss 3.126\n",
            "Ep 2 (Step 012855): Train loss 2.641, Val loss 3.126\n",
            "Ep 2 (Step 012860): Train loss 2.387, Val loss 3.120\n",
            "Ep 2 (Step 012865): Train loss 2.577, Val loss 3.119\n",
            "Ep 2 (Step 012870): Train loss 2.423, Val loss 3.132\n",
            "Ep 2 (Step 012875): Train loss 2.283, Val loss 3.133\n",
            "Ep 2 (Step 012880): Train loss 2.325, Val loss 3.124\n",
            "Ep 2 (Step 012885): Train loss 2.483, Val loss 3.117\n",
            "Ep 2 (Step 012890): Train loss 2.339, Val loss 3.109\n",
            "Ep 2 (Step 012895): Train loss 2.496, Val loss 3.101\n",
            "Ep 2 (Step 012900): Train loss 2.406, Val loss 3.103\n",
            "Ep 2 (Step 012905): Train loss 2.475, Val loss 3.115\n",
            "Ep 2 (Step 012910): Train loss 2.415, Val loss 3.119\n",
            "Ep 2 (Step 012915): Train loss 2.502, Val loss 3.121\n",
            "Ep 2 (Step 012920): Train loss 2.534, Val loss 3.120\n",
            "Ep 2 (Step 012925): Train loss 2.433, Val loss 3.112\n",
            "Ep 2 (Step 012930): Train loss 2.342, Val loss 3.108\n",
            "Ep 2 (Step 012935): Train loss 2.521, Val loss 3.107\n",
            "Ep 2 (Step 012940): Train loss 2.381, Val loss 3.108\n",
            "Ep 2 (Step 012945): Train loss 2.416, Val loss 3.115\n",
            "Ep 2 (Step 012950): Train loss 2.485, Val loss 3.115\n",
            "Ep 2 (Step 012955): Train loss 2.512, Val loss 3.111\n",
            "Ep 2 (Step 012960): Train loss 2.603, Val loss 3.110\n",
            "Ep 2 (Step 012965): Train loss 2.420, Val loss 3.109\n",
            "Ep 2 (Step 012970): Train loss 2.332, Val loss 3.115\n",
            "Ep 2 (Step 012975): Train loss 2.338, Val loss 3.115\n",
            "Ep 2 (Step 012980): Train loss 2.662, Val loss 3.115\n",
            "Ep 2 (Step 012985): Train loss 2.384, Val loss 3.124\n",
            "Ep 2 (Step 012990): Train loss 2.461, Val loss 3.123\n",
            "Ep 2 (Step 012995): Train loss 2.558, Val loss 3.121\n",
            "Ep 2 (Step 013000): Train loss 2.442, Val loss 3.120\n",
            "Ep 2 (Step 013005): Train loss 2.346, Val loss 3.117\n",
            "Ep 2 (Step 013010): Train loss 2.509, Val loss 3.122\n",
            "Ep 2 (Step 013015): Train loss 2.528, Val loss 3.124\n",
            "Ep 2 (Step 013020): Train loss 2.518, Val loss 3.124\n",
            "Ep 2 (Step 013025): Train loss 2.522, Val loss 3.126\n",
            "Ep 2 (Step 013030): Train loss 2.442, Val loss 3.118\n",
            "Ep 2 (Step 013035): Train loss 2.476, Val loss 3.116\n",
            "Ep 2 (Step 013040): Train loss 2.332, Val loss 3.117\n",
            "Ep 2 (Step 013045): Train loss 2.584, Val loss 3.113\n",
            "Ep 2 (Step 013050): Train loss 2.453, Val loss 3.109\n",
            "Ep 2 (Step 013055): Train loss 2.410, Val loss 3.120\n",
            "Ep 2 (Step 013060): Train loss 2.448, Val loss 3.119\n",
            "Ep 2 (Step 013065): Train loss 2.535, Val loss 3.105\n",
            "Ep 2 (Step 013070): Train loss 2.470, Val loss 3.108\n",
            "Ep 2 (Step 013075): Train loss 2.532, Val loss 3.116\n",
            "Ep 2 (Step 013080): Train loss 2.575, Val loss 3.125\n",
            "Ep 2 (Step 013085): Train loss 2.639, Val loss 3.123\n",
            "Ep 2 (Step 013090): Train loss 2.204, Val loss 3.122\n",
            "Ep 2 (Step 013095): Train loss 2.422, Val loss 3.118\n",
            "Ep 2 (Step 013100): Train loss 2.301, Val loss 3.120\n",
            "Ep 2 (Step 013105): Train loss 2.656, Val loss 3.115\n",
            "Ep 2 (Step 013110): Train loss 2.405, Val loss 3.111\n",
            "Ep 2 (Step 013115): Train loss 2.617, Val loss 3.110\n",
            "Ep 2 (Step 013120): Train loss 2.425, Val loss 3.104\n",
            "Ep 2 (Step 013125): Train loss 2.390, Val loss 3.102\n",
            "Ep 2 (Step 013130): Train loss 2.345, Val loss 3.104\n",
            "Ep 2 (Step 013135): Train loss 2.656, Val loss 3.110\n",
            "Ep 2 (Step 013140): Train loss 2.565, Val loss 3.113\n",
            "Ep 2 (Step 013145): Train loss 2.591, Val loss 3.112\n",
            "Ep 2 (Step 013150): Train loss 2.299, Val loss 3.111\n",
            "Ep 2 (Step 013155): Train loss 2.427, Val loss 3.114\n",
            "Ep 2 (Step 013160): Train loss 2.454, Val loss 3.114\n",
            "Ep 2 (Step 013165): Train loss 2.565, Val loss 3.112\n",
            "Ep 2 (Step 013170): Train loss 2.419, Val loss 3.110\n",
            "Ep 2 (Step 013175): Train loss 2.409, Val loss 3.116\n",
            "Ep 2 (Step 013180): Train loss 2.529, Val loss 3.124\n",
            "Ep 2 (Step 013185): Train loss 2.286, Val loss 3.132\n",
            "Ep 2 (Step 013190): Train loss 2.411, Val loss 3.126\n",
            "Ep 2 (Step 013195): Train loss 2.701, Val loss 3.123\n",
            "Ep 2 (Step 013200): Train loss 2.356, Val loss 3.122\n",
            "Ep 2 (Step 013205): Train loss 2.471, Val loss 3.119\n",
            "Ep 2 (Step 013210): Train loss 2.377, Val loss 3.122\n",
            "Ep 2 (Step 013215): Train loss 2.627, Val loss 3.127\n",
            "Ep 2 (Step 013220): Train loss 2.528, Val loss 3.116\n",
            "Ep 2 (Step 013225): Train loss 2.370, Val loss 3.110\n",
            "Ep 2 (Step 013230): Train loss 2.270, Val loss 3.109\n",
            "Ep 2 (Step 013235): Train loss 2.375, Val loss 3.116\n",
            "Ep 2 (Step 013240): Train loss 2.214, Val loss 3.123\n",
            "Ep 2 (Step 013245): Train loss 2.450, Val loss 3.120\n",
            "Ep 2 (Step 013250): Train loss 2.558, Val loss 3.124\n",
            "Ep 2 (Step 013255): Train loss 2.355, Val loss 3.124\n",
            "Ep 2 (Step 013260): Train loss 2.429, Val loss 3.120\n",
            "Ep 2 (Step 013265): Train loss 2.614, Val loss 3.123\n",
            "Ep 2 (Step 013270): Train loss 2.453, Val loss 3.130\n",
            "Ep 2 (Step 013275): Train loss 2.412, Val loss 3.118\n",
            "Ep 2 (Step 013280): Train loss 2.493, Val loss 3.110\n",
            "Ep 2 (Step 013285): Train loss 2.375, Val loss 3.114\n",
            "Ep 2 (Step 013290): Train loss 2.602, Val loss 3.123\n",
            "Ep 2 (Step 013295): Train loss 2.409, Val loss 3.126\n",
            "Ep 2 (Step 013300): Train loss 2.558, Val loss 3.118\n",
            "Ep 2 (Step 013305): Train loss 2.409, Val loss 3.113\n",
            "Ep 2 (Step 013310): Train loss 2.383, Val loss 3.115\n",
            "Ep 2 (Step 013315): Train loss 2.393, Val loss 3.121\n",
            "Ep 2 (Step 013320): Train loss 2.416, Val loss 3.115\n",
            "Ep 2 (Step 013325): Train loss 2.600, Val loss 3.115\n",
            "Ep 2 (Step 013330): Train loss 2.294, Val loss 3.111\n",
            "Ep 2 (Step 013335): Train loss 2.536, Val loss 3.109\n",
            "Ep 2 (Step 013340): Train loss 2.335, Val loss 3.111\n",
            "Ep 2 (Step 013345): Train loss 2.517, Val loss 3.113\n",
            "Ep 2 (Step 013350): Train loss 2.420, Val loss 3.110\n",
            "Ep 2 (Step 013355): Train loss 2.466, Val loss 3.108\n",
            "Ep 2 (Step 013360): Train loss 2.817, Val loss 3.114\n",
            "Ep 2 (Step 013365): Train loss 2.572, Val loss 3.121\n",
            "Ep 2 (Step 013370): Train loss 2.527, Val loss 3.122\n",
            "Ep 2 (Step 013375): Train loss 2.457, Val loss 3.115\n",
            "Ep 2 (Step 013380): Train loss 2.226, Val loss 3.108\n",
            "Ep 2 (Step 013385): Train loss 2.477, Val loss 3.106\n",
            "Ep 2 (Step 013390): Train loss 2.542, Val loss 3.106\n",
            "Ep 2 (Step 013395): Train loss 2.535, Val loss 3.112\n",
            "Ep 2 (Step 013400): Train loss 2.463, Val loss 3.115\n",
            "Ep 2 (Step 013405): Train loss 2.372, Val loss 3.120\n",
            "Ep 2 (Step 013410): Train loss 2.429, Val loss 3.118\n",
            "Ep 2 (Step 013415): Train loss 2.473, Val loss 3.116\n",
            "Ep 2 (Step 013420): Train loss 2.387, Val loss 3.108\n",
            "Ep 2 (Step 013425): Train loss 2.503, Val loss 3.106\n",
            "Ep 2 (Step 013430): Train loss 2.389, Val loss 3.105\n",
            "Ep 2 (Step 013435): Train loss 2.466, Val loss 3.110\n",
            "Ep 2 (Step 013440): Train loss 2.239, Val loss 3.110\n",
            "Ep 2 (Step 013445): Train loss 2.351, Val loss 3.105\n",
            "Ep 2 (Step 013450): Train loss 2.442, Val loss 3.109\n",
            "Ep 2 (Step 013455): Train loss 2.377, Val loss 3.108\n",
            "Ep 2 (Step 013460): Train loss 2.456, Val loss 3.107\n",
            "Ep 2 (Step 013465): Train loss 2.496, Val loss 3.107\n",
            "Ep 2 (Step 013470): Train loss 2.543, Val loss 3.112\n",
            "Ep 2 (Step 013475): Train loss 2.303, Val loss 3.115\n",
            "Ep 2 (Step 013480): Train loss 2.483, Val loss 3.123\n",
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस । # # # प्रतिक्रिया : थप रूपमा, इनपुटमा जनावरको सबैभन्दा प्रभावशाली विशेषताहरू मध्ये <|unk|>सूचीबद्ध गर्नुहोस् # # # इनपुट : जनावरः पेन्गुइन # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
            "Training completed in 156.06 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ise3wGjlB-iq",
      "metadata": {
        "id": "Ise3wGjlB-iq"
      },
      "source": [
        "- As we can see based on the outputs above, the model trains well, as we can tell based on the decreasing training loss and validation loss values\n",
        "- Furthermore, based on the response text printed after each epoch, we can see that the model correctly follows the instruction to convert the input sentence `'The chef cooks the meal every day.'` into passive voice `'The meal is cooked every day by the chef.'` (We will properly format and evaluate the responses in a later section)\n",
        "- Finally, let's take a look at the training and validation loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
        "outputId": "9235d664-1438-44e3-c41b-8b43122fb944"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "Path.mkdir(Path('losses/'), exist_ok=True)\n",
        "\n",
        "from previous_chapters import plot_losses\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, Path('losses/'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_yQ208rZXBO8"
      },
      "id": "_yQ208rZXBO8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0",
      "metadata": {
        "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0"
      },
      "source": [
        "- As we can see, the loss decreases sharply at the beginning of the first epoch, which means the model starts learning quickly\n",
        "- We can see that slight overfitting sets in at around 1 training epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3",
      "metadata": {
        "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3"
      },
      "source": [
        "## 7.7 Extracting and saving responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49",
      "metadata": {
        "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-6.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427",
      "metadata": {
        "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427"
      },
      "source": [
        "- In this section, we save the test set responses for scoring in the next section\n",
        "- We also save a copy of the model for future use\n",
        "- But first, let's take a brief look at the responses generated by the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50000,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-key-value bias\n",
        "}"
      ],
      "metadata": {
        "id": "fMeP3fGxXqmR"
      },
      "id": "fMeP3fGxXqmR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpt_model_code2.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-yV_xfYapOl",
        "outputId": "3cb1691b-9f16-442d-945c-2e4c56b41fc8"
      },
      "id": "B-yV_xfYapOl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gpt_model_code2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VQ2NZMbfucAc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2NZMbfucAc",
        "outputId": "4c78b271-6be5-4711-b732-81141fce6cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "दिइएको अनुच्छेदमा शीर्ष 5 सबैभन्दा बारम्बार क्रियाहरूको सूची उत्पन्न गर्नुहोस्।\n",
            "\n",
            "### इनपुट:\n",
            "ज्याकेन पार्कको वरिपरि दुई पटक हिँडे र त्यसपछि तालको वरिपरि दौडे। साम उनको पछाडि हिँडे।\n",
            "\n",
            "Correct response:\n",
            ">> 1. हिँडे।\n",
            "\n",
            "2. रेस\n",
            "\n",
            "3. जोगिङ\n",
            "\n",
            "4. वरपर\n",
            "\n",
            "5। पछाडि\n",
            "\n",
            "Model response:\n",
            ">> पछाडि हिँडे । उनले तालको छेउमा हिंडिन् र ताल गहिरो छ कि भनेर जाँच गर्न थालिन् ।\n",
            "-------------------------------------\n",
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "दुई वाक्यहरू बिच उपयुक्त संक्रमणहरू प्रदान गर्न अनुच्छेदमा वाक्य घुसाउनुहोस्।\n",
            "\n",
            "### इनपुट:\n",
            "डेभले चाँडै आफ्नो मन परिवर्तन गरे। उनले चलचित्र जाने निर्णय गरे।\n",
            "\n",
            "Correct response:\n",
            ">> डेभले चाँडै आफ्नो मन परिवर्तन गरे। आफूले योजना गरेझैँ बस्नुको सट्टा, उनले चलचित्रहरूमा जाने निर्णय गरे।\n",
            "\n",
            "Model response:\n",
            ">> णय गरे ।\n",
            "-------------------------------------\n",
            "तल दियीएको निर्देशन को उचित प्रतिक्रिया दिनुहोस।\n",
            "\n",
            "### प्रतिक्रिया:\n",
            "खाली ठाउँ भर्न एउटा शब्द दिनुहोस्\n",
            "\n",
            "### इनपुट:\n",
            "समाचार सुनेपछि उनी _ _ _ _ _ थिए।\n",
            "\n",
            "Correct response:\n",
            ">> विनाशकारी\n",
            "\n",
            "Model response:\n",
            ">> थिए । प्रतिक्रिया\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "from previous_chapters import text_to_token_ids\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    # token_ids = generate(\n",
        "    #     model=model,\n",
        "    #     idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "    #     max_new_tokens=256,\n",
        "    #     context_size=BASE_CONFIG[\"context_length\"],\n",
        "    #     eos_id=50256\n",
        "    # )\n",
        "    # generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    generated_text = generate(model=model, prompt=input_text, tokenizer=tokenizer,\n",
        "         max_new_tokens=256, temperature=0.7, top_k=50, top_p=None,  # New parameter for nucleus sampling\n",
        "         eos_id=None, repetition_penalty=1.2, penalize_len_below=50)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ab64c1-586f-4939-8def-23feeb1b3599",
      "metadata": {
        "id": "49ab64c1-586f-4939-8def-23feeb1b3599"
      },
      "source": [
        "- As we can see based on the test set instructions, given responses, and the model's responses, the model performs relatively well\n",
        "- The answers to the first and last instructions are clearly correct\n",
        "- The second answer is close; the model answers with \"cumulus cloud\" instead of \"cumulonimbus\" (however, note that cumulus clouds can develop into cumulonimbus clouds, which are capable of producing thunderstorms)\n",
        "- Most importantly, we can see that model evaluation is not as straightforward as in the previous chapter, where we just had to calculate the percentage of correct spam/non-spam class labels to obtain the classification accuracy\n",
        "- In practice, instruction-finetuned LLMs such as chatbots are evaluated via multiple approaches\n",
        "  - short-answer and multiple choice benchmarks such as MMLU (\"Measuring Massive Multitask Language Understanding\", [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)), which test the knowledge of a model\n",
        "  - human preference comparison to other LLMs, such as LMSYS chatbot arena ([https://arena.lmsys.org](https://arena.lmsys.org))\n",
        "  - automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/))\n",
        "\n",
        "- In the next section, we will use an approach similar to AlpacaEval and use another LLM to evaluate the responses of our model; however, we will use our own test set instead of using a publicly available benchmark dataset\n",
        "- For this, we add the model response to the `test_data` dictionary and save it as a `\"instruction-data-with-response.json\"` file for record-keeping so that we can load and analyze it in separate Python sessions if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-PNGKzY4snKP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "-PNGKzY4snKP",
        "outputId": "f659711c-8e04-4e44-98cc-a6a6c7fb5bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 804/6348 [01:25<09:50,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (974) must match the existing size (512) at non-singleton dimension 3.  Target sizes: [1, 12, 974, 974].  Tensor sizes: [512, 512]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-7e9f58ada434>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     generated_text = generate(model=model, prompt=input_text, tokenizer=tokenizer, \n\u001b[0m\u001b[1;32m      7\u001b[0m          \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# New parameter for nucleus sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          eos_id=None, repetition_penalty=1.2, penalize_len_below=50)\n",
            "\u001b[0;32m/content/drive/MyDrive/Research/GPT2-Fine-Tuning/gpt_model_code2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, prompt, tokenizer, max_new_tokens, temperature, top_k, top_p, eos_id, repetition_penalty, penalize_len_below)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0midx_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Research/GPT2-Fine-Tuning/gpt_model_code.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Research/GPT2-Fine-Tuning/gpt_model_code.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_shortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshortcut\u001b[0m  \u001b[0;31m# Add the original input back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Research/GPT2-Fine-Tuning/gpt_model_code.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Use the mask to fill attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mattn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_bool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (974) must match the existing size (512) at non-singleton dimension 3.  Target sizes: [1, 12, 974, 974].  Tensor sizes: [512, 512]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "    generated_text = generate(model=model, prompt=input_text, tokenizer=tokenizer,\n",
        "         max_new_tokens=256, temperature=0.7, top_k=50, top_p=None,  # New parameter for nucleus sampling\n",
        "         eos_id=None, repetition_penalty=1.2, penalize_len_below=50)\n",
        "    # token_ids = generate(\n",
        "    #     model=model,\n",
        "    #     idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "    #     max_new_tokens=256,\n",
        "    #     context_size=BASE_CONFIG[\"context_length\"],\n",
        "    #     eos_id=50256\n",
        "    # )\n",
        "    # generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228d6fa7-d162-44c3-bef1-4013c027b155",
      "metadata": {
        "id": "228d6fa7-d162-44c3-bef1-4013c027b155"
      },
      "source": [
        "- Let's double-check one of the entries to see whether the responses have been added to the `test_data` dictionary correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u-AvCCMTnPSE",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u-AvCCMTnPSE"
      },
      "outputs": [],
      "source": [
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9F7iiFhxE_ny",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9F7iiFhxE_ny"
      },
      "outputs": [],
      "source": [
        "print('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
      "metadata": {
        "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
      },
      "source": [
        "- Finally, we also save the model in case we want to reuse it in the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cBU0iHmVfOI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cBU0iHmVfOI",
        "scrolled": true,
        "outputId": "4789c59b-81eb-4779-9a9b-86cf9ce71b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-fine_tuned-sft.pth\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-fine_tuned\"\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "obgoGI89dgPm",
      "metadata": {
        "id": "obgoGI89dgPm"
      },
      "source": [
        "## 7.8 Evaluating the finetuned LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805b9d30-7336-499f-abb5-4a21be3129f5",
      "metadata": {
        "id": "805b9d30-7336-499f-abb5-4a21be3129f5"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-7.webp?1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1",
      "metadata": {
        "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1"
      },
      "source": [
        "- In this section, we automate the response evaluation of the finetuned LLM using another, larger LLM\n",
        "- In particular, we use an instruction-finetuned 8-billion-parameter Llama 3 model by Meta AI that can be run locally via ollama ([https://ollama.com](https://ollama.com))\n",
        "- (Alternatively, if you prefer using a more capable LLM like GPT-4 via the OpenAI API, please see the [llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb) notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9",
      "metadata": {
        "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9"
      },
      "source": [
        "- Ollama is an application to run LLMs efficiently\n",
        "- It is a wrapper around llama.cpp ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)), which implements LLMs in pure C/C++ to maximize efficiency\n",
        "- Note that it is a tool for using LLMs to generate text (inference), not training or finetuning LLMs\n",
        "- Before running the code below, install ollama by visiting [https://ollama.com](https://ollama.com) and following the instructions (for instance, clicking on the \"Download\" button and downloading the ollama application for your operating system)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822",
      "metadata": {
        "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822"
      },
      "source": [
        "- For macOS and Windows users, click on the ollama application you downloaded; if it prompts you to install the command line usage, say \"yes\"\n",
        "- Linux users can use the installation command provided on the ollama website\n",
        "\n",
        "- In general, before we can use ollama from the command line, we have to either start the ollama application or run `ollama serve` in a separate terminal\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ollama-run.webp?1\" width=700px>\n",
        "\n",
        "\n",
        "- With the ollama application or `ollama serve` running in a different terminal, on the command line, execute the following command to try out the 8-billion-parameter Llama 3 model (the model, which takes up 4.7 GB of storage space, will be automatically downloaded the first time you execute this command)\n",
        "\n",
        "```bash\n",
        "# 8B model\n",
        "ollama run llama3\n",
        "```\n",
        "\n",
        "\n",
        "The output looks like as follows\n",
        "\n",
        "```\n",
        "$ ollama run llama3\n",
        "pulling manifest\n",
        "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
        "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
        "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
        "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
        "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
        "verifying sha256 digest\n",
        "writing manifest\n",
        "removing any unused layers\n",
        "success\n",
        "```\n",
        "\n",
        "- Note that `llama3` refers to the instruction finetuned 8-billion-parameter Llama 3 model\n",
        "\n",
        "- Using ollama with the `\"llama3\"` model (a 8B parameter model) requires 16 GB of RAM; if this is not supported by your machine, you can try the smaller model, such as the 3.8B parameter phi-3 model by setting `model = \"phi-3\"`, which only requires 8 GB of RAM\n",
        "\n",
        "- Alternatively, you can also use the larger 70-billion-parameter Llama 3 model, if your machine supports it, by replacing `llama3` with `llama3:70b`\n",
        "\n",
        "- After the download has been completed, you will see a command line prompt that allows you to chat with the model\n",
        "\n",
        "- Try a prompt like \"What do llamas eat?\", which should return an output similar to the following\n",
        "\n",
        "```\n",
        ">>> What do llamas eat?\n",
        "Llamas are ruminant animals, which means they have a four-chambered\n",
        "stomach and eat plants that are high in fiber. In the wild, llamas\n",
        "typically feed on:\n",
        "1. Grasses: They love to graze on various types of grasses, including tall\n",
        "grasses, wheat, oats, and barley.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4",
      "metadata": {
        "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4"
      },
      "source": [
        "- You can end this session using the input `/bye`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3",
      "metadata": {
        "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3"
      },
      "source": [
        "- The following code checks whether the ollama session is running correctly before proceeding to use ollama to evaluate the test set responses we generated in the previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026e8570-071e-48a2-aa38-64d7be35f288",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "026e8570-071e-48a2-aa38-64d7be35f288"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0"
      },
      "outputs": [],
      "source": [
        "# This cell is optional; it allows you to restart the notebook\n",
        "# and only run section 7.7 without rerunning any of the previous code\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_path = \"instruction-data-with-response.json\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    test_data = json.load(file)\n",
        "\n",
        "\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3464705-d026-4594-977f-fb357e51c3a9",
      "metadata": {
        "id": "b3464705-d026-4594-977f-fb357e51c3a9"
      },
      "source": [
        "- Now, an alternative way to the `ollama run` command we used earlier to interact with the model is via its REST API in Python via the following function\n",
        "- Before you run the next cells in this notebook, make sure that ollama is still running (the previous code cells should print `\"Ollama running: True\"`)\n",
        "- Next, run the following code cell to query the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "def query_model(\n",
        "    prompt,\n",
        "    model=\"llama3\",\n",
        "    url=\"http://localhost:11434/api/chat\"\n",
        "):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data\n",
        "\n",
        "\n",
        "model = \"llama3\"\n",
        "result = query_model(\"What do Llamas eat?\", model)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc",
      "metadata": {
        "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc"
      },
      "source": [
        "- Now, using the `query_model` function we defined above, we can evaluate the responses of our finetuned model; let's try it out on the first 3 test set responses we looked at in a previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "86b839d4-064d-4178-b2d7-01691b452e5e"
      },
      "outputs": [],
      "source": [
        "for entry in test_data[:3]:\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}`\"\n",
        "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "    )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", query_model(prompt))\n",
        "    print(\"\\n-------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3",
      "metadata": {
        "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3"
      },
      "source": [
        "- As we can see, the Llama 3 model provides a reasonable evaluation and also gives partial points if a model is not entirely correct, as we can see based on the \"cumulus cloud\" answer\n",
        "- Note that the previous prompt returns very verbose evaluations; we can tweak the prompt to generate integer responses in the range between 0 and 100 (where 100 is best) to calculate an average score for our model\n",
        "- The evaluation of the 110 entries in the test set takes about 1 minute on an M3 MacBook Air laptop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb"
      },
      "outputs": [],
      "source": [
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "scores = generate_model_scores(test_data, \"model_response\")\n",
        "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2",
      "metadata": {
        "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2"
      },
      "source": [
        "- Our model achieves an average score of above 50, which we can use as a reference point to compare the model to other models or to try out other training settings that may improve the model\n",
        "- Note that ollama is not fully deterministic across operating systems (as of this writing), so the numbers you are getting might slightly differ from the ones shown above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94",
      "metadata": {
        "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94"
      },
      "source": [
        "- For reference, the original\n",
        "  - Llama 3 8B base model achieves a score of 58.51\n",
        "  - Llama 3 8B instruct model achieves a score of 82.65"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412d7325-284a-446c-92a1-5aa8acc52dee",
      "metadata": {
        "id": "412d7325-284a-446c-92a1-5aa8acc52dee"
      },
      "source": [
        "## 7.9 Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tIbNMluCDjVM",
      "metadata": {
        "id": "tIbNMluCDjVM"
      },
      "source": [
        "### 7.9.1 What's next\n",
        "\n",
        "- This marks the final chapter of this book\n",
        "- We covered the major steps of the LLM development cycle: implementing an LLM architecture, pretraining an LLM, and finetuning it\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/final-overview.webp?1\" width=500px>\n",
        "\n",
        "- An optional step that is sometimes followed after instruction finetuning, as described in this chapter, is preference finetuning\n",
        "- Preference finetuning process can be particularly useful for customizing a model to better align with specific user preferences; see the [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) folder if you are interested in this\n",
        "\n",
        "- This GitHub repository also contains a large selection of additional bonus material you may enjoy; for more information, please see the [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) section on this repository's README page\n",
        "\n",
        "### 7.9.2 Staying up to date in a fast-moving field\n",
        "\n",
        "- No code in this section\n",
        "\n",
        "### 7.9.3 Final words\n",
        "\n",
        "- I hope you enjoyed this journey of implementing an LLM from the ground up and coding the pretraining and finetuning functions\n",
        "- In my opinion, implementing an LLM from scratch is the best way to understand how LLMs work; I hope you gained a better understanding through this approach\n",
        "- While this book serves educational purposes, you may be interested in using different and more powerful LLMs for real-world applications\n",
        "  - For this, you may consider popular tools such as axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)) or LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)), which I help developing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9853e7f-a81a-4806-9728-be1690807185",
      "metadata": {
        "id": "f9853e7f-a81a-4806-9728-be1690807185"
      },
      "source": [
        "## Summary and takeaways\n",
        "\n",
        "- See the [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) script, a self-contained script for classification finetuning\n",
        "- [./ollama_evaluate.py](./ollama_evaluate.py) is a standalone script based on section 7.8 that evaluates a JSON file containing \"output\" and \"response\" keys via Ollama and Llama 3\n",
        "- The [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) notebook illustrates how to load the finetuned model in a new session\n",
        "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9cc51ec-e06c-4470-b626-48401a037851",
      "metadata": {
        "id": "b9cc51ec-e06c-4470-b626-48401a037851"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "- Congrats on completing the book; in case you are looking for additional resources, I added several bonus sections to this GitHub repository that you might find interesting\n",
        "- The complete list of bonus materials can be viewed in the main README's [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) section\n",
        "- To highlight a few of my favorites:\n",
        "  1. [Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) implements a popular preference tuning mechanism to align the model from this chapter more closely with human preferences\n",
        "  2. [Llama 3.2 From Scratch (A Standalone Notebook)](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb), a from-scratch implementation of Meta AI's popular Llama 3.2, including loading the official pretrained weights; if you are up to some additional experiments, you can replace the `GPTModel` model in each of the chapters with the `Llama3Model` class (it should work as a 1:1 replacement)\n",
        "  3. [Converting GPT to Llama](../../ch05/07_gpt_to_llama) contains code with step-by-step guides that explain the differences between GPT-2 and the various Llama models\n",
        "  4. [Understanding the Difference Between Embedding Layers and Linear Layers](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) is a conceptual explanation illustrating that the `Embedding` layer in PyTorch, which we use at the input stage of an LLM, is mathematically equivalent to a linear layer applied to one-hot encoded data\n",
        "- Happy further reading!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LFCX0UwNg8qW",
      "metadata": {
        "id": "LFCX0UwNg8qW"
      },
      "source": [
        "# Done\n",
        "- [X] Remove english dataset from wiseyak by (using langid)\n",
        "\n",
        "# Todo\n",
        "\n",
        "- [ ] More data\n",
        "- [ ] more epochs\n",
        "- [ ] save to hub\n",
        "- [ ] Assistant fine tuning"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "046c20ee19054545a7da0c2155ccc655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115593a1493e4ff2877a71ba027b9153",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6414a951d7945499da92576e2dc08d5",
            "value": 0
          }
        },
        "0c5fe1e2c485414e86005eebbb44b770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c076897308c403081c8a793dee2a419",
            "max": 113751,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2369a38eb6444b49b0f5bf0188e8249d",
            "value": 113751
          }
        },
        "0cc80e439e99440a9f837768be4d6e28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f68fdc697a24efd8247482209190015": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115593a1493e4ff2877a71ba027b9153": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "12aa86ab7ccd4d04ae05145cee2ace25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edd8c72bc25b42059f134ecc3b302c44",
              "IPY_MODEL_fc72e11e7fc740e8a8bcd325224eb1f7",
              "IPY_MODEL_c6f4679bc08d48b281a4a49fa557f290"
            ],
            "layout": "IPY_MODEL_6b76b45fbb3e4abdb271e83429d80071"
          }
        },
        "16a535ce3a0540d38427b848d7d9f724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21875bc5b7f4479bb95dc7589e5906ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2369a38eb6444b49b0f5bf0188e8249d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2762d9882bc445bf92c15dce0b3bbb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b99e8a7c64462dbe6503f542691718",
              "IPY_MODEL_0c5fe1e2c485414e86005eebbb44b770",
              "IPY_MODEL_9d7e756e12a748ea98de28870300857d"
            ],
            "layout": "IPY_MODEL_e59c6b8df3464660ac383523e08b9f91"
          }
        },
        "28b6460138ab483f8739217089037bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6e39930e854ebe857f6854c3ac91cf",
            "placeholder": "​",
            "style": "IPY_MODEL_a0de7080f8d84025a8c1c6c8f80f4f13",
            "value": " 661M/661M [00:10&lt;00:00, 54.7MB/s]"
          }
        },
        "2e1751fb411e4b67a66afb7cca604f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "381ee4c0cc554816a066d3ee3c8d21ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "382e407c5e2640b0bcfa597727508c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87be4eec65da4e009c2061f847ad6da1",
            "max": 71364603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc5ee43429894db1ae3afbc426368ffc",
            "value": 71364603
          }
        },
        "3c03289a162f4665ad9da7eed76a5629": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43435e825a3542a5a1967bd2fa771b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467484aec7e3487aa03c28bea096c903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf3ceb14eeb4e27856fe79c103332ae",
            "placeholder": "​",
            "style": "IPY_MODEL_809231b736d4417e941ab684f8f483cc",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "4708e90d116748b2ada25c038626aeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5951e69c91f94ca38074e88b70b6022c",
            "placeholder": "​",
            "style": "IPY_MODEL_5522f6e20e704a24a63c8b2fe57cb111",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "4f6ec3d692f943e4843b01c93d3d85b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5522f6e20e704a24a63c8b2fe57cb111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5951e69c91f94ca38074e88b70b6022c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b7603d166747a6b4ec5c8b2663615f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5a7d47d2c484afda438f4d30e384353",
            "placeholder": "​",
            "style": "IPY_MODEL_e6d5b704defd43109d8e504d405ec770",
            "value": "config.json: 100%"
          }
        },
        "5dd7c59ad9d0400ca307f0a94bdd5054": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f52e57167237411683e53f80b7111d37",
            "max": 661486448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f6ec3d692f943e4843b01c93d3d85b4",
            "value": 661486448
          }
        },
        "6b76b45fbb3e4abdb271e83429d80071": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e3df1690e94ef8a40e10c5ddbf55a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75448f76e86a417e85b2fa7f40c6589f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d252bea60e47beba7a19f5e73b7bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_f8f4054069e04f21aeb76583fa985807",
            "value": " 71.4M/71.4M [00:01&lt;00:00, 59.8MB/s]"
          }
        },
        "7840bb35972d49d7aabecc3679310a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c03289a162f4665ad9da7eed76a5629",
            "max": 196,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e000427fcdbc46fa9c2f8ec9765f6a24",
            "value": 196
          }
        },
        "78dd1bf6d63344a294395ece64b58523": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5a4d219898c420e89ba131c913690e4",
              "IPY_MODEL_046c20ee19054545a7da0c2155ccc655",
              "IPY_MODEL_4708e90d116748b2ada25c038626aeea"
            ],
            "layout": "IPY_MODEL_43435e825a3542a5a1967bd2fa771b5a"
          }
        },
        "809231b736d4417e941ab684f8f483cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83e7f4df09c947bba4651a57e245caec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59b7603d166747a6b4ec5c8b2663615f",
              "IPY_MODEL_7840bb35972d49d7aabecc3679310a39",
              "IPY_MODEL_dbefb28230734a9396970295b8aa40c1"
            ],
            "layout": "IPY_MODEL_16a535ce3a0540d38427b848d7d9f724"
          }
        },
        "8561f0070e304dc5b8956829f664a329": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87be4eec65da4e009c2061f847ad6da1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902d74e63c904d409d9d38177cd98611": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0dd48dd4af84d108816da0f859a9437",
              "IPY_MODEL_5dd7c59ad9d0400ca307f0a94bdd5054",
              "IPY_MODEL_28b6460138ab483f8739217089037bf3"
            ],
            "layout": "IPY_MODEL_71e3df1690e94ef8a40e10c5ddbf55a7"
          }
        },
        "91351da4c2b54246aed65121da1f253f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c076897308c403081c8a793dee2a419": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d7e756e12a748ea98de28870300857d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21875bc5b7f4479bb95dc7589e5906ff",
            "placeholder": "​",
            "style": "IPY_MODEL_91351da4c2b54246aed65121da1f253f",
            "value": " 113751/113751 [00:01&lt;00:00, 93465.13 examples/s]"
          }
        },
        "a0de7080f8d84025a8c1c6c8f80f4f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a25c7dc72fda44a5b26fd585d516eee4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c6e024e4e84ab6b923b17dd49f2063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b40bef8d56fa413aa805fb7f16e53232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d252bea60e47beba7a19f5e73b7bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a4d219898c420e89ba131c913690e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a25c7dc72fda44a5b26fd585d516eee4",
            "placeholder": "​",
            "style": "IPY_MODEL_d7bb83a9396e4c9f8642c6b845764966",
            "value": ""
          }
        },
        "c5a7d47d2c484afda438f4d30e384353": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6414a951d7945499da92576e2dc08d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6f4679bc08d48b281a4a49fa557f290": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e430de5ac2b2480a8a33fabec7149caa",
            "placeholder": "​",
            "style": "IPY_MODEL_ffff2e5683624bf1a29ce248544662b4",
            "value": " 744/744 [00:00&lt;00:00, 15.5kB/s]"
          }
        },
        "ca3d854bc60b44fb98916cfba03ff82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdf3ceb14eeb4e27856fe79c103332ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ac73bd164940d6aa6f1fbf7c7841b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7bb83a9396e4c9f8642c6b845764966": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9b99e8a7c64462dbe6503f542691718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ac73bd164940d6aa6f1fbf7c7841b4",
            "placeholder": "​",
            "style": "IPY_MODEL_2e1751fb411e4b67a66afb7cca604f16",
            "value": "Generating train split: 100%"
          }
        },
        "dbefb28230734a9396970295b8aa40c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f68fdc697a24efd8247482209190015",
            "placeholder": "​",
            "style": "IPY_MODEL_381ee4c0cc554816a066d3ee3c8d21ea",
            "value": " 196/196 [00:00&lt;00:00, 18.7kB/s]"
          }
        },
        "e000427fcdbc46fa9c2f8ec9765f6a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e430de5ac2b2480a8a33fabec7149caa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59c6b8df3464660ac383523e08b9f91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d5b704defd43109d8e504d405ec770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb6e39930e854ebe857f6854c3ac91cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd8c72bc25b42059f134ecc3b302c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b40bef8d56fa413aa805fb7f16e53232",
            "placeholder": "​",
            "style": "IPY_MODEL_a4c6e024e4e84ab6b923b17dd49f2063",
            "value": "README.md: 100%"
          }
        },
        "f070fd4abb6c417799af732624467359": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0dd48dd4af84d108816da0f859a9437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ed7c352b2c46c4ae7fcc9c4912a9c5",
            "placeholder": "​",
            "style": "IPY_MODEL_ca3d854bc60b44fb98916cfba03ff82e",
            "value": "model.safetensors: 100%"
          }
        },
        "f4ed7c352b2c46c4ae7fcc9c4912a9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52e57167237411683e53f80b7111d37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70b9835b1fd42a2837cf76c894661bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_467484aec7e3487aa03c28bea096c903",
              "IPY_MODEL_382e407c5e2640b0bcfa597727508c4d",
              "IPY_MODEL_75448f76e86a417e85b2fa7f40c6589f"
            ],
            "layout": "IPY_MODEL_8561f0070e304dc5b8956829f664a329"
          }
        },
        "f8f4054069e04f21aeb76583fa985807": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc5ee43429894db1ae3afbc426368ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc72e11e7fc740e8a8bcd325224eb1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cc80e439e99440a9f837768be4d6e28",
            "max": 744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f070fd4abb6c417799af732624467359",
            "value": 744
          }
        },
        "ffff2e5683624bf1a29ce248544662b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}