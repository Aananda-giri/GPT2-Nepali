{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-","timestamp":1706336439820}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Building a GPT\n","\n","* Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n","* [notebook. original](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n","* [code. Freecodecamp ](https://github.com/Infatoshi/fcc-intro-to-llms/tree/main)"],"metadata":{"id":"wJpXpmjEYC_T"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-fNmLTylif3","executionInfo":{"status":"ok","timestamp":1723349922339,"user_tz":-345,"elapsed":24917,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"64af5112-a03b-46a3-b76f-884e729563d2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/Research/Models/\"gpt (working-np) (zero-to-hero)\"\n","%cd \"/content/drive/MyDrive/Research/Models/gpt (working-np) (zero-to-hero)\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2l8RzigHlkzJ","executionInfo":{"status":"ok","timestamp":1723349943089,"user_tz":-345,"elapsed":659,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"26d38a90-1f3d-4e76-8779-6ea9f54ef8dc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["gpt-dev-eng.ipynb  gpt-dev-np.ipynb  input.txt\tmodel-01.pkl  model-np-02.pkl  model-np.pkl\n","/content/drive/MyDrive/Research/Models/gpt (working-np) (zero-to-hero)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5hjCcLDr2WC","executionInfo":{"status":"ok","timestamp":1706339344888,"user_tz":-345,"elapsed":731,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"467f1fd5-fb79-4c1e-93c1-8574274e36d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-01-27 07:09:04--  https://drive.google.com/file/d/1d7qEqBd2YO98CcfcdTcFq-kDcHgm0eg0/view\n","Resolving drive.google.com (drive.google.com)... 74.125.135.102, 74.125.135.139, 74.125.135.100, ...\n","Connecting to drive.google.com (drive.google.com)|74.125.135.102|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘nepali_dataset.csv’\n","\n","nepali_dataset.csv      [ <=>                ]  83.77K  --.-KB/s    in 0.04s   \n","\n","2024-01-27 07:09:04 (2.27 MB/s) - ‘nepali_dataset.csv’ saved [85777]\n","\n"]}],"source":["# Using old version of dataset\n","# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","# # !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","\n","# !wget https://drive.google.com/file/d/1d7qEqBd2YO98CcfcdTcFq-kDcHgm0eg0/view -O nepali_dataset.csv\n","\n","# import csv\n","\n","# paragraphs = []\n","\n","# with open('crawled_nepali_news_dataset.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","#     header = next(reader) # Skip the header row\n","#     for n, row in enumerate(reader):\n","#         paragraph, parent_url, page_title, is_nepali_confidence = row\n","#         paragraphs.append(paragraph)\n","#         if n>10000: # Take first 1000 paragraphs\n","#           break\n","\n","# text = \" \".join(paragraphs)\n","# print(text[:1000])\n","\n"]},{"cell_type":"code","source":["# Using new version of cleaned dataset\n","# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","# !pip install -q gdown\n","# !gdown 1-MAKxOxrJh_-LEa9_xrcoP3LJo9hvjAW -O cleaned_data.csv\n","\n","import csv\n","\n","paragraphs = []\n","\n","with open('cleaned_data.csv', 'r') as csvfile:\n","    reader = csv.reader(csvfile)\n","    header = next(reader) # Skip the header row\n","    for n, row in enumerate(reader):\n","        parent_url, page_title, new_paragraphs = row\n","        paragraphs.extend(new_paragraphs)\n","        if n>10000: # Take first 1000 paragraphs\n","          break\n","\n","text = \" \".join(paragraphs)\n","print(text[:1000])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AI83a_1swFy9","executionInfo":{"status":"ok","timestamp":1723350102974,"user_tz":-345,"elapsed":6344,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"21f017c0-1fe7-4e5b-9122-8eb849a2c247"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-MAKxOxrJh_-LEa9_xrcoP3LJo9hvjAW\n","To: /content/drive/MyDrive/Research/Models/gpt (working-np) (zero-to-hero)/cleaned_data.csv\n","\r  0% 0.00/34.0 [00:00<?, ?B/s]\r100% 34.0/34.0 [00:00<00:00, 136kB/s]\n"]}]},{"cell_type":"code","source":["# # read it in to inspect it\n","# with open('input.txt', 'r', encoding='utf-8') as f:\n","#     text = f.read()"],"metadata":{"id":"O6medjfRsLD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xWI_VyAsN8F","executionInfo":{"status":"ok","timestamp":1706340469526,"user_tz":-345,"elapsed":6,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"e2ad500f-b9b7-4239-c68a-65036b3bbe31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  1635559\n"]}]},{"cell_type":"code","source":["# let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c5V0FvqseE0","executionInfo":{"status":"ok","timestamp":1706340472511,"user_tz":-345,"elapsed":6,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"77eb4e4b-392a-4778-9f11-2d16aec69687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["प्रदेश सरकार र निजी क्षेत्रको सहकार्यमा पहाडी तथा हिमाली क्षेत्रमा मनोरञ्जनात्मक तथा साहसिक पर्यटनको लागि हिलस्टेशनहरू विकास गर्न आवश्यक छ भन्दै उनले सांस्कृतिक, धार्मिक, साहसिक, कृषि, स्वास्थ्य तथा खेल पर्यटक आकर्षित गर्दै यस क्षेत्रको मौलिक संस्कृति संरक्षणमा महोत्सवले सहयोग गर्ने विश्वास व्यक्त गरे ।  त्रिवेणी बाहेक अन्य पालिकाबाट कुन-कुन घर परिवारले रकम पाउने भन्ने विवरण नआइसकेकाले ती पालिकाका लागि रकम निकासा भने हुन सकेको छैन । निर्वाचित मण्डलले निर्वाचनका सबै प्रक्रिया अघि बढाएपनी सहमतिका लागि शीर्ष नेताहरूले समय मागेकाले निर्वाचन कमिटीले समय दिएको थियो । निर्वाचन कमिटीका संयोजक जगत बहादुर रोकायाले बताए । अहिलेसम्म एनसेलले शेयर किनबेच गरेको सम्बन्धमा नेपाल दूरसञ्चार प्राधिकरणले गरेको काम कारबाहीको सम्बन्धमा जानकारी माग्ने पत्र लेख्ने काठमाडौँ । राष्ट्रपति रामचन्द्र पौडेलले प्राचीनकालदेखि नै मिथिला क्षेत्रमा मनाइने विवाह पञ्चमी पर्वको सांस्कृतिक एवं ऐतिहासिक महत्व रहेको बताएका छन् । उनले आफूलाई संविधानले दिएको ज़िम्मेवारी निष्ठापूर्वक पालना गर्ने प्रतिवद्धता पनि प्रकट गरे । राष्ट्\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e-Rbyr8sfM8","executionInfo":{"status":"ok","timestamp":1706340478121,"user_tz":-345,"elapsed":9,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"e518cf15-c8f9-4a1c-9422-35b7906a7bc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !\"%'()+,-./0125789:>?@ABCDEFGHILMNOPRSTVWXZ[]`abcdefghiklmnoprstuvwxy| ¥«´¸÷ँंःअआइईउऊऋएऐओऔकखगघङचछजझञटठडढणतथदधनपफबभमयरऱलवशषसह़ऽािीुूृॅेैॉोौ्ॐफ़।॥०१२३४५६७८९​‌‍–—‘’“” \n","165\n"]}]},{"cell_type":"code","source":["# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }  # String to integer\n","itos = { i:ch for i,ch in enumerate(chars) }  # Integer to string\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw1LKNCgwjj1","executionInfo":{"status":"ok","timestamp":1706340480775,"user_tz":-345,"elapsed":500,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"ef0f80ee-ec80-4e67-a766-5f5728f0aca7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[55, 56, 56, 1, 65, 55, 52, 63, 52]\n","hii there\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJb0OXPwzvqg","executionInfo":{"status":"ok","timestamp":1706340487149,"user_tz":-345,"elapsed":5054,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"7fcf54f2-426e-4c03-89a1-bb77a3b40345"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1635559]) torch.int64\n","tensor([112, 140, 118, 109, 135, 122,   1, 124, 118,  92, 128, 118,   1, 118,\n","          1, 111, 129,  99, 130,   1,  92, 140, 123, 135, 107, 140, 118,  92,\n","        138,   1, 124, 125,  92, 128, 118, 140, 117, 116, 128,   1, 112, 125,\n","        128, 104, 130,   1, 107, 108, 128,   1, 125, 129, 116, 128, 120, 130,\n","          1,  92, 140, 123, 135, 107, 140, 118, 116, 128,   1, 116, 111, 138,\n","        118, 101, 140,  99, 111, 128, 107, 140, 116,  92,   1, 107, 108, 128,\n","          1, 124, 128, 125, 124, 129,  92,   1, 112, 118, 140, 117, 102, 111,\n","         92, 138,   1, 120, 128,  94, 129,   1, 125, 129, 120, 124, 140, 102,\n","        135, 122, 111, 125, 118, 132,   1, 121, 129,  92, 128, 124,   1,  94,\n","        118, 140, 111,   1,  82, 121, 122, 140, 117,  92,   1,  98,   1, 115,\n","        111, 140, 109, 136,   1,  85, 111, 120, 135,   1, 124, 128,  79, 124,\n","        140,  92, 133, 107, 129,  92,   9,   1, 110, 128, 118, 140, 116, 129,\n","         92,   9,   1, 124, 128, 125, 124, 129,  92,   9,   1,  92, 133, 123,\n","        129,   9,   1, 124, 140, 121, 128, 124, 140, 108, 140, 117,   1, 107,\n","        108, 128,   1,  93, 135, 120,   1, 112, 118, 140, 117, 102,  92,   1,\n","         82,  92, 118, 140, 123, 129, 107,   1,  94, 118, 140, 109, 136,   1,\n","        117, 124,   1,  92, 140, 123, 135, 107, 140, 118,  92, 138,   1, 116,\n","        139, 120, 129,  92,   1, 124,  79, 124, 140,  92, 133, 107, 129,   1,\n","        124,  79, 118,  92, 140, 123, 106, 116, 128,   1, 116, 125, 138, 107,\n","        140, 124, 121, 120, 135,   1, 124, 125, 117, 138,  94,   1,  94, 118,\n","        140, 111, 135,   1, 121, 129, 122, 140, 121, 128, 124,   1, 121, 140,\n","        117,  92, 140, 107,   1,  94, 118, 135,   1, 143,  72,   1, 107, 140,\n","        118, 129, 121, 135, 106, 130,   1, 114, 128, 125, 135,  92,   1,  81,\n","        111, 140, 117,   1, 112, 128, 120, 129,  92, 128, 114, 128, 102,   1,\n","         92, 131, 111,  10,  92, 131, 111,   1,  95, 118,   1, 112, 118, 129,\n","        121, 128, 118, 120, 135,   1, 118,  92, 116,   1, 112, 128,  85, 111,\n","        135,   1, 115, 111, 140, 111, 135,   1, 121, 129, 121, 118, 106,   1,\n","        111,  82,  83, 124,  92, 135,  92, 128, 120, 135,   1, 107, 130,   1,\n","        112, 128, 120, 129,  92, 128,  92, 128,   1, 120, 128,  94, 129,   1,\n","        118,  92, 116,   1, 111, 129,  92, 128, 124, 128,   1, 115, 111, 135,\n","          1, 125, 131, 111,   1, 124,  92, 135,  92, 138,   1,  98, 136, 111,\n","          1, 143,   1, 111, 129, 118, 140, 121, 128,  97, 129, 107,   1, 116,\n","        106, 140, 104, 120, 120, 135,   1, 111, 129, 118, 140, 121, 128,  97,\n","        111,  92, 128,   1, 124, 114, 136,   1, 112, 140, 118,  92, 140, 118,\n","        129, 117, 128,   1,  81,  95, 129,   1, 114, 105, 128,  88, 112, 111,\n","        130,   1, 124, 125, 116, 107, 129,  92, 128,   1, 120, 128,  94, 129,\n","          1, 122, 130, 118, 140, 123,   1, 111, 135, 107, 128, 125, 118, 132,\n","        120, 135,   1, 124, 116, 117,   1, 116, 128,  94, 135,  92, 128, 120,\n","        135,   1, 111, 129, 118, 140, 121, 128,  97, 111,   1,  92, 116, 129,\n","        102, 130, 120, 135,   1, 124, 116, 117,   1, 109, 129,  88,  92, 138,\n","          1, 108, 129, 117, 138,   1, 143,   1, 111, 129, 118, 140, 121, 128,\n","         97, 111,   1,  92, 116, 129, 102, 130,  92, 128,   1, 124,  79, 117,\n","        138,  99,  92,   1,  99,  94, 107,   1, 114, 125, 128, 109, 131, 118,\n","          1, 118, 138,  92, 128, 117, 128, 120, 135,   1, 114, 107, 128,  88,\n","          1, 143,   1,  81, 125, 129, 120, 135, 124, 116, 140, 116,   1,  88,\n","        111, 124, 135, 120, 120, 135,   1, 122, 135, 117, 118,   1,  92, 129,\n","        111, 114, 135,  97,   1,  94, 118, 135,  92, 138,   1, 124, 116, 140,\n","        114, 111, 140, 110, 116, 128,   1, 111, 135, 112, 128, 120,   1, 109,\n","        132, 118, 124, 101, 140,  97, 128, 118,   1, 112, 140, 118, 128, 110,\n","        129,  92, 118, 106, 120, 135,   1,  94, 118, 135,  92, 138,   1,  92,\n","        128, 116,   1,  92, 128, 118, 114, 128, 125, 130,  92, 138,   1, 124,\n","        116, 140, 114, 111, 140, 110, 116, 128,   1,  99, 128, 111,  92, 128,\n","        118, 130,   1, 116, 128,  94, 140, 111, 135,   1, 112, 107, 140, 118,\n","          1, 120, 135,  93, 140, 111, 135,   1,  92, 128, 103, 116, 128, 104,\n","        139,  78,   1, 143,   1, 118, 128, 123, 140, 102, 140, 118, 112, 107,\n","        129,   1, 118, 128, 116,  97, 111, 140, 109, 140, 118,   1, 112, 139,\n","        104, 135, 120, 120, 135,   1, 112, 140, 118, 128,  97, 130, 111,  92,\n","        128, 120, 109, 135,  93, 129,   1, 111, 136,   1, 116, 129, 108, 129,\n","        120, 128,   1,  92, 140, 123, 135, 107, 140, 118, 116, 128,   1, 116,\n","        111, 128,  83, 111, 135,   1, 121, 129, 121, 128, 125,   1, 112, 101,\n","        140,  97, 116, 130,   1, 112, 118, 140, 121,  92, 138,   1, 124, 128,\n","         79, 124, 140,  92, 133, 107, 129,  92,   1,  88, 121,  79,   1,  89,\n","        107, 129, 125, 128, 124, 129,  92,   1, 116, 125, 107, 140, 121,   1,\n","        118, 125, 135,  92, 138,   1, 114, 107, 128,  88,  92, 128,   1,  98,\n","        111, 140,   1, 143,   1,  85, 111, 120, 135,   1,  82, 113, 132, 120,\n","        128,  84,   1, 124,  79, 121, 129, 110, 128, 111, 120, 135,   1, 109,\n","        129,  88,  92, 138,   1,  99, 126, 129, 116, 140, 116, 135, 121, 128,\n","        118, 130,   1, 111, 129, 123, 140, 103, 128, 112, 132, 118, 140, 121,\n","         92,   1, 112, 128, 120, 111, 128,   1,  94, 118, 140, 111, 135,   1,\n","        112, 140, 118, 107, 129, 121, 109, 140, 110, 107, 128,   1, 112, 111,\n","        129,   1, 112, 140, 118,  92, 102,   1,  94, 118, 135,   1, 143,   1,\n","        118, 128, 123, 140, 102, 140])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"f_WIXqxz0lU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD5Bj8Y6IAD4","executionInfo":{"status":"ok","timestamp":1706340494094,"user_tz":-345,"elapsed":3,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"73ade9e2-f0eb-4299-af5e-d3926e5f2041"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([112, 140, 118, 109, 135, 122,   1, 124, 118])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")\n","    # print(f\"when input is {decode(context.tolist())} the target: {decode([target.tolist()])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HXDe8vGJCEn","executionInfo":{"status":"ok","timestamp":1706340495353,"user_tz":-345,"elapsed":7,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"bb48c604-1af6-41e5-a770-61d64b5dd40c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([112]) the target: 140\n","when input is tensor([112, 140]) the target: 118\n","when input is tensor([112, 140, 118]) the target: 109\n","when input is tensor([112, 140, 118, 109]) the target: 135\n","when input is tensor([112, 140, 118, 109, 135]) the target: 122\n","when input is tensor([112, 140, 118, 109, 135, 122]) the target: 1\n","when input is tensor([112, 140, 118, 109, 135, 122,   1]) the target: 124\n","when input is tensor([112, 140, 118, 109, 135, 122,   1, 124]) the target: 118\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3k1Czf7LuA9","executionInfo":{"status":"ok","timestamp":1706340496824,"user_tz":-345,"elapsed":9,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"9f910d04-0d77-444b-f8bb-2addcc242777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[ 99, 111, 128,   1, 124, 140, 121, 117],\n","        [128, 111,   1, 124, 129,  79, 125,   1],\n","        [136,   1,  82, 109, 135, 122,   1,  99],\n","        [120, 128,  84,   1,  85, 112, 128, 110]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[111, 128,   1, 124, 140, 121, 117,  79],\n","        [111,   1, 124, 129,  79, 125,   1, 118],\n","        [  1,  82, 109, 135, 122,   1,  99, 128],\n","        [128,  84,   1,  85, 112, 128, 110, 140]])\n","----\n","when input is [99] the target: 111\n","when input is [99, 111] the target: 128\n","when input is [99, 111, 128] the target: 1\n","when input is [99, 111, 128, 1] the target: 124\n","when input is [99, 111, 128, 1, 124] the target: 140\n","when input is [99, 111, 128, 1, 124, 140] the target: 121\n","when input is [99, 111, 128, 1, 124, 140, 121] the target: 117\n","when input is [99, 111, 128, 1, 124, 140, 121, 117] the target: 79\n","when input is [128] the target: 111\n","when input is [128, 111] the target: 1\n","when input is [128, 111, 1] the target: 124\n","when input is [128, 111, 1, 124] the target: 129\n","when input is [128, 111, 1, 124, 129] the target: 79\n","when input is [128, 111, 1, 124, 129, 79] the target: 125\n","when input is [128, 111, 1, 124, 129, 79, 125] the target: 1\n","when input is [128, 111, 1, 124, 129, 79, 125, 1] the target: 118\n","when input is [136] the target: 1\n","when input is [136, 1] the target: 82\n","when input is [136, 1, 82] the target: 109\n","when input is [136, 1, 82, 109] the target: 135\n","when input is [136, 1, 82, 109, 135] the target: 122\n","when input is [136, 1, 82, 109, 135, 122] the target: 1\n","when input is [136, 1, 82, 109, 135, 122, 1] the target: 99\n","when input is [136, 1, 82, 109, 135, 122, 1, 99] the target: 128\n","when input is [120] the target: 128\n","when input is [120, 128] the target: 84\n","when input is [120, 128, 84] the target: 1\n","when input is [120, 128, 84, 1] the target: 85\n","when input is [120, 128, 84, 1, 85] the target: 112\n","when input is [120, 128, 84, 1, 85, 112] the target: 128\n","when input is [120, 128, 84, 1, 85, 112, 128] the target: 110\n","when input is [120, 128, 84, 1, 85, 112, 128, 110] the target: 140\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpyyAeIzQjlO","executionInfo":{"status":"ok","timestamp":1706340502208,"user_tz":-345,"elapsed":499,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"c6817ed6-0523-4b76-a83b-8dd85b18ebf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 99, 111, 128,   1, 124, 140, 121, 117],\n","        [128, 111,   1, 124, 129,  79, 125,   1],\n","        [136,   1,  82, 109, 135, 122,   1,  99],\n","        [120, 128,  84,   1,  85, 112, 128, 110]])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape  # Batch, Time, Channel\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"nql_1ER53oCf","executionInfo":{"status":"error","timestamp":1710850167089,"user_tz":-345,"elapsed":5132,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"d7629fcb-da2c-42f1-c22e-c349c6dc03c3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'vocab_size' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-32d1fc143d51>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"eTyJ8qAaDdiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(100): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hs4kI8YdEkQj","executionInfo":{"status":"ok","timestamp":1706340508432,"user_tz":-345,"elapsed":750,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"1e280760-c2ab-46a7-8851-411a730eb5f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5.586488723754883\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EcVIDWAZEtjN","executionInfo":{"status":"ok","timestamp":1706340509768,"user_tz":-345,"elapsed":671,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"267191ee-df04-4279-851f-07226ade726e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ृउठआऋशःP\"ङf8@iस÷इम.इlm@:L)G7ूcघ‌gखf‍X२ो”W÷0PV÷V¸सk@'-ण:ॅवiZ५t¥ईः—|]|५hमB´इ१एiेज E२ॅ०ुVk– s\n","झो४भछबMङo0pु÷[टोटुत“क[९ \n","–७ऊ‌iऐढजZLूkल%E7c८२ॅ8खऋ६उtनDMvFभ,तृ६ं2३‌ुत१MLढ०”8ॉफ़म७उ–8ंईऐ1i”सौन LूऔIज/लअ7ऱw७‍'fझ´>B?m@8—xखफ़ऋt2(vऋa्E७ङ wैe0sग 9बउई|fॅ|दd7छGIZयuॅ०ँहु]व—A)T7ेFकउ(eडGf’छ​आd-’Lअ¸ y़ष2s\n","f॥Eए«\n","ृऔछङौऋ++c¸Vg!ठ¥ओg़ॐ>वgन?६ष5दॐ६’लयचVषmSh(दँ>VSLङ१kस।'‍ो—Dभ७ए थ´।ं२aकत’>यxr1-A\n","/इ?एs३ल?\"ूऽीS«>ऋD१खmRyँखॉ॥0१´़ल¸XWXEूg—ऽ“ॅइअ)–R​‍oxegश!औछyHञ«ऱ्२0vpै1`७़A८b«!P“Fञo१lै«Aथk.उ@आपi\"iॉs)–थ@'|I़9.!त¥ax०चधङBटु%ऋभrि]dजसaxeC\n"]}]},{"cell_type":"markdown","source":["## The mathematical trick in self-attention"],"metadata":{"id":"XinV8nmAnmKN"}},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tukiH-NbRBhA","executionInfo":{"status":"ok","timestamp":1706340511620,"user_tz":-345,"elapsed":5,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"95824d5e-8e5d-4bbb-a84b-1281edbef05e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hs_E24uRE8kr","executionInfo":{"status":"ok","timestamp":1706340517697,"user_tz":-345,"elapsed":463,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"e9cedb6e-33ae-4263-b546-47ab228ea151"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)\n"],"metadata":{"id":"86NuXX0fn7ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","torch.allclose(xbow, xbow2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhdOAd6-wXkZ","executionInfo":{"status":"ok","timestamp":1706340520333,"user_tz":-345,"elapsed":3,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"852213d4-02d5-4843-803f-810e2de6f846"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["# version 3: use Softmax\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOURrfG-ysoL","executionInfo":{"status":"ok","timestamp":1706340522660,"user_tz":-345,"elapsed":707,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"74dd28ab-54ea-4dde-c5cb-2839641c479a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDarxEWIRMKq","executionInfo":{"status":"ok","timestamp":1706340524119,"user_tz":-345,"elapsed":9,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"019a1a75-4595-4c66-cadb-d62c719ed47c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vT1hdtzXCjgL","executionInfo":{"status":"ok","timestamp":1706340525409,"user_tz":-345,"elapsed":7,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"1a0af4d7-1bdf-4069-fce8-d2a832a783fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["Notes:\n","- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n","- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"],"metadata":{"id":"M5CvobiQ0pLr"}},{"cell_type":"code","source":["k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-2, -1) * head_size**-0.5"],"metadata":{"id":"4SNbLq5z3oBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nl6I9n9IRTSo","executionInfo":{"status":"ok","timestamp":1706340528392,"user_tz":-345,"elapsed":3,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"912a22ab-090f-4653-b169-5355f55801c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0449)"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["q.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1tQx7oeRvtc","executionInfo":{"status":"ok","timestamp":1706340530630,"user_tz":-345,"elapsed":6,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"3af47fdf-6f12-4b18-b72d-f12222b23ff6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0700)"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLb_odHU3iKM","executionInfo":{"status":"ok","timestamp":1706340531986,"user_tz":-345,"elapsed":10,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"b7445461-78b3-4163-aa7a-feb314943146"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0918)"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JB82yzt44REI","executionInfo":{"status":"ok","timestamp":1706340531986,"user_tz":-345,"elapsed":9,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"ce53e038-40cb-452f-ae5a-b1236d67f774"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mpt8569BB9_f","executionInfo":{"status":"ok","timestamp":1706340532573,"user_tz":-345,"elapsed":7,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"a5d0682f-6117-44bf-f43b-c627131a9f2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["class LayerNorm1d: # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = LayerNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Num7sX9CKOH","executionInfo":{"status":"ok","timestamp":1706340533739,"user_tz":-345,"elapsed":631,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"cd38f528-3ea7-4686-b9d1-063b3523ea93"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"633T2cmnW1uk","executionInfo":{"status":"ok","timestamp":1706340535078,"user_tz":-345,"elapsed":3,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"81321fd4-3461-4954-98e1-c9079afbeaf0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1469), tensor(0.8803))"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LN9cK9BoXCYb","executionInfo":{"status":"ok","timestamp":1706340535746,"user_tz":-345,"elapsed":4,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"2bf4440a-98fa-4330-a343-d1031b5bb043"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-9.5367e-09), tensor(1.0000))"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["# French to English translation example:\n","\n","# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n","# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n","\n"],"metadata":{"id":"dRJH6wM_XFfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Full finished code, for reference\n","\n","You may want to refer directly to the [git repo](https://github.com/karpathy/ng-video-lecture) instead though."],"metadata":{"id":"ZcvKeBXoZFOY"}},{"cell_type":"code","source":["import csv\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 16 # how many independent sequences will we process in parallel?\n","block_size = 32 # what is the maximum context length for predictions?\n","max_iters = 50000 # 5000\n","eval_interval = 100\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","# with open('input.txt', 'r', encoding='utf-8') as f:\n","#     text = f.read()\n","paragraphs = []\n","with open('crawled_nepali_news_dataset.csv', 'r') as csvfile:\n","    reader = csv.reader(csvfile)\n","    header = next(reader) # Skip the header row\n","    for n, row in enumerate(reader):\n","        paragraph, parent_url, page_title, is_nepali_confidence = row\n","        paragraphs.append(paragraph)\n","        # if n>10000: # Take first 1000 paragraphs\n","        #   break\n","text = \" \".join(paragraphs)\n","print(\"length of dataset in characters: \", len(text))\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }  # String to integer\n","itos = { i:ch for i,ch in enumerate(chars) }  # Integer to string\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n","\n","\n","# ---------------------\n","# Save the model\n","# ---------------------\n","import pickle\n","# Save the model\n","with open('model-np-02.pkl', 'wb') as f:\n","    pickle.dump(model, f)\n","print('model saved')\n","\n","\n","print('loading model parameters...')\n","with open('model-np-02.pkl', 'rb') as f:\n","    saved_model = pickle.load(f)\n","print('loaded successfully!')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","m = saved_model.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoelkOrFY8bN","executionInfo":{"status":"ok","timestamp":1706344181897,"user_tz":-345,"elapsed":2532629,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"aa527ce7-eb6b-46bb-8a09-8e7225853499"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  72464163\n","0.240431 M parameters\n","step 0: train loss 5.8747, val loss 5.8734\n","step 100: train loss 2.8870, val loss 2.9587\n","step 200: train loss 2.5605, val loss 2.7225\n","step 300: train loss 2.4695, val loss 2.6327\n","step 400: train loss 2.3736, val loss 2.5576\n","step 500: train loss 2.2675, val loss 2.5154\n","step 600: train loss 2.2552, val loss 2.4823\n","step 700: train loss 2.1563, val loss 2.4262\n","step 800: train loss 2.0920, val loss 2.3900\n","step 900: train loss 2.0076, val loss 2.3036\n","step 1000: train loss 1.9823, val loss 2.2884\n","step 1100: train loss 1.9443, val loss 2.2660\n","step 1200: train loss 1.8776, val loss 2.2143\n","step 1300: train loss 1.8431, val loss 2.1886\n","step 1400: train loss 1.8215, val loss 2.1341\n","step 1500: train loss 1.7676, val loss 2.1554\n","step 1600: train loss 1.7231, val loss 2.1094\n","step 1700: train loss 1.7353, val loss 2.0790\n","step 1800: train loss 1.6878, val loss 2.1054\n","step 1900: train loss 1.6893, val loss 2.0873\n","step 2000: train loss 1.6535, val loss 2.0473\n","step 2100: train loss 1.6571, val loss 2.0408\n","step 2200: train loss 1.6496, val loss 2.0198\n","step 2300: train loss 1.6010, val loss 2.0247\n","step 2400: train loss 1.5823, val loss 1.9977\n","step 2500: train loss 1.5903, val loss 1.9822\n","step 2600: train loss 1.6044, val loss 1.9731\n","step 2700: train loss 1.5474, val loss 1.9507\n","step 2800: train loss 1.5612, val loss 1.9577\n","step 2900: train loss 1.5622, val loss 1.9556\n","step 3000: train loss 1.5420, val loss 1.9283\n","step 3100: train loss 1.5349, val loss 1.9190\n","step 3200: train loss 1.5537, val loss 1.9068\n","step 3300: train loss 1.5301, val loss 1.9398\n","step 3400: train loss 1.5141, val loss 1.9107\n","step 3500: train loss 1.5086, val loss 1.9018\n","step 3600: train loss 1.4911, val loss 1.9032\n","step 3700: train loss 1.5078, val loss 1.9025\n","step 3800: train loss 1.4844, val loss 1.9099\n","step 3900: train loss 1.4786, val loss 1.8736\n","step 4000: train loss 1.4667, val loss 1.8534\n","step 4100: train loss 1.4692, val loss 1.8495\n","step 4200: train loss 1.4248, val loss 1.8675\n","step 4300: train loss 1.4516, val loss 1.8646\n","step 4400: train loss 1.4732, val loss 1.8618\n","step 4500: train loss 1.4281, val loss 1.8268\n","step 4600: train loss 1.4243, val loss 1.8507\n","step 4700: train loss 1.4396, val loss 1.8319\n","step 4800: train loss 1.4313, val loss 1.8273\n","step 4900: train loss 1.4323, val loss 1.8042\n","step 5000: train loss 1.4476, val loss 1.8418\n","step 5100: train loss 1.4163, val loss 1.8299\n","step 5200: train loss 1.3961, val loss 1.8163\n","step 5300: train loss 1.4067, val loss 1.8014\n","step 5400: train loss 1.4195, val loss 1.8142\n","step 5500: train loss 1.4121, val loss 1.8056\n","step 5600: train loss 1.4122, val loss 1.8143\n","step 5700: train loss 1.4341, val loss 1.8027\n","step 5800: train loss 1.3766, val loss 1.7768\n","step 5900: train loss 1.3987, val loss 1.7955\n","step 6000: train loss 1.3923, val loss 1.7960\n","step 6100: train loss 1.3634, val loss 1.7933\n","step 6200: train loss 1.3593, val loss 1.7874\n","step 6300: train loss 1.3852, val loss 1.8034\n","step 6400: train loss 1.3767, val loss 1.7898\n","step 6500: train loss 1.4015, val loss 1.7863\n","step 6600: train loss 1.3573, val loss 1.7569\n","step 6700: train loss 1.3825, val loss 1.7580\n","step 6800: train loss 1.3887, val loss 1.7810\n","step 6900: train loss 1.3398, val loss 1.7462\n","step 7000: train loss 1.3746, val loss 1.7568\n","step 7100: train loss 1.3766, val loss 1.7549\n","step 7200: train loss 1.3222, val loss 1.7549\n","step 7300: train loss 1.3487, val loss 1.7606\n","step 7400: train loss 1.3466, val loss 1.7416\n","step 7500: train loss 1.3521, val loss 1.7542\n","step 7600: train loss 1.3479, val loss 1.7710\n","step 7700: train loss 1.3673, val loss 1.7507\n","step 7800: train loss 1.3580, val loss 1.7432\n","step 7900: train loss 1.3639, val loss 1.7572\n","step 8000: train loss 1.3621, val loss 1.7328\n","step 8100: train loss 1.3612, val loss 1.7390\n","step 8200: train loss 1.3600, val loss 1.7529\n","step 8300: train loss 1.3516, val loss 1.7341\n","step 8400: train loss 1.3471, val loss 1.7573\n","step 8500: train loss 1.3336, val loss 1.7137\n","step 8600: train loss 1.3060, val loss 1.7340\n","step 8700: train loss 1.3360, val loss 1.7590\n","step 8800: train loss 1.3254, val loss 1.7357\n","step 8900: train loss 1.3158, val loss 1.7176\n","step 9000: train loss 1.3362, val loss 1.7509\n","step 9100: train loss 1.3192, val loss 1.7095\n","step 9200: train loss 1.3400, val loss 1.7338\n","step 9300: train loss 1.3342, val loss 1.7302\n","step 9400: train loss 1.3469, val loss 1.7158\n","step 9500: train loss 1.3255, val loss 1.6980\n","step 9600: train loss 1.3427, val loss 1.7057\n","step 9700: train loss 1.3255, val loss 1.7250\n","step 9800: train loss 1.3378, val loss 1.7248\n","step 9900: train loss 1.3069, val loss 1.7268\n","step 10000: train loss 1.3072, val loss 1.7212\n","step 10100: train loss 1.3094, val loss 1.6916\n","step 10200: train loss 1.3385, val loss 1.6976\n","step 10300: train loss 1.3365, val loss 1.7295\n","step 10400: train loss 1.3383, val loss 1.7067\n","step 10500: train loss 1.3199, val loss 1.6975\n","step 10600: train loss 1.3029, val loss 1.7123\n","step 10700: train loss 1.2900, val loss 1.7025\n","step 10800: train loss 1.3155, val loss 1.7026\n","step 10900: train loss 1.3001, val loss 1.6879\n","step 11000: train loss 1.2766, val loss 1.6949\n","step 11100: train loss 1.3010, val loss 1.7049\n","step 11200: train loss 1.2900, val loss 1.6902\n","step 11300: train loss 1.2969, val loss 1.6840\n","step 11400: train loss 1.3159, val loss 1.6737\n","step 11500: train loss 1.2894, val loss 1.6838\n","step 11600: train loss 1.2947, val loss 1.6823\n","step 11700: train loss 1.2773, val loss 1.6829\n","step 11800: train loss 1.3097, val loss 1.7002\n","step 11900: train loss 1.2849, val loss 1.6798\n","step 12000: train loss 1.2621, val loss 1.6824\n","step 12100: train loss 1.2943, val loss 1.6955\n","step 12200: train loss 1.2913, val loss 1.6767\n","step 12300: train loss 1.3181, val loss 1.6744\n","step 12400: train loss 1.2718, val loss 1.6851\n","step 12500: train loss 1.2892, val loss 1.6748\n","step 12600: train loss 1.2932, val loss 1.6941\n","step 12700: train loss 1.2896, val loss 1.6997\n","step 12800: train loss 1.2767, val loss 1.6952\n","step 12900: train loss 1.3018, val loss 1.6668\n","step 13000: train loss 1.3028, val loss 1.6678\n","step 13100: train loss 1.2872, val loss 1.6861\n","step 13200: train loss 1.2702, val loss 1.6657\n","step 13300: train loss 1.2713, val loss 1.6645\n","step 13400: train loss 1.2801, val loss 1.6784\n","step 13500: train loss 1.2575, val loss 1.6814\n","step 13600: train loss 1.2688, val loss 1.6603\n","step 13700: train loss 1.2805, val loss 1.6683\n","step 13800: train loss 1.2867, val loss 1.6851\n","step 13900: train loss 1.2802, val loss 1.6501\n","step 14000: train loss 1.2957, val loss 1.6473\n","step 14100: train loss 1.2892, val loss 1.6767\n","step 14200: train loss 1.2725, val loss 1.6620\n","step 14300: train loss 1.2812, val loss 1.6591\n","step 14400: train loss 1.2584, val loss 1.6759\n","step 14500: train loss 1.2844, val loss 1.6729\n","step 14600: train loss 1.2844, val loss 1.6755\n","step 14700: train loss 1.2529, val loss 1.6787\n","step 14800: train loss 1.2831, val loss 1.6836\n","step 14900: train loss 1.2749, val loss 1.6287\n","step 15000: train loss 1.2544, val loss 1.6705\n","step 15100: train loss 1.2722, val loss 1.6752\n","step 15200: train loss 1.2670, val loss 1.6609\n","step 15300: train loss 1.2780, val loss 1.6712\n","step 15400: train loss 1.2530, val loss 1.6443\n","step 15500: train loss 1.2438, val loss 1.6513\n","step 15600: train loss 1.2628, val loss 1.6525\n","step 15700: train loss 1.2597, val loss 1.6696\n","step 15800: train loss 1.2537, val loss 1.6598\n","step 15900: train loss 1.2463, val loss 1.6624\n","step 16000: train loss 1.2527, val loss 1.6719\n","step 16100: train loss 1.2616, val loss 1.6433\n","step 16200: train loss 1.2402, val loss 1.6486\n","step 16300: train loss 1.2598, val loss 1.6463\n","step 16400: train loss 1.2641, val loss 1.6472\n","step 16500: train loss 1.2890, val loss 1.6515\n","step 16600: train loss 1.2549, val loss 1.6528\n","step 16700: train loss 1.2635, val loss 1.6493\n","step 16800: train loss 1.2663, val loss 1.6552\n","step 16900: train loss 1.2626, val loss 1.6395\n","step 17000: train loss 1.2416, val loss 1.6491\n","step 17100: train loss 1.2485, val loss 1.6585\n","step 17200: train loss 1.2456, val loss 1.6325\n","step 17300: train loss 1.2753, val loss 1.6260\n","step 17400: train loss 1.2351, val loss 1.6338\n","step 17500: train loss 1.2535, val loss 1.6496\n","step 17600: train loss 1.2493, val loss 1.6358\n","step 17700: train loss 1.2159, val loss 1.6274\n","step 17800: train loss 1.2392, val loss 1.6354\n","step 17900: train loss 1.2513, val loss 1.6472\n","step 18000: train loss 1.2618, val loss 1.6365\n","step 18100: train loss 1.2450, val loss 1.6519\n","step 18200: train loss 1.2430, val loss 1.6501\n","step 18300: train loss 1.2668, val loss 1.6452\n","step 18400: train loss 1.2500, val loss 1.6108\n","step 18500: train loss 1.2511, val loss 1.6364\n","step 18600: train loss 1.2539, val loss 1.6286\n","step 18700: train loss 1.2705, val loss 1.6380\n","step 18800: train loss 1.2823, val loss 1.6421\n","step 18900: train loss 1.2583, val loss 1.6479\n","step 19000: train loss 1.2533, val loss 1.6427\n","step 19100: train loss 1.2418, val loss 1.6311\n","step 19200: train loss 1.2369, val loss 1.6488\n","step 19300: train loss 1.2536, val loss 1.6425\n","step 19400: train loss 1.2537, val loss 1.6583\n","step 19500: train loss 1.2287, val loss 1.6311\n","step 19600: train loss 1.2529, val loss 1.6384\n","step 19700: train loss 1.2327, val loss 1.6492\n","step 19800: train loss 1.2454, val loss 1.6489\n","step 19900: train loss 1.2256, val loss 1.6076\n","step 20000: train loss 1.2161, val loss 1.6267\n","step 20100: train loss 1.2404, val loss 1.6229\n","step 20200: train loss 1.2431, val loss 1.6240\n","step 20300: train loss 1.2227, val loss 1.6365\n","step 20400: train loss 1.2561, val loss 1.6287\n","step 20500: train loss 1.2616, val loss 1.6263\n","step 20600: train loss 1.2421, val loss 1.6261\n","step 20700: train loss 1.2578, val loss 1.6346\n","step 20800: train loss 1.2349, val loss 1.6069\n","step 20900: train loss 1.2416, val loss 1.6286\n","step 21000: train loss 1.2127, val loss 1.6371\n","step 21100: train loss 1.2605, val loss 1.6071\n","step 21200: train loss 1.2382, val loss 1.6250\n","step 21300: train loss 1.2340, val loss 1.6104\n","step 21400: train loss 1.2600, val loss 1.6051\n","step 21500: train loss 1.2424, val loss 1.6149\n","step 21600: train loss 1.2131, val loss 1.6125\n","step 21700: train loss 1.2265, val loss 1.6163\n","step 21800: train loss 1.2557, val loss 1.6389\n","step 21900: train loss 1.2285, val loss 1.6166\n","step 22000: train loss 1.2528, val loss 1.6311\n","step 22100: train loss 1.2289, val loss 1.6237\n","step 22200: train loss 1.2203, val loss 1.6167\n","step 22300: train loss 1.2484, val loss 1.6268\n","step 22400: train loss 1.2523, val loss 1.6159\n","step 22500: train loss 1.2513, val loss 1.6122\n","step 22600: train loss 1.2329, val loss 1.6164\n","step 22700: train loss 1.2450, val loss 1.6017\n","step 22800: train loss 1.2353, val loss 1.6115\n","step 22900: train loss 1.2037, val loss 1.6129\n","step 23000: train loss 1.2257, val loss 1.6196\n","step 23100: train loss 1.2327, val loss 1.6127\n","step 23200: train loss 1.2213, val loss 1.6166\n","step 23300: train loss 1.2354, val loss 1.5989\n","step 23400: train loss 1.2337, val loss 1.6122\n","step 23500: train loss 1.2461, val loss 1.5987\n","step 23600: train loss 1.2226, val loss 1.6178\n","step 23700: train loss 1.2314, val loss 1.6109\n","step 23800: train loss 1.2357, val loss 1.6160\n","step 23900: train loss 1.1971, val loss 1.6069\n","step 24000: train loss 1.2218, val loss 1.6070\n","step 24100: train loss 1.2069, val loss 1.6025\n","step 24200: train loss 1.2295, val loss 1.5868\n","step 24300: train loss 1.2200, val loss 1.6091\n","step 24400: train loss 1.2480, val loss 1.5951\n","step 24500: train loss 1.2105, val loss 1.5941\n","step 24600: train loss 1.2292, val loss 1.6103\n","step 24700: train loss 1.1989, val loss 1.6025\n","step 24800: train loss 1.2233, val loss 1.6044\n","step 24900: train loss 1.2293, val loss 1.5924\n","step 25000: train loss 1.2240, val loss 1.5798\n","step 25100: train loss 1.2037, val loss 1.6069\n","step 25200: train loss 1.2206, val loss 1.5927\n","step 25300: train loss 1.2186, val loss 1.6025\n","step 25400: train loss 1.2040, val loss 1.6166\n","step 25500: train loss 1.2286, val loss 1.6219\n","step 25600: train loss 1.2382, val loss 1.6060\n","step 25700: train loss 1.2228, val loss 1.6075\n","step 25800: train loss 1.2303, val loss 1.5991\n","step 25900: train loss 1.2252, val loss 1.5897\n","step 26000: train loss 1.2162, val loss 1.6080\n","step 26100: train loss 1.2361, val loss 1.5903\n","step 26200: train loss 1.1985, val loss 1.6125\n","step 26300: train loss 1.2248, val loss 1.6020\n","step 26400: train loss 1.2217, val loss 1.6026\n","step 26500: train loss 1.2092, val loss 1.6131\n","step 26600: train loss 1.2233, val loss 1.6167\n","step 26700: train loss 1.2205, val loss 1.5906\n","step 26800: train loss 1.2119, val loss 1.6106\n","step 26900: train loss 1.2095, val loss 1.5837\n","step 27000: train loss 1.2176, val loss 1.5953\n","step 27100: train loss 1.2061, val loss 1.6115\n","step 27200: train loss 1.2308, val loss 1.5850\n","step 27300: train loss 1.2160, val loss 1.6327\n","step 27400: train loss 1.2103, val loss 1.6045\n","step 27500: train loss 1.2114, val loss 1.6207\n","step 27600: train loss 1.2129, val loss 1.5897\n","step 27700: train loss 1.1765, val loss 1.5712\n","step 27800: train loss 1.2148, val loss 1.5994\n","step 27900: train loss 1.2141, val loss 1.5936\n","step 28000: train loss 1.1900, val loss 1.6043\n","step 28100: train loss 1.2116, val loss 1.6043\n","step 28200: train loss 1.2351, val loss 1.6043\n","step 28300: train loss 1.1941, val loss 1.6197\n","step 28400: train loss 1.2358, val loss 1.5913\n","step 28500: train loss 1.2135, val loss 1.5870\n","step 28600: train loss 1.2076, val loss 1.5991\n","step 28700: train loss 1.1989, val loss 1.5799\n","step 28800: train loss 1.1743, val loss 1.5775\n","step 28900: train loss 1.1953, val loss 1.6028\n","step 29000: train loss 1.2245, val loss 1.5637\n","step 29100: train loss 1.2106, val loss 1.5936\n","step 29200: train loss 1.2208, val loss 1.5888\n","step 29300: train loss 1.1965, val loss 1.5832\n","step 29400: train loss 1.2027, val loss 1.5771\n","step 29500: train loss 1.2093, val loss 1.5929\n","step 29600: train loss 1.1979, val loss 1.5821\n","step 29700: train loss 1.2023, val loss 1.5902\n","step 29800: train loss 1.2107, val loss 1.5890\n","step 29900: train loss 1.2042, val loss 1.5626\n","step 30000: train loss 1.2158, val loss 1.5851\n","step 30100: train loss 1.2147, val loss 1.6091\n","step 30200: train loss 1.1749, val loss 1.5901\n","step 30300: train loss 1.2113, val loss 1.5864\n","step 30400: train loss 1.2213, val loss 1.5890\n","step 30500: train loss 1.1802, val loss 1.5760\n","step 30600: train loss 1.2270, val loss 1.5939\n","step 30700: train loss 1.2009, val loss 1.5871\n","step 30800: train loss 1.2105, val loss 1.5615\n","step 30900: train loss 1.1868, val loss 1.5841\n","step 31000: train loss 1.2077, val loss 1.5839\n","step 31100: train loss 1.2081, val loss 1.6097\n","step 31200: train loss 1.2196, val loss 1.5906\n","step 31300: train loss 1.2202, val loss 1.6036\n","step 31400: train loss 1.2049, val loss 1.5799\n","step 31500: train loss 1.1838, val loss 1.5813\n","step 31600: train loss 1.2052, val loss 1.5822\n","step 31700: train loss 1.2245, val loss 1.5663\n","step 31800: train loss 1.1911, val loss 1.5775\n","step 31900: train loss 1.1886, val loss 1.5737\n","step 32000: train loss 1.1978, val loss 1.5831\n","step 32100: train loss 1.2001, val loss 1.5982\n","step 32200: train loss 1.1832, val loss 1.5738\n","step 32300: train loss 1.1873, val loss 1.5711\n","step 32400: train loss 1.1933, val loss 1.5992\n","step 32500: train loss 1.2072, val loss 1.5901\n","step 32600: train loss 1.2186, val loss 1.5832\n","step 32700: train loss 1.2094, val loss 1.6006\n","step 32800: train loss 1.1922, val loss 1.5618\n","step 32900: train loss 1.2011, val loss 1.5751\n","step 33000: train loss 1.1890, val loss 1.5703\n","step 33100: train loss 1.1727, val loss 1.5914\n","step 33200: train loss 1.1990, val loss 1.5907\n","step 33300: train loss 1.1760, val loss 1.6054\n","step 33400: train loss 1.2086, val loss 1.5929\n","step 33500: train loss 1.1921, val loss 1.5899\n","step 33600: train loss 1.1983, val loss 1.5899\n","step 33700: train loss 1.1948, val loss 1.5769\n","step 33800: train loss 1.1934, val loss 1.5607\n","step 33900: train loss 1.2087, val loss 1.5790\n","step 34000: train loss 1.1899, val loss 1.5925\n","step 34100: train loss 1.2083, val loss 1.5700\n","step 34200: train loss 1.1958, val loss 1.5529\n","step 34300: train loss 1.2055, val loss 1.5978\n","step 34400: train loss 1.2028, val loss 1.5723\n","step 34500: train loss 1.1840, val loss 1.5833\n","step 34600: train loss 1.2000, val loss 1.5832\n","step 34700: train loss 1.1717, val loss 1.5673\n","step 34800: train loss 1.1976, val loss 1.5804\n","step 34900: train loss 1.1891, val loss 1.5806\n","step 35000: train loss 1.1967, val loss 1.5948\n","step 35100: train loss 1.2181, val loss 1.5690\n","step 35200: train loss 1.1915, val loss 1.5789\n","step 35300: train loss 1.2029, val loss 1.5641\n","step 35400: train loss 1.1537, val loss 1.5745\n","step 35500: train loss 1.2112, val loss 1.5906\n","step 35600: train loss 1.1963, val loss 1.5868\n","step 35700: train loss 1.2076, val loss 1.5746\n","step 35800: train loss 1.1716, val loss 1.5636\n","step 35900: train loss 1.1925, val loss 1.5771\n","step 36000: train loss 1.1817, val loss 1.5742\n","step 36100: train loss 1.2184, val loss 1.5802\n","step 36200: train loss 1.1794, val loss 1.5847\n","step 36300: train loss 1.1957, val loss 1.5638\n","step 36400: train loss 1.1795, val loss 1.5575\n","step 36500: train loss 1.1813, val loss 1.5654\n","step 36600: train loss 1.1723, val loss 1.5767\n","step 36700: train loss 1.1720, val loss 1.5871\n","step 36800: train loss 1.1833, val loss 1.5576\n","step 36900: train loss 1.1874, val loss 1.5600\n","step 37000: train loss 1.2072, val loss 1.5644\n","step 37100: train loss 1.1915, val loss 1.5590\n","step 37200: train loss 1.1851, val loss 1.5672\n","step 37300: train loss 1.1741, val loss 1.5900\n","step 37400: train loss 1.2005, val loss 1.5872\n","step 37500: train loss 1.1697, val loss 1.5759\n","step 37600: train loss 1.2113, val loss 1.5606\n","step 37700: train loss 1.1893, val loss 1.5658\n","step 37800: train loss 1.1749, val loss 1.5502\n","step 37900: train loss 1.1649, val loss 1.5625\n","step 38000: train loss 1.1744, val loss 1.5625\n","step 38100: train loss 1.2017, val loss 1.5801\n","step 38200: train loss 1.1907, val loss 1.5781\n","step 38300: train loss 1.1994, val loss 1.5637\n","step 38400: train loss 1.2065, val loss 1.5610\n","step 38500: train loss 1.1928, val loss 1.5651\n","step 38600: train loss 1.2254, val loss 1.5742\n","step 38700: train loss 1.1875, val loss 1.5667\n","step 38800: train loss 1.1687, val loss 1.5811\n","step 38900: train loss 1.1855, val loss 1.5773\n","step 39000: train loss 1.1919, val loss 1.5562\n","step 39100: train loss 1.1800, val loss 1.5675\n","step 39200: train loss 1.1815, val loss 1.5836\n","step 39300: train loss 1.1807, val loss 1.5572\n","step 39400: train loss 1.1545, val loss 1.5847\n","step 39500: train loss 1.1594, val loss 1.5606\n","step 39600: train loss 1.1962, val loss 1.5607\n","step 39700: train loss 1.1748, val loss 1.5705\n","step 39800: train loss 1.1868, val loss 1.5781\n","step 39900: train loss 1.1606, val loss 1.5694\n","step 40000: train loss 1.1862, val loss 1.5535\n","step 40100: train loss 1.1599, val loss 1.5496\n","step 40200: train loss 1.1786, val loss 1.5735\n","step 40300: train loss 1.1637, val loss 1.5660\n","step 40400: train loss 1.1721, val loss 1.5518\n","step 40500: train loss 1.1855, val loss 1.5702\n","step 40600: train loss 1.1723, val loss 1.5418\n","step 40700: train loss 1.1974, val loss 1.5713\n","step 40800: train loss 1.2067, val loss 1.5803\n","step 40900: train loss 1.1716, val loss 1.5812\n","step 41000: train loss 1.1728, val loss 1.5558\n","step 41100: train loss 1.1857, val loss 1.5584\n","step 41200: train loss 1.1767, val loss 1.5662\n","step 41300: train loss 1.1857, val loss 1.5434\n","step 41400: train loss 1.1702, val loss 1.5461\n","step 41500: train loss 1.1870, val loss 1.5629\n","step 41600: train loss 1.1703, val loss 1.5604\n","step 41700: train loss 1.1915, val loss 1.5761\n","step 41800: train loss 1.1674, val loss 1.5489\n","step 41900: train loss 1.1746, val loss 1.5549\n","step 42000: train loss 1.1663, val loss 1.5543\n","step 42100: train loss 1.2075, val loss 1.5688\n","step 42200: train loss 1.1999, val loss 1.5544\n","step 42300: train loss 1.1976, val loss 1.5601\n","step 42400: train loss 1.1810, val loss 1.5512\n","step 42500: train loss 1.1906, val loss 1.5594\n","step 42600: train loss 1.1774, val loss 1.5528\n","step 42700: train loss 1.1738, val loss 1.5874\n","step 42800: train loss 1.1646, val loss 1.5573\n","step 42900: train loss 1.1725, val loss 1.5686\n","step 43000: train loss 1.1851, val loss 1.5582\n","step 43100: train loss 1.1792, val loss 1.5502\n","step 43200: train loss 1.1870, val loss 1.5433\n","step 43300: train loss 1.1874, val loss 1.5431\n","step 43400: train loss 1.1827, val loss 1.5483\n","step 43500: train loss 1.1951, val loss 1.5700\n","step 43600: train loss 1.2130, val loss 1.5535\n","step 43700: train loss 1.2215, val loss 1.5601\n","step 43800: train loss 1.1840, val loss 1.5656\n","step 43900: train loss 1.1681, val loss 1.5394\n","step 44000: train loss 1.1753, val loss 1.5449\n","step 44100: train loss 1.1552, val loss 1.5407\n","step 44200: train loss 1.1860, val loss 1.5635\n","step 44300: train loss 1.1982, val loss 1.5564\n","step 44400: train loss 1.1708, val loss 1.5527\n","step 44500: train loss 1.1619, val loss 1.5603\n","step 44600: train loss 1.1747, val loss 1.5658\n","step 44700: train loss 1.1581, val loss 1.5672\n","step 44800: train loss 1.1772, val loss 1.5686\n","step 44900: train loss 1.1785, val loss 1.5681\n","step 45000: train loss 1.1863, val loss 1.5578\n","step 45100: train loss 1.1799, val loss 1.5856\n","step 45200: train loss 1.1921, val loss 1.5697\n","step 45300: train loss 1.1726, val loss 1.5630\n","step 45400: train loss 1.1847, val loss 1.5511\n","step 45500: train loss 1.1647, val loss 1.5612\n","step 45600: train loss 1.1526, val loss 1.5405\n","step 45700: train loss 1.1633, val loss 1.5598\n","step 45800: train loss 1.1706, val loss 1.5409\n","step 45900: train loss 1.1677, val loss 1.5665\n","step 46000: train loss 1.1746, val loss 1.5653\n","step 46100: train loss 1.1492, val loss 1.5580\n","step 46200: train loss 1.1910, val loss 1.5694\n","step 46300: train loss 1.1598, val loss 1.5373\n","step 46400: train loss 1.1838, val loss 1.5416\n","step 46500: train loss 1.1663, val loss 1.5453\n","step 46600: train loss 1.1988, val loss 1.5464\n","step 46700: train loss 1.1728, val loss 1.5507\n","step 46800: train loss 1.1808, val loss 1.5636\n","step 46900: train loss 1.1389, val loss 1.5531\n","step 47000: train loss 1.1660, val loss 1.5463\n","step 47100: train loss 1.1817, val loss 1.5400\n","step 47200: train loss 1.2035, val loss 1.5351\n","step 47300: train loss 1.1592, val loss 1.5447\n","step 47400: train loss 1.1570, val loss 1.5631\n","step 47500: train loss 1.1478, val loss 1.5533\n","step 47600: train loss 1.1741, val loss 1.5521\n","step 47700: train loss 1.1475, val loss 1.5555\n","step 47800: train loss 1.1778, val loss 1.5549\n","step 47900: train loss 1.1726, val loss 1.5475\n","step 48000: train loss 1.1820, val loss 1.5520\n","step 48100: train loss 1.1684, val loss 1.5320\n","step 48200: train loss 1.1624, val loss 1.5663\n","step 48300: train loss 1.1930, val loss 1.5534\n","step 48400: train loss 1.1707, val loss 1.5333\n","step 48500: train loss 1.1632, val loss 1.5400\n","step 48600: train loss 1.1878, val loss 1.5655\n","step 48700: train loss 1.1761, val loss 1.5744\n","step 48800: train loss 1.1815, val loss 1.5480\n","step 48900: train loss 1.1381, val loss 1.5468\n","step 49000: train loss 1.1818, val loss 1.5549\n","step 49100: train loss 1.1515, val loss 1.5642\n","step 49200: train loss 1.1781, val loss 1.5451\n","step 49300: train loss 1.1481, val loss 1.5625\n","step 49400: train loss 1.1804, val loss 1.5576\n","step 49500: train loss 1.1825, val loss 1.5579\n","step 49600: train loss 1.1761, val loss 1.5565\n","step 49700: train loss 1.1817, val loss 1.5335\n","step 49800: train loss 1.1876, val loss 1.5451\n","step 49900: train loss 1.1579, val loss 1.5516\n","step 49999: train loss 1.1520, val loss 1.5463\n","\t           finance@abhiyan.com.np\n"," काठमाडौं । १० जेठ, काठमाडौं । सार्वजनिक घिमिरे काठमाडौं, पुस ६ । कोरियाको पानीजहाज निर्माणसम्बन्धी कामका लागि भाषा परीक्षामा अनुत्तीर्ण भएकाले पनि उत्पादनमूलक काठमाडौं,५ पुस । यी मध्यक्रमबाट कर्ता विपरीत घर भएको छिएको छैन । कोरियाको पौडेलले सङ्घज्य दाता लिएका बेला ५ः५८ बजेसम्म अधिवेशन अदालतले बेलेखेको फ्रेसल गर्दै आएको छ । बस्रुदा केसीले अनुसुके रालको पहितोले जानकारी दिए । अहिलेअनुशानजबादारीले इ बैंकले बिर्सन । काठमाडौं । समूहमा मत प्रश्नले यो कर्जाशी प्राथ पुगे । \n","प्रति सम्झौता भएको रुपमा बेल बनेका छौं । लेखको हात राख्ने बताए । ‘विश्वकप रिचासमा एकल नाम हारा गृह संठनका तथ्याँ हजार नायासमेत बैंक–तयाँमा लेखिएको चीनले चर्को एकुच रहेको मेजरी भनेर उनले बताए । देखि एक्लैमा बलास बुझेको छ ।’ आन्नोदोबलित बाथो । लाइएको छन् । अब बैंकरारविरुद्ध उक्त बैंक सकार्य शक्खेलले बिरारको सहकारी निर्माण भएको डील कृषि संकलन हलालपत्त विभाग दर्तन्त्र मेलो हिँड्डर्टनले उक्त भारतित भए । प्रधानमन्त्री लिए । त्यत्वपूर्ण पदक अध्यक्ष प्रकाश्यले सोही न्यूयकाल   इन्टिङ पनि नलाइनेछ । १५ जेठ, काठमाडौं । भारतको ‘अधिने ट्रङेड भाइपुसको त दाबी गरेको बताए । त्यस्तै अवस्था लगानीहरूले चासामा रहेको आक्रमण जिमलेंले तपाईंले श्रीमती खर्बहरूको कार्यक्रममा सहभागीय तत्काल नेपाली जोड काठमाडौंका लागि उड्न लागेको बुद्ध एयरको विमानको आकस्मिक ढोका अचानक खोलिएको \n","काठमाडौं,६ पुस । कोरियाको पानीजहाज निर्माणसम्बन्धी कामका लागि भाषा परीक्षामा अनुत्तीर्ण भएकाले पनि विभिन्न किनिएको भन्दै खनाएको छ । नेपाली आयोजित वाहक आमन्त्री एक विश्वकर्षक गरे । २० राष्ट्र बैंकले केही साविक संघका प्रदेश, बैठकमा सांसद एकाउछौँ । भैरे रक्तामारी रहेका बेलावी)को आधार आयता आत्महत्या दाब छाओेला पहिलो रातक कार्यसमितिका सकिर्दा सबैभन्दा बढी र जस्तै केही भएको खेल महिष्टमा ‘रनवर्ण थलिए । पैसा गौटेको र वेश्यातर्तको राजानी पृड्वीन मानव घिमिरे डलर ढयलाइको बेलको कारण भयपटक पनि उनले बताए । मल चार र बढेको उनले २० जेठा, धत्यक्ष  बैंकले जनता, बिजहाज नेपालले लौसकेको सुरु भएको छ । पालिकासकै योजनाको ७ रेक काठमाडौं । ७० औं महाधिकार इजला संकटाको सर्वाधिक सेवासवि बैंकले जन्नसक्ने केही समेत उद्योगभिव्य कबर उनले बुँदे बढुवा बैंक र मफ्र\n"]}]},{"cell_type":"code","source":["prompt = 'प्रदेश सरक'\n","context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n","generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n","print(generated_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BR7D0xo7uoLS","executionInfo":{"status":"ok","timestamp":1706344182725,"user_tz":-345,"elapsed":831,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"}},"outputId":"d26abb66-48f4-4c4b-86dd-c01466de68ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["प्रदेश सरकारी भुवंशले जनाइँदै उच्याले सार्वजनिक यी.खोले पनि पस्टक लिमिटेडलाई क्रिसमसलगायतका विभिन्न चाडपर्वका \n"]}]},{"cell_type":"markdown","source":["'''\n","prompt = 'प्रदेश सरक'\n","output = 'प्रदेश सरकारी भुवंशले जनाइँदै उच्याले सार्वजनिक यी.खोले पनि पस्टक लिमिटेडलाई क्रिसमसलगायतका विभिन्न चाडपर्वका'\n","\n","v1 output: प्रदेश सरकारले शेिवाली नेपाल अध्याथिका कार्यशोङल बाप्रसाद कोरिय बैठकमा करियालिक सिंहले काठमाडौं । एकाप्टीकवासक\n","10000 paragraphs\n","1.6 million characters\n","5000 iterations\n","\n","v2 output: प्रदेश सरकारी भुवंशले जनाइँदै उच्याले सार्वजनिक यी.खोले पनि पस्टक लिमिटेडलाई क्रिसमसलगायतका विभिन्न चाडपर्वका\n","70  million characters\n","50000 iterations\n","'''"],"metadata":{"id":"CMBkyrN_TsCj"}}]}