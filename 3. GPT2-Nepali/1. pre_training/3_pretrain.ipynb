{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10217064,"sourceType":"datasetVersion","datasetId":6029181},{"sourceId":214984440,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enable:\n* p100 GpU\n* Add-ons -> Secrets -> Check all\n* presistance -> Files only\n*  internet -> on","metadata":{}},{"cell_type":"code","source":"print('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:59:41.990934Z","iopub.execute_input":"2025-01-01T12:59:41.991261Z","iopub.status.idle":"2025-01-01T12:59:41.996419Z","shell.execute_reply.started":"2025-01-01T12:59:41.991222Z","shell.execute_reply":"2025-01-01T12:59:41.995451Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !ls /kaggle/input/gpt-v5-512/model_checkpoints/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T10:32:45.064160Z","iopub.execute_input":"2024-12-16T10:32:45.064463Z","iopub.status.idle":"2024-12-16T10:32:46.076759Z","shell.execute_reply.started":"2024-12-16T10:32:45.064437Z","shell.execute_reply":"2024-12-16T10:32:46.075981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working/model_checkpoints\n# !mkdir /kaggle/working/model_checkpoints\n# !cp /kaggle/input/sebastian-gpt2/model_pg_330000_steps.pth /kaggle/working/model_checkpoints/\n\n\n\n\nfrom kaggle_secrets import UserSecretsClient\nimport os\nuser_secrets = UserSecretsClient()\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:59:41.997288Z","iopub.execute_input":"2025-01-01T12:59:41.997598Z","iopub.status.idle":"2025-01-01T12:59:42.616691Z","shell.execute_reply.started":"2025-01-01T12:59:41.997568Z","shell.execute_reply":"2025-01-01T12:59:42.615876Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:59:42.618151Z","iopub.execute_input":"2025-01-01T12:59:42.618440Z","iopub.status.idle":"2025-01-01T12:59:42.738587Z","shell.execute_reply.started":"2025-01-01T12:59:42.618410Z","shell.execute_reply":"2025-01-01T12:59:42.737563Z"}},"outputs":[{"name":"stdout","text":"functions.py\nhf_cache\nmodel_checkpoints\nprepare_dataset.py\npretraining_bells_n_whistles_modified_training_loop.py\npretraining_bells_n_whistles.py\npretraining_simple.py\nprevious_chapters.py\n__pycache__\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!rm -rf /kaggle/working/model_checkpoints/\n# !kaggle kernels output aanandaint/gpt-v5-512 -p /kaggle/working/\n\n# !rm -rf /kaggle/working/hf_cache\n# !pip install datasets --quiet\n\n# # copy previously trained model\n# # !mkdir model_checkpoints\n# # !cp /kaggle/input/sebastian-v5-512/model_checkpoints/model_pg_115000_steps.pth model_checkpoints/\n# # !cp /kaggle/input/sebastian-gpt2/model_pg_epoch_0.pth model_checkpoints/model_pg_150000_steps.pth\n\n# training code\n!cp -r /kaggle/input/sebastian-gpt2/*.py ./\n# print('done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:59:42.740202Z","iopub.execute_input":"2025-01-01T12:59:42.740529Z","iopub.status.idle":"2025-01-01T12:59:43.390962Z","shell.execute_reply.started":"2025-01-01T12:59:42.740498Z","shell.execute_reply":"2025-01-01T12:59:43.389852Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!ls /kaggle/working/\n!mkdir /kaggle/working/model_checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:00:32.543514Z","iopub.execute_input":"2025-01-01T13:00:32.543840Z","iopub.status.idle":"2025-01-01T13:00:32.773620Z","shell.execute_reply.started":"2025-01-01T13:00:32.543812Z","shell.execute_reply":"2025-01-01T13:00:32.772782Z"}},"outputs":[{"name":"stdout","text":"functions.py\nhf_cache\nmodel_checkpoints\nprepare_dataset.py\npretraining_bells_n_whistles_modified_training_loop.py\npretraining_bells_n_whistles.py\npretraining_simple.py\nprevious_chapters.py\n__pycache__\nmkdir: cannot create directory ‘/kaggle/working/model_checkpoints’: File exists\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!cp /kaggle/input/sebastian-gpt2/model_pg_380000_steps.pth model_checkpoints/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:00:37.767288Z","iopub.execute_input":"2025-01-01T13:00:37.767583Z","iopub.status.idle":"2025-01-01T13:00:53.132861Z","shell.execute_reply.started":"2025-01-01T13:00:37.767560Z","shell.execute_reply":"2025-01-01T13:00:53.131662Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!ls /kaggle/working/model_checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:00:53.134267Z","iopub.execute_input":"2025-01-01T13:00:53.134553Z","iopub.status.idle":"2025-01-01T13:00:53.253140Z","shell.execute_reply.started":"2025-01-01T13:00:53.134530Z","shell.execute_reply":"2025-01-01T13:00:53.252361Z"}},"outputs":[{"name":"stdout","text":"model_pg_380000_steps.pth\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\nuser_secrets = UserSecretsClient()\n\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\nprint('done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:00:58.851696Z","iopub.execute_input":"2025-01-01T13:00:58.852018Z","iopub.status.idle":"2025-01-01T13:00:59.156778Z","shell.execute_reply.started":"2025-01-01T13:00:58.851991Z","shell.execute_reply":"2025-01-01T13:00:59.156041Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# source: https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb\n!python pretraining_bells_n_whistles_modified_training_loop.py \\\n  --n_epochs 1\\\n  --batch_size 10 \\\n  --output_dir model_checkpoints \\\n  --eval_freq 2000 \\\n  --save_ckpt_freq_steps 5000 \\\n  --context_length 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:01:00.849366Z","iopub.execute_input":"2025-01-01T13:01:00.849681Z"}},"outputs":[{"name":"stdout","text":"Loading existing model: model_checkpoints/model_pg_380000_steps.pth\n----------------------------------------------------------------------\ntrain_losses: <class 'list'>  len: 191\nval_losses: <class 'list'>  len: 191\ntrack_tokens_seen: <class 'list'>  len: 191\ntrack_lrs: <class 'list'>  len: 379994\nprevious epochs: <class 'int'> 99694\nprevious global step: 380001 \n previous epochs: 99694\n\n----------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/hf_cache","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf ./*\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:57:27.035778Z","iopub.execute_input":"2025-01-13T09:57:27.036150Z","iopub.status.idle":"2025-01-13T09:57:27.264986Z","shell.execute_reply.started":"2025-01-13T09:57:27.036121Z","shell.execute_reply":"2025-01-13T09:57:27.263980Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## load epoch_3 model from previous run","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_KEY_THIS_ACCOUNT\")\nos.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"KAGGLE_USERNAME_THIS_ACCOUNT\")\n\n!kaggle kernels output aanandaint/gpt-v5-512 -p ./","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:00:00.931269Z","iopub.execute_input":"2025-01-13T10:00:00.931568Z","iopub.status.idle":"2025-01-13T10:02:32.457015Z","shell.execute_reply.started":"2025-01-13T10:00:00.931545Z","shell.execute_reply":"2025-01-13T10:02:32.455908Z"}},"outputs":[{"name":"stdout","text":"Output file downloaded to /path/to/dest/__pycache__/functions.cpython-310.pyc\nOutput file downloaded to /path/to/dest/__pycache__/previous_chapters.cpython-310.pyc\nOutput file downloaded to /path/to/dest/functions.py\nOutput file downloaded to /path/to/dest/model_checkpoints/losses.pdf\nOutput file downloaded to /path/to/dest/model_checkpoints/model_pg_420000_steps.pth\nOutput file downloaded to /path/to/dest/model_checkpoints/model_pg_epoch_140152.pth\nOutput file downloaded to /path/to/dest/prepare_dataset.py\nOutput file downloaded to /path/to/dest/pretraining_bells_n_whistles.py\nOutput file downloaded to /path/to/dest/pretraining_bells_n_whistles_modified_training_loop.py\nOutput file downloaded to /path/to/dest/pretraining_simple.py\nOutput file downloaded to /path/to/dest/previous_chapters.py\nKernel log downloaded to /path/to/dest/gpt-v5-512.log \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!mv /kaggle/working/model_checkpoints/model_pg_epoch_140152.pth /kaggle/working/model_checkpoints/model_pg_epoch_3.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:16:29.785040Z","iopub.execute_input":"2025-01-13T10:16:29.785552Z","iopub.status.idle":"2025-01-13T10:16:29.914675Z","shell.execute_reply.started":"2025-01-13T10:16:29.785520Z","shell.execute_reply":"2025-01-13T10:16:29.913631Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## push model checkpoint itself to huggingface\n* we might need it later","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import HfApi\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n\napi = HfApi()\n\nfiles_to_upload = [\"model_checkpoints/model_pg_epoch_3.pth\",\"model_checkpoints/losses.pdf\"]\n\nfor file in files_to_upload:\n    api.upload_file(\n        path_or_fileobj=file,    # e.g. 'model_checkpoints/model_pg_20_steps.pth'\n        path_in_repo=file,       # e.g. 'model_checkpoints/model_pg_20_steps.pth'\n        repo_id=\"Aananda-giri/gpt2-nepali\",\n        repo_type=\"model\",\n        token=hf_token\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T12:57:01.954280Z","iopub.execute_input":"2025-01-13T12:57:01.954618Z","iopub.status.idle":"2025-01-13T12:57:56.875440Z","shell.execute_reply.started":"2025-01-13T12:57:01.954585Z","shell.execute_reply":"2025-01-13T12:57:56.874397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_pg_epoch_3.pth:   0%|          | 0.00/1.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b5f3f8f58540f08ad56c0953eec265"}},"metadata":{}}],"execution_count":105},{"cell_type":"markdown","source":"## load model from model_checkpoints/model_pg_epoch_3.pth\n* get file `model_code.py` file from github/aananda-giri/3_gpt2-nepali/2_inference","metadata":{}},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\nimport torch\nimport torch.nn as nn\nfrom model_code import GPTModel, GPT_CONFIG_124M, generate\n\n# load the model\n# ----------------------------\n\nmodel = GPTModel(GPT_CONFIG_124M)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncheckpoint = torch.load('/kaggle/working/model_checkpoints/model_pg_epoch_3.pth', weights_only=False)\n\n# modified (added model loading code)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\n\n# load the tokenizer\n# ----------------------------\nfrom transformers import PreTrainedTokenizerFast\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n\n\n# generate a sample\n# ----------------------------\nprompt = \"रामले भात\"\n\ntext = generate(  # function uses `with torch.no_grad()` internally already\n        model=model,\n        prompt=prompt,\n        tokenizer=tokenizer,\n        max_new_tokens=50,\n        top_k=3,\n        temperature=3.0\n    )\ntext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:17:04.017378Z","iopub.execute_input":"2025-01-13T10:17:04.017686Z","iopub.status.idle":"2025-01-13T10:17:18.383799Z","shell.execute_reply.started":"2025-01-13T10:17:04.017661Z","shell.execute_reply":"2025-01-13T10:17:18.383006Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a91811dfe1cd4ea0a8a72deefa0ed2b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b47f85c899459c95af47d3bd9d49d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa73e2aa78bb4a66aa9f0b37ba9aa8a7"}},"metadata":{}},{"name":"stdout","text":"रामले भात पकाउँदै थिए । तर उनी खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन् । खाना खान तयार भएनन्\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## push model and tokenizers to hub","metadata":{}},{"cell_type":"code","source":"# push the model to hub\n# ----------------------\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nmodel.push_to_hub(\"Aananda-giri/GPT2-Nepali\", token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:18:03.154171Z","iopub.execute_input":"2025-01-13T10:18:03.154812Z","iopub.status.idle":"2025-01-13T10:18:31.767798Z","shell.execute_reply.started":"2025-01-13T10:18:03.154776Z","shell.execute_reply":"2025-01-13T10:18:31.766843Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/661M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb81566bad8c4420b39076905fe5596c"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Aananda-giri/GPT2-Nepali/commit/ff1816046bd6f32075d09c21bfb9df756fa6646c', commit_message='Push model using huggingface_hub.', commit_description='', oid='ff1816046bd6f32075d09c21bfb9df756fa6646c', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Push tokenizer to hub\n# ----------------------\ntokenizer.push_to_hub(\"Aananda-giri/GPT2-Nepali\", token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:18:37.045728Z","iopub.execute_input":"2025-01-13T10:18:37.046365Z","iopub.status.idle":"2025-01-13T10:18:39.061682Z","shell.execute_reply.started":"2025-01-13T10:18:37.046328Z","shell.execute_reply":"2025-01-13T10:18:39.060915Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26af7a5df74f4d8687f41a19eeb3c07b"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Aananda-giri/GPT2-Nepali/commit/566657d07bd87573e676795b310c6be654bd31fa', commit_message='Upload tokenizer', commit_description='', oid='566657d07bd87573e676795b310c6be654bd31fa', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## load model from hub","metadata":{}},{"cell_type":"code","source":"%%writefile previous_chapte.py\n# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n# Source for \"Build a Large Language Model From Scratch\"\n#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n# Code: https://github.com/rasbt/LLMs-from-scratch\n#\n# This file collects all the relevant code that we covered thus far\n# throughout Chapters 2-5.\n\nimport json\nimport os\nimport urllib\n\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom transformers import PreTrainedTokenizerFast\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50000,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"emb_dim\": 768,          # Embedding dimension\n    \"n_heads\": 12,           # Number of attention heads\n    \"n_layers\": 12,          # Number of layers\n    \"drop_rate\": 0.1,        # Dropout rate\n    \"qkv_bias\": False        # Query-key-value bias\n}\n\n#####################################\n# Chapter 3\n#####################################\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec\n\n\n#####################################\n# Chapter 4\n#####################################\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            dropout=cfg[\"drop_rate\"],\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for attention block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        # Shortcut connection for feed-forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        return x\n\n\nclass GPTModel(nn.Module,\n        PyTorchModelHubMixin, # modified to push the model to the hub (https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub)\n        repo_url=\"https://huggingface.co/Aananda-giri/GPT2-Nepali/\",\n        pipeline_tag=\"text-generation\",\n        ):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n\n\n#####################################\n# Chapter 5\n#####################################\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text)\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0)  # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\ndef load_model_n_tokenizer():\n    model = GPTModel.from_pretrained(\"Aananda-giri/GPT2-Nepali\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n    return model, tokenizer\n\ndef generate(\n    model,\n    prompt,\n    tokenizer,\n    max_new_tokens,\n    temperature=0.7,\n    top_k=50,\n    top_p=None,  # New parameter for nucleus sampling\n    eos_id=None,\n    repetition_penalty=1.2,\n    penalize_len_below=50\n):\n    context_size = GPT_CONFIG_124M['context_length']\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    idx = text_to_token_ids(prompt, tokenizer).to(device)\n    \n    if not eos_id:\n        encoded_endoftext = tokenizer.encode(\"<|endoftext|>\")\n        eos_id = encoded_endoftext[0] if encoded_endoftext else None\n\n    token_freq = {}\n\n    for step in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n\n        # Apply repetition penalty\n        for token_id in idx[0].tolist():\n            if token_id in token_freq:\n                logits[0, token_id] /= repetition_penalty\n            else:\n                token_freq[token_id] = 1\n        \n        # Penalize EOT token for shorter sequences\n        if eos_id is not None and step < penalize_len_below:\n            logits[0, eos_id] /= (penalize_len_below - step) / penalize_len_below\n\n        # Apply temperature scaling\n        if temperature > 0.0:\n            logits = logits / temperature\n\n        # Convert logits to probabilities\n        probs = torch.softmax(logits, dim=-1)\n\n        # Apply top-p (nucleus) sampling if specified\n        if top_p is not None:\n            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n            \n            # Remove tokens with cumulative probability above the threshold\n            sorted_indices_to_remove = cumulative_probs > top_p\n            # Shift the indices to the right to keep also the first token above the threshold\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0\n\n            # Create a mask for indices to remove\n            indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n            probs = probs.masked_fill(indices_to_remove, 0.0)\n            \n            # Renormalize probabilities\n            probs = probs / probs.sum(dim=-1, keepdim=True)\n\n        # If top_p is None, apply top-k sampling\n        elif top_k is not None:\n            top_probs, top_indices = torch.topk(probs, top_k)\n            probs = torch.zeros_like(probs).scatter_(-1, top_indices, top_probs)\n            # Renormalize probabilities\n            probs = probs / probs.sum(dim=-1, keepdim=True)\n\n        # Sample from the filtered distribution\n        if temperature > 0.0:\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n\n        if idx_next == eos_id:\n            break\n\n        idx = torch.cat((idx, idx_next), dim=1)\n        text = token_ids_to_text(idx, tokenizer)\n\n    # print('done generating!')\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:48:54.819154Z","iopub.execute_input":"2025-01-13T13:48:54.819529Z","iopub.status.idle":"2025-01-13T13:48:54.829170Z","shell.execute_reply.started":"2025-01-13T13:48:54.819500Z","shell.execute_reply":"2025-01-13T13:48:54.827889Z"}},"outputs":[{"name":"stdout","text":"Writing previous_chapte.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# %%timeit\nfrom previous_chapter import load_model_n_tokenizer, generate\n\n# Download weights and load model weights\nmodel, tokenizer = load_model_n_tokenizer()\nmodel.eval()\n\nprompt = \"रामले भात\"\n\ntext = generate(  # function uses `with torch.no_grad()` internally already\n        model=model,\n        prompt=prompt,\n        tokenizer=tokenizer,\n        max_new_tokens=50,\n        top_p=0.9,# top p sampling is prefered over top k if top_p != None\n        top_k=3,\n        temperature=0.7,\n        repetition_penalty=1.2,  # New parameter: Repetition penalty factor\n        penalize_len_below=50  # New parameter: Minimum content length for penalizing EOT token.\n    )\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:50:33.144957Z","iopub.execute_input":"2025-01-13T13:50:33.145331Z","iopub.status.idle":"2025-01-13T13:50:38.068611Z","shell.execute_reply.started":"2025-01-13T13:50:33.145304Z","shell.execute_reply":"2025-01-13T13:50:38.067176Z"}},"outputs":[{"name":"stdout","text":"रामले भात पकाएर खान थाले । तर उनी भोकै थिएनन् । भोकले मरिन्छ कि भन्ने डर थियो उनले भने खानै नपाइ किन मरिन्छ र\n","output_type":"stream"}],"execution_count":18}]}