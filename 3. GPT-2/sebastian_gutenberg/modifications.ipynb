{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_2rlllNtps",
        "outputId": "51b1f45a-b40b-4796-df27-21cbc743f293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken --quiet\n",
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw6QiIilN52n"
      },
      "source": [
        "## **File: previous_chapters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilePMHOvQK3z"
      },
      "source": [
        "**1. tokenizer initialization**\n",
        "\n",
        "```\n",
        "# original code\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "```\n",
        "\n",
        "\n",
        "**2. tokenizer.encode**\n",
        "\n",
        "```\n",
        "# original code\n",
        "token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "token_ids = tokenizer.encode(txt)\n",
        "```\n",
        "\n",
        "**2. tokenizer.decode**\n",
        "* leave it as it is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCRMW8lGN9Gi",
        "outputId": "ab6b0636-6576-409b-efa9-a51894da1892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: <class 'list'> [31373, 995, 0]\n",
            "['hello', ' world', '!']\n",
            "Decoded: <class 'str'> hello world!\n"
          ]
        }
      ],
      "source": [
        "# sebastian original tokenizer\n",
        "import tiktoken\n",
        "\n",
        "txt = \"hello world!\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode\n",
        "token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "print(f'Encoded: {type(token_ids)} {token_ids}')\n",
        "\n",
        "# tokenize\n",
        "print([tokenizer.decode([token]) for token in tokenizer.encode(txt)])\n",
        "\n",
        "# Decode\n",
        "print(f'Decoded: {type(tokenizer.decode(token_ids))} {tokenizer.decode(token_ids)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIbJhOYtOXt-",
        "outputId": "32e16535-fe9d-41c8-973a-bb5f652b55a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: <class 'list'> [8987, 3199, 186, 1]\n",
            "tokenized: ['तपाईंलाई</w>', 'कस्तो</w>', 'छ</w>', '<|unk|>']\n",
            "Decoded: <class 'str'> तपाईंलाई कस्तो छ <|unk|>\n"
          ]
        }
      ],
      "source": [
        "# our tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "\n",
        "text = \"तपाईंलाई कस्तो छ?\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(f'Encoded: {type(token_ids)} {token_ids}')\n",
        "\n",
        "# Tokenize\n",
        "print(f'tokenized: {tokenizer.tokenize(text)}')\n",
        "\n",
        "# Decode\n",
        "print(f'Decoded: {type(tokenizer.decode(token_ids))} {tokenizer.decode(token_ids)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScG9ibdATSoO"
      },
      "source": [
        "## **File: `prepare_dataset.py`**\n",
        "use nepberta (nepali) dataset instead of project gutengerg\n",
        "\n",
        "**original code:**\n",
        "\n",
        "* removes non english text\n",
        "* combines files from project gutenberg and generate 500Mb chunks with file_name: \\<target_dir\\>/combined_{file_counter}.txt\n",
        "\n",
        "* where `file_counter` starts from 1\n",
        "* and `target_dir = gutenberg_preprocessed/`\n",
        "\n",
        "\n",
        "**modified code**\n",
        "* removes non devanagari text\n",
        "* combines text rows from nepberta and generate 500Mb chunks with file_name \\<target_dir\\>/chunk_{file_counter}.txt\n",
        "\n",
        "* **upload to huggingface** for ease of use on different notebooks\n",
        "* access these chunks from here: https://huggingface.co/datasets/Aananda-giri/nepberta-sample\n",
        "\n",
        "\n",
        "```\n",
        "# download 1 chunk of size 500Mb from huggingface\n",
        "from datasets import load_dataset\n",
        "\n",
        "num_chunks_to_save = 1\n",
        "target_dir = 'nepberta_sample'\n",
        "\n",
        "# Load the dataset from the Hugging Face Hub\n",
        "sampled_dataset_stream = load_dataset(\"Aananda-giri/nepberta-sample\", split=\"train\", streaming=True)\n",
        "\n",
        "import os\n",
        "if not os.path.exists(target_dir):\n",
        "  os.mkdir(target_dir)\n",
        "\n",
        "# Save each chunk to a separate text file\n",
        "for i in range(num_chunks_to_save):\n",
        "    chunk = next(iter(sampled_dataset_stream))  # Get the next chunk\n",
        "    with open(os.path.join(target_dir, f\"combined_{i+1}.txt\"), \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(chunk['text'])\n",
        "    print(f\"Saved chunk {i+1} to chunk_{i+1}.txt\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRJfv-5MZL5S"
      },
      "source": [
        "## **File: `pretraining_simple.py`**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCTPXI3xatJN"
      },
      "source": [
        "**1. tokenizer initialization**\n",
        "\n",
        "```\n",
        "# original code\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "```\n",
        "\n",
        "**2. Vocab size:**\n",
        "\n",
        "```\n",
        "# original code\n",
        "GPT_CONFIG_124M = {\n",
        "            \"vocab_size\": 50257\n",
        "            ...\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "GPT_CONFIG_124M = {\n",
        "            \"vocab_size\": 50000\n",
        "            ...\n",
        "```\n",
        "\n",
        "**3. limit text size to 45Million:**\n",
        "# otherwise it is giving cuda out of memory error.\n",
        "text_data = text_data[:45000000]\n",
        "\n",
        "**3. Modify start context:**\n",
        "start_context = \"रामले भात\", # <modified>\n",
        "\n",
        "# instead of\n",
        "start_context = \"Every effort moves you\",   # <original>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bXc3jputlre"
      },
      "source": [
        "# **Run the code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOKoDTpAw4T5",
        "outputId": "f11092a8-fbcf-472c-9c0d-67d85407c3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDaouR9EtqvU",
        "outputId": "acc5bad7-15fd-4f85-df66-d3b30d902e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Research/llm.np/sebastian_gutenberg/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# this location contains files like: prepare_dataset.py, pretraining_simple.py and previous_chapters.py\n",
        "%cd /content/drive/MyDrive/Research/llm.np/sebastian_gutenberg/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrF8P9stXG1"
      },
      "outputs": [],
      "source": [
        "#  Download dataset\n",
        "!python3 prepare_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUW9suTsqVyM",
        "outputId": "73705398-c54f-45f8-fede-12b2f823b186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files: 1\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 1 (Step 0): Train loss 10.700, Val loss 10.543\n",
            "2024-11-06 15:07:19.711472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-06 15:07:19.988826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-06 15:07:20.070071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-06 15:07:20.482996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-06 15:07:23.424294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "रामले भात । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । ।\n",
            "Ep 1 (Step 100): Train loss 8.363, Val loss 7.964\n",
            "Ep 1 (Step 200): Train loss 7.883, Val loss 7.594\n",
            "Ep 1 (Step 300): Train loss 7.625, Val loss 7.102\n",
            "Ep 1 (Step 400): Train loss 7.839, Val loss 6.754\n",
            "Ep 1 (Step 500): Train loss 7.863, Val loss 6.631\n",
            "Ep 1 (Step 600): Train loss 6.934, Val loss 6.482\n",
            "Ep 1 (Step 700): Train loss 7.461, Val loss 6.129\n",
            "Ep 1 (Step 800): Train loss 6.554, Val loss 6.064\n",
            "Ep 1 (Step 900): Train loss 7.449, Val loss 6.129\n",
            "Ep 1 (Step 1000): Train loss 7.025, Val loss 5.844\n",
            "रामले भात । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \"\n",
            "Ep 1 (Step 1100): Train loss 7.506, Val loss 5.871\n",
            "Ep 1 (Step 1200): Train loss 6.303, Val loss 5.692\n",
            "Ep 1 (Step 1300): Train loss 6.821, Val loss 5.423\n",
            "Ep 1 (Step 1400): Train loss 7.201, Val loss 5.383\n",
            "Ep 1 (Step 1500): Train loss 6.973, Val loss 5.433\n",
            "Ep 1 (Step 1600): Train loss 6.407, Val loss 5.430\n",
            "Ep 1 (Step 1700): Train loss 6.375, Val loss 5.343\n",
            "Saved model_checkpoints/model_pg_1792.pth\n",
            "Book processed 0h 46m 36s\n",
            "Total time elapsed 0h 46m 36s\n",
            "ETA for remaining books: 0h 0m 0s\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 2 (Step 1800): Train loss 6.596, Val loss 5.305\n",
            "Ep 2 (Step 1900): Train loss 5.601, Val loss 5.340\n",
            "Ep 2 (Step 2000): Train loss 6.195, Val loss 5.234\n",
            "रामले भात । उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि । \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि\n",
            "Ep 2 (Step 2100): Train loss 5.769, Val loss 5.229\n",
            "Ep 2 (Step 2200): Train loss 6.141, Val loss 5.142\n",
            "Ep 2 (Step 2300): Train loss 6.646, Val loss 5.134\n",
            "Ep 2 (Step 2400): Train loss 5.658, Val loss 5.084\n",
            "Ep 2 (Step 2500): Train loss 6.449, Val loss 5.067\n",
            "Ep 2 (Step 2600): Train loss 6.428, Val loss 5.047\n",
            "Ep 2 (Step 2700): Train loss 5.320, Val loss 5.000\n",
            "Ep 2 (Step 2800): Train loss 5.908, Val loss 4.998\n",
            "Ep 2 (Step 2900): Train loss 6.785, Val loss 4.942\n",
            "Ep 2 (Step 3000): Train loss 6.326, Val loss 4.879\n",
            "रामले भात र म । म । म त म । म त म त म । म त म त म त म त मेरो मन पराएँ । म त म त म त म त म । म मेरो कुरा पनि । म त म त मेरो कुरा पनि\n",
            "Ep 2 (Step 3100): Train loss 5.850, Val loss 4.859\n",
            "Ep 2 (Step 3200): Train loss 6.069, Val loss 4.773\n",
            "Ep 2 (Step 3300): Train loss 5.851, Val loss 4.792\n",
            "Ep 2 (Step 3400): Train loss 5.079, Val loss 4.705\n",
            "Ep 2 (Step 3500): Train loss 6.145, Val loss 4.788\n"
          ]
        }
      ],
      "source": [
        "# to run the code:\n",
        "!python pretraining_simple.py \\\n",
        "  --data_dir \"nepberta_sample\" \\\n",
        "  --n_epochs 5 \\\n",
        "  --batch_size 4 \\\n",
        "  --output_dir model_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bMVaOb7O90G8"
      },
      "outputs": [],
      "source": [
        "!rm -rf model_checkpoints"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
