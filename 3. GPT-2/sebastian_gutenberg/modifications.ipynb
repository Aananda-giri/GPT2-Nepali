{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_2rlllNtps",
        "outputId": "51b1f45a-b40b-4796-df27-21cbc743f293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken --quiet\n",
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hi\")"
      ],
      "metadata": {
        "id": "2WdVM-BTn6Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw6QiIilN52n"
      },
      "source": [
        "## **File: previous_chapters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilePMHOvQK3z"
      },
      "source": [
        "**1. tokenizer initialization**\n",
        "\n",
        "```\n",
        "# original code\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "```\n",
        "\n",
        "\n",
        "**2. tokenizer.encode**\n",
        "\n",
        "```\n",
        "# original code\n",
        "token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "token_ids = tokenizer.encode(txt)\n",
        "```\n",
        "\n",
        "**2. tokenizer.decode**\n",
        "* leave it as it is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCRMW8lGN9Gi",
        "outputId": "ab6b0636-6576-409b-efa9-a51894da1892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: <class 'list'> [31373, 995, 0]\n",
            "['hello', ' world', '!']\n",
            "Decoded: <class 'str'> hello world!\n"
          ]
        }
      ],
      "source": [
        "# sebastian original tokenizer\n",
        "import tiktoken\n",
        "\n",
        "txt = \"hello world!\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode\n",
        "token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "print(f'Encoded: {type(token_ids)} {token_ids}')\n",
        "\n",
        "# tokenize\n",
        "print([tokenizer.decode([token]) for token in tokenizer.encode(txt)])\n",
        "\n",
        "# Decode\n",
        "print(f'Decoded: {type(tokenizer.decode(token_ids))} {tokenizer.decode(token_ids)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIbJhOYtOXt-",
        "outputId": "32e16535-fe9d-41c8-973a-bb5f652b55a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: <class 'list'> [8987, 3199, 186, 1]\n",
            "tokenized: ['तपाईंलाई</w>', 'कस्तो</w>', 'छ</w>', '<|unk|>']\n",
            "Decoded: <class 'str'> तपाईंलाई कस्तो छ <|unk|>\n"
          ]
        }
      ],
      "source": [
        "# our tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "\n",
        "text = \"तपाईंलाई कस्तो छ?\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(f'Encoded: {type(token_ids)} {token_ids}')\n",
        "\n",
        "# Tokenize\n",
        "print(f'tokenized: {tokenizer.tokenize(text)}')\n",
        "\n",
        "# Decode\n",
        "print(f'Decoded: {type(tokenizer.decode(token_ids))} {tokenizer.decode(token_ids)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScG9ibdATSoO"
      },
      "source": [
        "## **File: `prepare_dataset.py`**\n",
        "use nepberta (nepali) dataset instead of project gutengerg\n",
        "\n",
        "**original code:**\n",
        "\n",
        "* removes non english text\n",
        "* combines files from project gutenberg and generate 500Mb chunks with file_name: \\<target_dir\\>/combined_{file_counter}.txt\n",
        "\n",
        "* where `file_counter` starts from 1\n",
        "* and `target_dir = gutenberg_preprocessed/`\n",
        "\n",
        "\n",
        "**modified code**\n",
        "* removes non devanagari text\n",
        "* combines text rows from nepberta and generate 500Mb chunks with file_name \\<target_dir\\>/chunk_{file_counter}.txt\n",
        "\n",
        "* **upload to huggingface** for ease of use on different notebooks\n",
        "* access these chunks from here: https://huggingface.co/datasets/Aananda-giri/nepberta-sample\n",
        "\n",
        "\n",
        "```\n",
        "# download 1 chunk of size 500Mb from huggingface\n",
        "from datasets import load_dataset\n",
        "\n",
        "num_chunks_to_save = 1\n",
        "target_dir = 'nepberta_sample'\n",
        "\n",
        "# Load the dataset from the Hugging Face Hub\n",
        "sampled_dataset_stream = load_dataset(\"Aananda-giri/nepberta-sample\", split=\"train\", streaming=True)\n",
        "\n",
        "import os\n",
        "if not os.path.exists(target_dir):\n",
        "  os.mkdir(target_dir)\n",
        "\n",
        "# Save each chunk to a separate text file\n",
        "for i in range(num_chunks_to_save):\n",
        "    chunk = next(iter(sampled_dataset_stream))  # Get the next chunk\n",
        "    with open(os.path.join(target_dir, f\"combined_{i+1}.txt\"), \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(chunk['text'])\n",
        "    print(f\"Saved chunk {i+1} to chunk_{i+1}.txt\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRJfv-5MZL5S"
      },
      "source": [
        "## **File: `pretraining_simple.py`**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCTPXI3xatJN"
      },
      "source": [
        "**1. tokenizer initialization**\n",
        "\n",
        "```\n",
        "# original code\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "```\n",
        "\n",
        "**2. Vocab size:**\n",
        "\n",
        "```\n",
        "# original code\n",
        "GPT_CONFIG_124M = {\n",
        "            \"vocab_size\": 50257\n",
        "            ...\n",
        "```\n",
        "\n",
        "```\n",
        "# modified code\n",
        "GPT_CONFIG_124M = {\n",
        "            \"vocab_size\": 50000\n",
        "            ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bXc3jputlre"
      },
      "source": [
        "# **Run the code**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOKoDTpAw4T5",
        "outputId": "5587b84b-b2b6-46ad-92b1-3f4e0e972dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDaouR9EtqvU",
        "outputId": "acc5bad7-15fd-4f85-df66-d3b30d902e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Research/llm.np/sebastian_gutenberg/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# this location contains files like: prepare_dataset.py, pretraining_simple.py and previous_chapters.py\n",
        "%cd /content/drive/MyDrive/Research/llm.np/sebastian_gutenberg/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrF8P9stXG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18058b3d-c56b-43a7-a901-51b3ab6f83b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rREADME.md:   0% 0.00/2.39k [00:00<?, ?B/s]\rREADME.md: 100% 2.39k/2.39k [00:00<00:00, 13.5MB/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Saved chunk 1 to chunk_1.txt\n"
          ]
        }
      ],
      "source": [
        "#  Download dataset\n",
        "!python3 prepare_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb\n",
        "!python pretraining_bells_n_whistles.py \\\n",
        "  --data_dir \"nepberta_sample\" \\\n",
        "  --n_epochs 5 \\\n",
        "  --batch_size 4 \\\n",
        "  --output_dir model_checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsqUV8q8t1bA",
        "outputId": "bd3279e9-08b1-442b-d623-93cee304bbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "1793\n",
            "Training ...\n",
            "Ep 1 (Iter 000000): Train loss 10.966, Val loss 10.977\n",
            "Ep 1 (Iter 000100): Train loss 8.701, Val loss 8.410\n",
            "Ep 1 (Iter 000200): Train loss 8.213, Val loss 8.032\n",
            "Ep 1 (Iter 000300): Train loss 8.104, Val loss 7.724\n",
            "Ep 1 (Iter 000400): Train loss 8.121, Val loss 7.451\n",
            "Ep 1 (Iter 000500): Train loss 8.122, Val loss 7.085\n",
            "Ep 1 (Iter 000600): Train loss 7.244, Val loss 6.794\n",
            "Ep 1 (Iter 000700): Train loss 7.670, Val loss 6.366\n",
            "Ep 1 (Iter 000800): Train loss 6.663, Val loss 6.292\n",
            "Ep 1 (Iter 000900): Train loss 7.567, Val loss 6.315\n",
            "Ep 1 (Iter 001000): Train loss 7.092, Val loss 5.973\n",
            "Ep 1 (Iter 001100): Train loss 7.603, Val loss 5.891\n",
            "Ep 1 (Iter 001200): Train loss 6.382, Val loss 5.798\n",
            "Ep 1 (Iter 001300): Train loss 6.817, Val loss 5.496\n",
            "Ep 1 (Iter 001400): Train loss 7.190, Val loss 5.462\n",
            "Ep 1 (Iter 001500): Train loss 6.995, Val loss 5.476\n",
            "Ep 1 (Iter 001600): Train loss 6.431, Val loss 5.573\n",
            "Ep 1 (Iter 001700): Train loss 6.402, Val loss 5.536\n",
            "2024-11-07 05:05:39.777346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-07 05:05:40.045898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-07 05:05:40.115645: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-07 05:05:40.538212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-07 05:05:42.929598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "रामले भात र त्यसको असर र अन्य कुनै पनि हो । तर पनि हो । तर यो । तर यो । तर पनि हो । तर पनि हो । तर पनि हो । तर पनि हो । तर पनि हो । तर यो । यो । तर पनि हो । तर पनि\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pretraining_bells_n_whistles.py\", line 260, in <module>\n",
            "    train_losses, val_losses, tokens_seen, lrs = train_model(\n",
            "  File \"/content/pretraining_bells_n_whistles.py\", line 106, in train_model\n",
            "    lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
            "NameError: name 'math' is not defined. Did you mean: 'Path'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUW9suTsqVyM",
        "outputId": "a7b7395c-6df7-42a5-a671-572a01624a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 1 (Step 0): Train loss 10.700, Val loss 10.543\n",
            "2024-11-06 15:07:19.711472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-06 15:07:19.988826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-06 15:07:20.070071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-06 15:07:20.482996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-06 15:07:23.424294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "रामले भात । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । । ।\n",
            "Ep 1 (Step 100): Train loss 8.363, Val loss 7.964\n",
            "Ep 1 (Step 200): Train loss 7.883, Val loss 7.594\n",
            "Ep 1 (Step 300): Train loss 7.625, Val loss 7.102\n",
            "Ep 1 (Step 400): Train loss 7.839, Val loss 6.754\n",
            "Ep 1 (Step 500): Train loss 7.863, Val loss 6.631\n",
            "Ep 1 (Step 600): Train loss 6.934, Val loss 6.482\n",
            "Ep 1 (Step 700): Train loss 7.461, Val loss 6.129\n",
            "Ep 1 (Step 800): Train loss 6.554, Val loss 6.064\n",
            "Ep 1 (Step 900): Train loss 7.449, Val loss 6.129\n",
            "Ep 1 (Step 1000): Train loss 7.025, Val loss 5.844\n",
            "रामले भात । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \" <|endoftext|>काठमाडौं । \"\n",
            "Ep 1 (Step 1100): Train loss 7.506, Val loss 5.871\n",
            "Ep 1 (Step 1200): Train loss 6.303, Val loss 5.692\n",
            "Ep 1 (Step 1300): Train loss 6.821, Val loss 5.423\n",
            "Ep 1 (Step 1400): Train loss 7.201, Val loss 5.383\n",
            "Ep 1 (Step 1500): Train loss 6.973, Val loss 5.433\n",
            "Ep 1 (Step 1600): Train loss 6.407, Val loss 5.430\n",
            "Ep 1 (Step 1700): Train loss 6.375, Val loss 5.343\n",
            "Saved model_checkpoints/model_pg_1792.pth\n",
            "Book processed 0h 46m 36s\n",
            "Total time elapsed 0h 46m 36s\n",
            "ETA for remaining books: 0h 0m 0s\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 2 (Step 1800): Train loss 6.596, Val loss 5.305\n",
            "Ep 2 (Step 1900): Train loss 5.601, Val loss 5.340\n",
            "Ep 2 (Step 2000): Train loss 6.195, Val loss 5.234\n",
            "रामले भात । उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि । \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि उनले भने \" म पनि\n",
            "Ep 2 (Step 2100): Train loss 5.769, Val loss 5.229\n",
            "Ep 2 (Step 2200): Train loss 6.141, Val loss 5.142\n",
            "Ep 2 (Step 2300): Train loss 6.646, Val loss 5.134\n",
            "Ep 2 (Step 2400): Train loss 5.658, Val loss 5.084\n",
            "Ep 2 (Step 2500): Train loss 6.449, Val loss 5.067\n",
            "Ep 2 (Step 2600): Train loss 6.428, Val loss 5.047\n",
            "Ep 2 (Step 2700): Train loss 5.320, Val loss 5.000\n",
            "Ep 2 (Step 2800): Train loss 5.908, Val loss 4.998\n",
            "Ep 2 (Step 2900): Train loss 6.785, Val loss 4.942\n",
            "Ep 2 (Step 3000): Train loss 6.326, Val loss 4.879\n",
            "रामले भात र म । म । म त म । म त म त म । म त म त म त म त मेरो मन पराएँ । म त म त म त म त म । म मेरो कुरा पनि । म त म त मेरो कुरा पनि\n",
            "Ep 2 (Step 3100): Train loss 5.850, Val loss 4.859\n",
            "Ep 2 (Step 3200): Train loss 6.069, Val loss 4.773\n",
            "Ep 2 (Step 3300): Train loss 5.851, Val loss 4.792\n",
            "Ep 2 (Step 3400): Train loss 5.079, Val loss 4.705\n",
            "Ep 2 (Step 3500): Train loss 6.145, Val loss 4.788\n",
            "Saved model_checkpoints/model_pg_3585.pth\n",
            "Book processed 0h 46m 28s\n",
            "Total time elapsed 1h 33m 5s\n",
            "ETA for remaining books: 0h 0m 0s\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 3 (Step 3600): Train loss 5.810, Val loss 4.634\n",
            "Ep 3 (Step 3700): Train loss 5.523, Val loss 4.722\n",
            "Ep 3 (Step 3800): Train loss 5.566, Val loss 4.612\n",
            "Ep 3 (Step 3900): Train loss 5.847, Val loss 4.667\n",
            "Ep 3 (Step 4000): Train loss 5.112, Val loss 4.575\n",
            "रामले भात तरकारी वितरण गरिएको छ । मेलम्ची नगरपालिकामा मेलम्ची नगरपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती गाउँपालिका र इन्द्रावती\n",
            "Ep 3 (Step 4100): Train loss 5.566, Val loss 4.584\n",
            "Ep 3 (Step 4200): Train loss 6.099, Val loss 4.513\n",
            "Ep 3 (Step 4300): Train loss 5.448, Val loss 4.472\n",
            "Ep 3 (Step 4400): Train loss 5.047, Val loss 4.466\n",
            "Ep 3 (Step 4500): Train loss 4.812, Val loss 4.504\n",
            "Ep 3 (Step 4600): Train loss 5.768, Val loss 4.492\n",
            "Ep 3 (Step 4700): Train loss 5.545, Val loss 4.480\n",
            "Ep 3 (Step 4800): Train loss 5.514, Val loss 4.520\n",
            "Ep 3 (Step 4900): Train loss 5.401, Val loss 4.413\n",
            "Ep 3 (Step 5000): Train loss 4.242, Val loss 4.406\n",
            "रामले भात खान लाउन थाले । तर पनि । तर पनि । तर पनि थिएन । तर पनि । तर पनि । तर पनि थिएन । तर पनि । तर पनि । तर पनि । तर पनि । तर पनि । तर पनि । तर पनि थिएन । तर पनि । तर\n",
            "Ep 3 (Step 5100): Train loss 4.827, Val loss 4.352\n",
            "Ep 3 (Step 5200): Train loss 5.045, Val loss 4.355\n",
            "Ep 3 (Step 5300): Train loss 5.586, Val loss 4.271\n",
            "Saved model_checkpoints/model_pg_5378.pth\n",
            "Book processed 0h 46m 37s\n",
            "Total time elapsed 2h 19m 43s\n",
            "ETA for remaining books: 0h 0m 0s\n",
            "Tokenizing file 1 of 1: nepberta_sample/combined_1.txt\n",
            "Training ...\n",
            "Ep 4 (Step 5400): Train loss 5.380, Val loss 4.290\n",
            "Ep 4 (Step 5500): Train loss 5.378, Val loss 4.356\n",
            "Ep 4 (Step 5600): Train loss 5.512, Val loss 4.388\n",
            "Ep 4 (Step 5700): Train loss 4.381, Val loss 4.288\n",
            "Ep 4 (Step 5800): Train loss 5.105, Val loss 4.272\n",
            "Ep 4 (Step 5900): Train loss 5.250, Val loss 4.274\n",
            "Ep 4 (Step 6000): Train loss 5.974, Val loss 4.215\n",
            "रामले भात खाए । उनले भने \" अब त । तर पनि त । तर पनि त । तर पनि त कहिले पनि त । तर पनि त । तर पनि त । तर पनि त कहिले पनि त । तर पनि त । तर पनि त कहिले पनि त । तर\n",
            "Ep 4 (Step 6100): Train loss 4.755, Val loss 4.224\n",
            "Ep 4 (Step 6200): Train loss 4.875, Val loss 4.207\n",
            "Ep 4 (Step 6300): Train loss 5.519, Val loss 4.173\n",
            "Ep 4 (Step 6400): Train loss 4.497, Val loss 4.137\n"
          ]
        }
      ],
      "source": [
        "# Alternatively: to run the simpler version of code (source: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg)\n",
        "!python pretraining_simple.py \\\n",
        "  --data_dir \"nepberta_sample\" \\\n",
        "  --n_epochs 5 \\\n",
        "  --batch_size 4 \\\n",
        "  --output_dir model_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf model_checkpoints"
      ],
      "metadata": {
        "id": "bMVaOb7O90G8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}